Considering this code snippet:I was alarmed by Pylint with this message regarding the line with the if statement:The rule C1801, at first glance, did not sound very reasonable to me, and the definition on the reference guide does not explain why this is a problem. In fact, it downright calls it an incorrect use.My search attempts have also failed to provide me a deeper explanation. I do understand that a sequence's length property may be lazily evaluated, and that __len__ can be programmed to have side effects, but it is questionable whether that alone is problematic enough for Pylint to call such a use incorrect. Hence, before I simply configure my project to ignore the rule, I would like to know whether I am missing something in my reasoning.When is the use of len(SEQ) as a condition value problematic? What major situations is Pylint attempting to avoid with C1801?

2017-03-30 14:52:30Z

Considering this code snippet:I was alarmed by Pylint with this message regarding the line with the if statement:The rule C1801, at first glance, did not sound very reasonable to me, and the definition on the reference guide does not explain why this is a problem. In fact, it downright calls it an incorrect use.My search attempts have also failed to provide me a deeper explanation. I do understand that a sequence's length property may be lazily evaluated, and that __len__ can be programmed to have side effects, but it is questionable whether that alone is problematic enough for Pylint to call such a use incorrect. Hence, before I simply configure my project to ignore the rule, I would like to know whether I am missing something in my reasoning.When is the use of len(SEQ) as a condition value problematic? What major situations is Pylint attempting to avoid with C1801?It’s not really problematic to use len(SEQUENCE) – though it may not be as efficient (see chepner’s comment). Regardless, Pylint checks code for compliance with the PEP 8 style guide which states thatAs an occasional Python programmer, who flits between languages, I’d consider the len(SEQUENCE) construct to be more readable and explicit (「Explicit is better then implicit」).  However, using the fact that an empty sequence evaluates to False in a Boolean context is considered more「Pythonic」.Note that the use of len(seq) is in fact required (instead of just checking the bool value of seq) when using NumPy arrays.results in an exception:

ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()And hence for code that uses both Python lists and NumPy arrays, the C1801 message is less than helpful.Pylint was failing for my code and research led me to this post:This was my code before:This was after my code fix.  By using the int() attribute, I seem to have satisfied the Pep8/Pylint and doesn't seem to have a negative impact on my code:By adding .__trunc__() to the sequence it seems to have settled the need.I do not see a difference in the behaviour, but if anyone knows specifics that I am missing, please let me know.

What are some good Python ORM solutions? [closed]

eLuke

[What are some good Python ORM solutions? [closed]](https://stackoverflow.com/questions/53428/what-are-some-good-python-orm-solutions)

I'm evaluating and looking at using CherryPy for a project that's basically a JavaScript front-end from the client-side (browser) that talks to a Python web service on the back-end. So, I really need something fast and lightweight on the back-end that I can implement using Python that then speaks to the PostgreSQL DB via an ORM (JSON to the browser).I'm also looking at Django, which I like, since its ORM is built-in. However, I think Django might be a little more than I really need (i.e. more features than I really need == slower?).Anyone have any experience with different Python ORM solutions that can compare and contrast their features and functionality, speed, efficiency, etc.?

2008-09-10 04:49:32Z

I'm evaluating and looking at using CherryPy for a project that's basically a JavaScript front-end from the client-side (browser) that talks to a Python web service on the back-end. So, I really need something fast and lightweight on the back-end that I can implement using Python that then speaks to the PostgreSQL DB via an ORM (JSON to the browser).I'm also looking at Django, which I like, since its ORM is built-in. However, I think Django might be a little more than I really need (i.e. more features than I really need == slower?).Anyone have any experience with different Python ORM solutions that can compare and contrast their features and functionality, speed, efficiency, etc.?SQLAlchemy is more full-featured and powerful (uses the DataMapper pattern).  Django ORM has a cleaner syntax and is easier to write for (ActiveRecord pattern).  I don't know about performance differences.SQLAlchemy also has a declarative layer that hides some complexity and gives it a ActiveRecord-style syntax more similar to the Django ORM.I wouldn't worry about Django being "too heavy."  It's decoupled enough that you can use the ORM if you want without having to import the rest.That said, if I were already using CherryPy for the web layer and just needed an ORM, I'd probably opt for SQLAlchemy.If you're looking for lightweight and are already familiar with django-style declarative models, check out peewee: 

https://github.com/coleifer/peeweeExample:Check the docs for more examples.Storm has arguably the simplest API:And it makes it painless to drop down into raw SQL when you need to:I usually use SQLAlchemy.  It's pretty powerful and is probably the most mature python ORM.If you're planning on using CherryPy, you might also look into dejavu as it's by Robert Brewer (the guy that is the current CherryPy project leader).  I personally haven't used it, but I do know some people that love it.SQLObject is a little bit easier to use ORM than SQLAlchemy, but it's not quite as powerful.Personally, I wouldn't use the Django ORM unless I was planning on writing the entire project in Django, but that's just me.SQLAlchemy's declarative extension, which is becoming standard in 0.5, provides an all in one interface very much like that of Django or Storm.  It also integrates seamlessly with classes/tables configured using the datamapper style:We use Elixir alongside SQLAlchemy and have liked it so far. Elixir puts a layer on top of SQLAlchemy that makes it look more like the "ActiveRecord pattern" counter parts.This seems to be the canonical reference point for high-level database interaction in Python:

http://wiki.python.org/moin/HigherLevelDatabaseProgrammingFrom there, it looks like Dejavu implements Martin Fowler's DataMapper pattern fairly abstractly in Python.I think you might look at:AutumnStormThere is no conceivable way that the unused features in Django will give a performance penalty. Might just come in handy if you ever decide to upscale the project.I used Storm + SQLite for a small project, and was pretty happy with it until I added multiprocessing. Trying to use the database from multiple processes resulted in a "Database is locked" exception. I switched to SQLAlchemy, and the same code worked with no problems.SQLAlchemy is very, very powerful. However it is not thread safe make sure you keep that in mind when working with cherrypy in thread-pool mode. I'd check out SQLAlchemy It's really easy to use and the models you work with aren't bad at all. Django uses SQLAlchemy for it's ORM but using it by itself lets you use it's full power.Here's a small example on creating and selecting orm objects

How do I convert a string to a double in Python?

user46646

[How do I convert a string to a double in Python?](https://stackoverflow.com/questions/482410/how-do-i-convert-a-string-to-a-double-in-python)

I would like to know how to convert a string containing digits to a double.

2009-01-27 05:40:38Z

I would like to know how to convert a string containing digits to a double.There you go.  Use float (which behaves like and has the same precision as a C,C++, or Java double).The decimal operator might be more in line with what you are looking for:Be aware that if your string number contains more than 15 significant  digits float(s) will round it.In those cases it is better to use DecimalHere is an explanation and some code samples: 

https://docs.python.org/3/library/sys.html#sys.float_info

Argparse: Required arguments listed under「optional arguments」?

mort

[Argparse: Required arguments listed under「optional arguments」?](https://stackoverflow.com/questions/24180527/argparse-required-arguments-listed-under-optional-arguments)

I use the following simple code to parse some arguments; note that one of them is required. Unfortunately, when the user runs the script without providing the argument, the displayed usage/help text does not indicate that there is a non-optional argument, which I find very confusing. How can I get python to indicate that an argument is not optional?Here is the code:When running above code without providing the required argument, I get the following output:

2014-06-12 09:09:13Z

I use the following simple code to parse some arguments; note that one of them is required. Unfortunately, when the user runs the script without providing the argument, the displayed usage/help text does not indicate that there is a non-optional argument, which I find very confusing. How can I get python to indicate that an argument is not optional?Here is the code:When running above code without providing the required argument, I get the following output:Parameters starting with - or -- are usually considered optional. All other parameters are positional parameters and as such required by design (like positional function arguments). It is possible to require optional arguments, but this is a bit against their design. Since they are still part of the non-positional arguments, they will still be listed under the confusing header「optional arguments」even if they are required. The missing square brackets in the usage part however show that they are indeed required.See also the documentation:That being said, the headers「positional arguments」and「optional arguments」in the help are generated by two argument groups in which the arguments are automatically separated into. Now, you could「hack into it」and change the name of the optional ones, but a far more elegant solution would be to create another group for「required named arguments」(or whatever you want to call them):Since I prefer to list required arguments before optional, I hack around it via:and this outputs:I can live without 'help' showing up in the optional arguments group.Building off of @Karl Rosaenand this outputs:One more time, building off of @RalphyZThis one doesn't break the exposed API.Which will show the same as above and should survive future versions:

How do I check if I'm running on Windows in Python? [duplicate]

gman

[How do I check if I'm running on Windows in Python? [duplicate]](https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python)

I found the platform module but it says it returns 'Windows' and it's returning 'Microsoft' on my machine. I notice in another thread here on stackoverflow it returns 'Vista' sometimes.So, the question is, how do implemement?In a forward compatible way? If I have to check for things like 'Vista' then it will break when the next version of windows comes out.Note: The answers claiming this is a duplicate question do not actually answer the question isWindows. They answer the question "what platform". Since many flavors of windows exist none of them comprehensively describe how to get an answer of isWindows. 

2009-08-25 01:15:33Z

I found the platform module but it says it returns 'Windows' and it's returning 'Microsoft' on my machine. I notice in another thread here on stackoverflow it returns 'Vista' sometimes.So, the question is, how do implemement?In a forward compatible way? If I have to check for things like 'Vista' then it will break when the next version of windows comes out.Note: The answers claiming this is a duplicate question do not actually answer the question isWindows. They answer the question "what platform". Since many flavors of windows exist none of them comprehensively describe how to get an answer of isWindows. Python os moduleSpecifically for Python 3.6/3.7:In your case, you want to check for 'nt' as os.name output:There is also a note on os.name:Are you using platform.system?If that isn't working, maybe try platform.win32_ver and if it doesn't raise an exception, you're on Windows; but I don't know if that's forward compatible to 64-bit, since it has 32 in the name.But os.name is probably the way to go, as others have mentioned.You should be able to rely on os.name.edit: Now I'd say the clearest way to do this is via the platform module, as per the other answer.in sys too:or

Using sphinx with Markdown instead of RST

digi604

[Using sphinx with Markdown instead of RST](https://stackoverflow.com/questions/2471804/using-sphinx-with-markdown-instead-of-rst)

I hate RST but love sphinx. Is there a way that sphinx reads markdown instead of reStructuredText?

2010-03-18 16:57:05Z

I hate RST but love sphinx. Is there a way that sphinx reads markdown instead of reStructuredText?The "proper" way to do that would be to write a docutils parser for markdown.  (Plus a Sphinx option to choose the parser.)  The beauty of this would be instant support for all docutils output formats (but you might not care about that, as similar markdown tools already exist for most).  Ways to approach that without developing a parser from scratch:UPDATE: https://github.com/sgenoud/remarkdown is a markdown reader for docutils.  It didn't take any of the above shortcuts but uses a Parsley PEG grammar inspired by peg-markdown. Doesn't yet support directives.UPDATE: https://github.com/rtfd/recommonmark and is another docutils reader, natively supported on ReadTheDocs.  Derived from remarkdown but uses the CommonMark-py parser.  Doesn't support directives, but can convert more or less natural Markdown syntaxes to appropriate structures e.g. list of links to a toctree.  For other needs, an ```eval_rst fenced block lets you embed any rST directive.In all cases, you'll need to invent extensions of Markdown to represent Sphinx directives and roles.  While you may not need all of them, some like .. toctree:: are essential.

This I think is the hardest part.  reStructuredText before the Sphinx extensions was already richer than markdown.  Even heavily extended markdown, such as pandoc's, is mostly a subset of rST feature set.  That's a lot of ground to cover!Implementation-wise, the easiest thing is adding a generic construct to express any docutils role/directive.  The obvious candidates for syntax inspiration are:But such a generic mapping will not be the most markdown-ish solution...

Currently most active places to discuss markdown extensions are https://groups.google.com/forum/#!topic/pandoc-discuss, https://github.com/scholmd/scholmd/This also means you can't just reuse a markdown parser without extending it somehow.  Pandoc again lives up to its reputation as the swiss army knife of document conversion by supporting custom filtes.  (In fact, if I were to approach this I'd try to build a generic bridge between docutils readers/transformers/writers and pandoc readers/filters/writers.  It's more than you need but the payoff would be much wider than just sphinx/markdown.)Alternative crazy idea: instead of extending markdown to handle Sphinx, extend reStructuredText to support (mostly) a superset of markdown!  The beauty is you'll be able to use any Sphinx features as-is, yet be able to write most content in markdown.There is already considerable syntax overlap; most notably link syntax is incompatible.  I think if you add support to RST for markdown links, and ###-style headers, and change default `backticks` role to literal, and maybe change indented blocks to mean literal (RST supports > ... for quotations nowdays), you'll get something usable that supports most markdown.You can use Markdown and reStructuredText in the same Sphinx project. How to do this is succinctly documented on Read The Docs. Install recommonmark (pip install recommonmark) and then edit conf.py:I've created a small example project on Github (serra/sphinx-with-markdown) demonstrating how (and that) it works. It uses CommonMark 0.5.4 and recommonmark 0.4.0.This doesn't use Sphinx, but MkDocs will build your documentation using Markdown.  I also hate rst, and have really enjoyed MkDocs so far.Update: this is now officially supported and documented in the sphinx docs.It looks like a basic implementation has made it's way into Sphinx but word has not gotten round yet. See github issue commentinstall dependencies:adjust conf.py:Markdown and ReST do different things.RST provides an object model for working with documents.Markdown provides a way to engrave bits of text.It seems reasonable to want to reference your bits of Markdown content from your sphinx project, using RST to stub out the overall information architecture and flow of a larger document.  Let markdown do what it does, which is allow writers to focus on writing text.Is there a way to reference a markdown domain, just to engrave the content as-is?  RST/sphinx seems to have taken care of features like toctree without duplicating them in markdown.I went with Beni's suggestion of using pandoc for this task.  Once installed the following script will convert all markdown files in the source directory to rst files, so that you can just write all your documentation in markdown.  Hope this is useful for others.This is now officially supported: http://www.sphinx-doc.org/en/stable/markdown.htmlThere is a workaround.

The sphinx-quickstart.py script generates a Makefile.

You can easily invoke Pandoc from the Makefile every time you'd like to generate the documentation in order to convert Markdown to reStructuredText.Note that building documentation using maven and embedded Sphinx + MarkDown support is fully supported by following maven plugin :https://trustin.github.io/sphinx-maven-plugin/index.html

How to check if text is「empty」(spaces, tabs, newlines) in Python?

bodacydo

[How to check if text is「empty」(spaces, tabs, newlines) in Python?](https://stackoverflow.com/questions/2405292/how-to-check-if-text-is-empty-spaces-tabs-newlines-in-python)

How can I test if the string is empty in Python?For example,"<space><space><space>" is empty, so is"<space><tab><space><newline><space>", so is"<newline><newline><newline><tab><newline>", etc.

2010-03-08 22:29:25Z

How can I test if the string is empty in Python?For example,"<space><space><space>" is empty, so is"<space><tab><space><newline><space>", so is"<newline><newline><newline><tab><newline>", etc."Return true if there are only whitespace characters in the string and there is at least one character, false otherwise."Combine that with a special case for handling the empty string.Alternatively, you could useAnd then check if strippedString is empty.You want to use the isspace() methodThat's defined on every string object. Here it is an usage example for your specific use case:isspacefor those who expect a behaviour like the apache StringUtils.isBlank or Guava Strings.isNullOrEmpty :Check the length of the list given by of split() method.Or

Compare output of strip() method with null.Here is an answer that should work in all cases:If the variable is None, it will stop at not sand not evaluate further (since not None == True). Apparently, the strip()method takes care of the usual cases of tab, newline, etc. I'm assuming in your scenario, an empty string is a string that is truly empty or one that contains all white space.Note this does not check for NoneI used following: To check if a string is just a spaces or newlineUse this simple codeResemblence with c# string static method isNullOrWhiteSpace. 

How do I calculate percentiles with python/numpy?

Uri

[How do I calculate percentiles with python/numpy?](https://stackoverflow.com/questions/2374640/how-do-i-calculate-percentiles-with-python-numpy)

Is there a convenient way to calculate percentiles for a sequence or single-dimensional numpy array?I am looking for something similar to Excel's percentile function.I looked in NumPy's statistics reference, and couldn't find this. All I could find is the median (50th percentile), but not something more specific.

2010-03-03 20:21:13Z

Is there a convenient way to calculate percentiles for a sequence or single-dimensional numpy array?I am looking for something similar to Excel's percentile function.I looked in NumPy's statistics reference, and couldn't find this. All I could find is the median (50th percentile), but not something more specific.You might be interested in the SciPy Stats package. It has the percentile function you're after and many other statistical goodies.percentile() is available in numpy too.This ticket leads me to believe they won't be integrating percentile() into numpy anytime soon.By the way, there is a pure-Python implementation of percentile function, in case one doesn't want to depend on scipy.  The function is copied below:Here's how to do it without numpy, using only python to calculate the percentile.The definition of percentile I usually see expects as a result the value from the supplied list below which P percent of values are found... which means the result must be from the set, not an interpolation between set elements.  To get that, you can use a simpler function.If you would rather get the value from the supplied list at or below which P percent of values are found, then use this simple modification:Or with the simplification suggested by @ijustlovemath:check for scipy.stats module: Starting Python 3.8, the standard library comes with the quantiles function  as part of the statistics module:quantiles returns for a given distribution dist a list of n - 1 cut points separating the n quantile intervals (division of dist into n continuous intervals with equal probability):where n, in our case (percentiles) is 100.To calculate the percentile of a series, run:For example:In case you need the answer to be a member of the input numpy array:Just to add that the percentile function in numpy by default calculates the output as a linear weighted average of the two neighboring entries in the input vector.  In some cases people may want the returned percentile to be an actual element of the vector, in this case, from v1.9.0 onwards you can use the "interpolation" option, with either "lower", "higher" or "nearest".The latter is an actual entry in the vector, while the former is a linear interpolation of two vector entries that border the percentilefor a series: used describe functionssuppose you have df with following columns sales and id. you want to calculate percentiles for sales then it works like this,A convenient way to calculate percentiles for a one-dimensional numpy sequence or matrix is by using numpy.percentile <https://docs.scipy.org/doc/numpy/reference/generated/numpy.percentile.html>. Example:However, if there is any NaN value in your data, the above function will not be useful. The recommended function to use in that case is the numpy.nanpercentile <https://docs.scipy.org/doc/numpy/reference/generated/numpy.nanpercentile.html> function:In the two options presented above, you can still choose the interpolation mode. Follow the examples below for easier understanding.If your input array only consists of integer values, you might be interested in the percentil answer as an integer. If so, choose interpolation mode such as ‘lower’, ‘higher’, or ‘nearest’.

Removing nan values from an array

Dax Feliz

[Removing nan values from an array](https://stackoverflow.com/questions/11620914/removing-nan-values-from-an-array)

I want to figure out how to remove nan values from my array. My array looks something like this: How can I remove the nan values from x?

2012-07-23 21:36:54Z

I want to figure out how to remove nan values from my array. My array looks something like this: How can I remove the nan values from x?If you're using numpy for your arrays, you can also useEquivalently [Thanks to chbrown for the added shorthand] ExplanationThe inner function, numpy.isnan returns a boolean/logical array which has the value True everywhere that x is not-a-number. As we want the opposite, we use the logical-not operator, ~ to get an array with Trues everywhere that x is a valid number.Lastly we use this logical array to index into the original array x, to retrieve just the non-NaN values.works both for lists and numpy array

since v!=v only for NaNTry this:For more, read on List Comprehensions.For me the answer by @jmetz didn't work, however using pandas isnull() did.Doing the above :orI found that resetting to the same variable (x) did not remove the actual nan values and had to use a different variable. Setting it to a different variable removed the nans.

e.g. As shown by others works. But it will throw an error if the numpy dtype is not a native data type, for example if it is object. In that case you can use pandas.If you're using numpy The accepted answer changes shape for 2d arrays.

I present a solution here, using the Pandas dropna() functionality.

It works for 1D and 2D arrays. In the 2D case you can choose weather to drop the row or column containing np.nan.Result:A simplest way is: Documentation: https://docs.scipy.org/doc/numpy/reference/generated/numpy.nan_to_num.htmlThis is my approach to filter ndarray "X" for NaNs and infs,   I create a map of rows without any NaN and any inf as follows:idx is a tuple. It's second column (idx[1]) contains the indices of the array, where no NaN nor inf where found across the row.Then:filtered_X contains X without NaN nor inf.

Reading specific lines only

3zzy

[Reading specific lines only](https://stackoverflow.com/questions/2081836/reading-specific-lines-only)

I'm using a for loop to read a file, but I only want to read specific lines, say line #26 and #30. Is there any built-in feature to achieve this?Thanks

2010-01-17 17:14:11Z

I'm using a for loop to read a file, but I only want to read specific lines, say line #26 and #30. Is there any built-in feature to achieve this?ThanksIf the file to read is big, and you don't want to read the whole file in memory at once:Note that i == n-1 for the nth line.In Python 2.6 or later:The quick answer:or:There is a more elegant solution for extracting many lines: linecache (courtesy of "python: how to jump to a particular line in a huge text file?", a previous stackoverflow.com question).Quoting the python documentation linked above:Change the 4 to your desired line number, and you're on. Note that 4 would bring the fifth line as the count is zero-based.If the file might be very large, and cause problems when read into memory, it might be a good idea to take @Alok's advice and use enumerate().To Conclude:A fast and compact approach could be:this accepts any open file-like object thefile (leaving up to the caller whether it should be opened from a disk file, or via e.g a socket, or other file-like stream) and a set of zero-based line indices whatlines, and returns a list, with low memory footprint and reasonable speed.  If the number of lines to be returned is huge, you might prefer a generator:which is basically only good for looping upon -- note that the only difference comes from using rounded rather than square parentheses in the return statement, making a list comprehension and a generator expression respectively.Further note that despite the mention of "lines" and "file" these functions are much, much more general -- they'll work on any iterable, be it an open file or any other, returning a list (or generator) of items based on their progressive item-numbers.  So, I'd suggest using more appropriately general names;-).For the sake of offering another solution:I hope this is quick and easy :)if you want line 7Reading files is incredible fast. Reading a 100MB file takes less than 0.1 seconds (see my article Reading and Writing Files with Python). Hence you should read it completely and then work with the single lines.What most answer here do is not wrong, but bad style. Opening files should always be done with with as it makes sure that the file is closed again.So you should do it like this:If you happen to have a huge file and memory consumption is a concern, you can process it line by line:For the sake of completeness, here is one more option.Let's start with a definition from python docs:Though the slice notation is not directly applicable to iterators in general, the itertools package contains a replacement function:The additional advantage of the function is that it does not read the iterator until the end. So you can do more complex things:And to answer the original question:Some of these are lovely, but it can be done much more simply:That will use simply list slicing, it loads the whole file, but most systems will minimise memory usage appropriately, it's faster than most of the methods given above, and works on my 10G+ data files. Good luck!You can do a seek() call which positions your read head to a specified byte within the file. This won't help you unless you know exactly how many bytes (characters) are written in the file before the line you want to read. Perhaps your file is strictly formatted (each line is X number of bytes?) or, you could count the number of characters yourself (remember to include invisible characters like line breaks) if you really want the speed boost. Otherwise, you do have to read every line prior to the line you desire, as per one of the many solutions already proposed here.How about this:If you don't mind importing then fileinput does exactly what you need (this is you can read the line number of the current line)I prefer this approach because it's more general-purpose, i.e. you can use it on a file, on the result of f.readlines(), on a StringIO object, whatever:Here's my little 2 cents, for what it's worth ;)A better and minor change for Alok Singhal's answerFile objects have a .readlines() method which will give you a list of the contents of the file, one line per list item. After that, you can just use normal list slicing techniques.http://docs.python.org/library/stdtypes.html#file.readlines@OP, you can use enumerateUsing the with statement, this opens the file, prints lines 26 and 30, then closes the file. Simple!You can do this very simply with this syntax that someone already mentioned, but it's by far the easiest way to do it:To print line# 3,   Original author: Frank HofmannIf your large text file file is strictly well-structured (meaning every line has the same length l), you could use for n-th lineDisclaimer This does only work for files with the same length!To print certain lines in a text file.  Create a "lines2print" list and then

just print when the enumeration is "in" the lines2print list.

To get rid of extra '\n' use line.strip() or line.strip('\n').

I just like "list comprehension" and try to use when I can.

I like the "with" method to read text files in order to prevent

leaving a file open for any reason.or if list is small just type in list as a list into the comprehension.To print desired line.

To print line above/below required line.execute---->dline("D:\dummy.txt",6)  i.e dline("file path", line_number, if you want upper line of the searched line give 1 for lower -1 this is optional default value will be taken 0)If you want to read specific lines, such as line starting after some threshold line then you can use the following codes,

file  = open("files.txt","r")

lines = file.readlines() ## convert to list of lines

datas  = lines[11:]  ## raed the specific linesI think this would work

ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()

Homunculus Reticulli

[ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()](https://stackoverflow.com/questions/10062954/valueerror-the-truth-value-of-an-array-with-more-than-one-element-is-ambiguous)

I just discovered a logical bug in my code which was causing all sorts of problems. I was inadvertently doing a bitwise AND instead of a logical AND.I changed the code from:TO:To my surprise, I got the rather cryptic error message:Why was a similar error not emitted when I use a bitwise operation - and how do I fix this?

2012-04-08 12:56:07Z

I just discovered a logical bug in my code which was causing all sorts of problems. I was inadvertently doing a bitwise AND instead of a logical AND.I changed the code from:TO:To my surprise, I got the rather cryptic error message:Why was a similar error not emitted when I use a bitwise operation - and how do I fix this?r is a numpy (rec)array. So r["dt"] >= startdate is also a (boolean)

array. For numpy arrays the & operation returns the elementwise-and of the two

boolean arrays.The NumPy developers felt there was no one commonly understood way to evaluate

an array in boolean context: it could mean True if any element is

True, or it could mean True if all elements are True, or True if the array has non-zero length, just to name three possibilities.Since different users might have different needs and different assumptions, the

NumPy developers refused to guess and instead decided to raise a ValueError

whenever one tries to evaluate an array in boolean context. Applying and to

two numpy arrays causes the two arrays to be evaluated in boolean context (by

calling __bool__ in Python3 or __nonzero__ in Python2).Your original code looks correct. However, if you do want and, then instead of a and b use (a-b).any() or (a-b).all().I had the same problem (i.e. indexing with multi-conditions, here it's finding data in a certain date range). The (a-b).any() or (a-b).all() seem not working, at least for me. Alternatively I found another solution which works perfectly for my desired functionality (The truth value of an array with more than one element is ambigous when trying to index an array).Instead of using suggested code above, simply using a numpy.logical_and(a,b) would work. Here you may want to rewrite the code as The reason for the exception is that and implicitly calls bool. First on the left operand and (if the left operand is True) then on the right operand. So x and y is equivalent to bool(x) and bool(y). However the bool on a numpy.ndarray (if it contains more than one element) will throw the exception you have seen:The bool() call is implicit in and, but also in if, while, or, so any of the following examples will also fail:There are more functions and statements in Python that hide bool calls, for example 2 < x < 10 is just another way of writing 2 < x and x < 10. And the and will call bool: bool(2 < x) and bool(x < 10).The element-wise equivalent for and would be the np.logical_and function, similarly you could use np.logical_or as equivalent for or. For boolean arrays - and comparisons like <, <=, ==, !=, >= and > on NumPy arrays return boolean NumPy arrays - you can also use the element-wise bitwise functions (and operators): np.bitwise_and (& operator)and bitwise_or (| operator):A complete list of logical and binary functions can be found in the NumPy documentation:if you work with pandas what solved the issue for me was that i was trying to do calculations when I had NA values, the solution was to run:df = df.dropna()And after that the calculation that failed.try this=>  numpy.array(r)   or     numpy.array(yourvariable)   followed by the command to compare whatever you wish to.

When should I use uuid.uuid1() vs. uuid.uuid4() in python?

rocketmonkeys

[When should I use uuid.uuid1() vs. uuid.uuid4() in python?](https://stackoverflow.com/questions/1785503/when-should-i-use-uuid-uuid1-vs-uuid-uuid4-in-python)

I understand the differences between the two from the docs.uuid1():

Generate a UUID from a host ID, sequence number, and the current timeuuid4():

Generate a random UUID.So uuid1 uses machine/sequence/time info to generate a UUID.  What are the pros and cons of using each?I know uuid1() can have privacy concerns, since it's based off of machine-information.  I wonder if there's any more subtle when choosing one or the other.  I just use uuid4() right now, since it's a completely random UUID.  But I wonder if I should be using uuid1 to lessen the risk of collisions.Basically, I'm looking for people's tips for best-practices on using one vs. the other.  Thanks!

2009-11-23 19:48:24Z

I understand the differences between the two from the docs.uuid1():

Generate a UUID from a host ID, sequence number, and the current timeuuid4():

Generate a random UUID.So uuid1 uses machine/sequence/time info to generate a UUID.  What are the pros and cons of using each?I know uuid1() can have privacy concerns, since it's based off of machine-information.  I wonder if there's any more subtle when choosing one or the other.  I just use uuid4() right now, since it's a completely random UUID.  But I wonder if I should be using uuid1 to lessen the risk of collisions.Basically, I'm looking for people's tips for best-practices on using one vs. the other.  Thanks!uuid1() is guaranteed to not produce any collisions (under the assumption you do not create too many of them at the same time). I wouldn't use it if it's important that there's no connection between the uuid and the computer, as the mac address gets used to make it unique across computers.You can create duplicates by creating more than 214 uuid1 in less than 100ns, but this is not a problem for most use cases.uuid4() generates, as you said, a random UUID. The chance of a collision is really, really, really small. Small enough, that you shouldn't worry about it. The problem is, that a bad random-number generator makes it more likely to have collisions.This excellent answer by Bob Aman sums it up nicely. (I recommend reading the whole answer.)One instance when you may consider uuid1() rather than uuid4() is when UUIDs are produced on separate machines, for example when multiple online transactions are process on several machines for scaling purposes.In such a situation, the risks of having collisions due to poor choices in the way the pseudo-random number generators are initialized, for example, and also the potentially higher numbers of UUIDs produced render more likely the possibility of creating duplicate IDs.Another interest of uuid1(), in that case is that the machine where each GUID was initially produced is implicitly recorded (in the "node" part of UUID).  This and the time info, may help if only with debugging.My team just ran into trouble using UUID1 for a database upgrade script where we generated ~120k UUIDs within a couple of minutes. The UUID collision led to violation of a primary key constraint.We've upgraded 100s of servers but on our Amazon EC2 instances we ran into this issue a few times. I suspect poor clock resolution and switching to UUID4 solved it for us.One thing to note when using uuid1, if you use the default call (without giving clock_seq parameter) you have a chance of running into collisions: you have only 14 bit of randomness (generating 18 entries within 100ns gives you roughly 1% chance of a collision see birthday paradox/attack). The problem will never occur in most use cases, but on a virtual machine with poor clock resolution it will bite you.Perhaps something that's not been mentioned is that of locality. A MAC address or time-based ordering (UUID1) can afford increased database performance, since it's less work to sort numbers closer-together than those distributed randomly (UUID4) (see here).A second related issue, is that using UUID1 can be useful in debugging, even if origin data is lost or not explicitly stored (this is obviously in conflict with the privacy issue mentioned by the OP).In addition to the accepted answer, there's a third option that can be useful in some cases:You can make a hybrid between v1 & v4 by deliberately generating v1 UUIDs with a random broadcast MAC address (this is allowed by the v1 spec). The resulting v1 UUID is time dependant (like regular v1), but lacks all host-specific information (like v4). It's also much closer to v4 in it's collision-resistance: v1mc = 60 bits of time + 61 random bits = 121 unique bits; v4 = 122 random bits.First place I encountered this was Postgres' uuid_generate_v1mc() function. I've since used the following python equivalent:(note: I've got a longer + faster version that creates the UUID object directly; can post if anyone wants)In case of LARGE volumes of calls/second, this has the potential to exhaust system randomness.  You could use the stdlib random module instead (it will probably also be faster). But BE WARNED: it only takes a few hundred UUIDs before an attacker can determine the RNG state, and thus partially predict future UUIDs. 

How to filter a dictionary according to an arbitrary condition function?

Adam Matan

[How to filter a dictionary according to an arbitrary condition function?](https://stackoverflow.com/questions/2844516/how-to-filter-a-dictionary-according-to-an-arbitrary-condition-function)

I have a dictionary of points, say:I want to create a new dictionary with all the points whose x and y value is smaller than 5, i.e. points 'a', 'b' and 'd'.According to the the book, each dictionary has the items() function, which returns a list of (key, pair)  tuple:So I have written this:Is there a more elegant way? I was expecting Python to have some super-awesome dictionary.filter(f) function...

2010-05-16 16:30:34Z

I have a dictionary of points, say:I want to create a new dictionary with all the points whose x and y value is smaller than 5, i.e. points 'a', 'b' and 'd'.According to the the book, each dictionary has the items() function, which returns a list of (key, pair)  tuple:So I have written this:Is there a more elegant way? I was expecting Python to have some super-awesome dictionary.filter(f) function...Nowadays, in Python 2.7 and up, you can use a dict comprehension:And in Python 3:You could choose to call .iteritems() instead of .items() if you're in Python 2 and points may have a lot of entries.all(x < 5 for x in v) may be overkill if you know for sure each point will always be 2D only (in that case you might express the same constraint with an and) but it will work fine;-).I think that Alex Martelli's answer is definitely the most elegant way to do this, but just wanted to add a way to satisfy your want for a super awesome dictionary.filter(f) method in a Pythonic sort of way:Basically we create a class that inherits from dict, but adds the filter method. We do need to use .items() for the the filtering, since using .iteritems() while destructively iterating will raise exception.

How do I check that multiple keys are in a dict in a single pass?

hughdbrown

[How do I check that multiple keys are in a dict in a single pass?](https://stackoverflow.com/questions/1285911/how-do-i-check-that-multiple-keys-are-in-a-dict-in-a-single-pass)

I want to do something like:How do I check whether both 'foo' and 'bar' are in dict foo?

2009-08-17 02:12:35Z

I want to do something like:How do I check whether both 'foo' and 'bar' are in dict foo?Well, you could do this:Put in your own values for D and QYou don't have to wrap the left side in a set. You can just do this:This also performs better than the all(k in d...) solution.Using sets:Alternatively:How about this:While I like Alex Martelli's answer, it doesn't seem Pythonic to me.  That is, I thought an important part of being Pythonic is to be easily understandable.  With that goal, <= isn't easy to understand.While it's more characters, using issubset() as suggested by Karl Voigtland's answer is more understandable.  Since that method can use a dictionary as an argument, a short, understandable solution is:I'd like to use {'foo', 'bar'} in place of set(('foo', 'bar')), because it's shorter.  However, it's not that understandable and I think the braces are too easily confused as being a dictionary.I think this is the smartest and pithonic.Alex Martelli's solution set(queries) <= set(my_dict) is the shortest code but may not be the fastest. Assume Q = len(queries) and D = len(my_dict).This takes O(Q) + O(D) to make the two sets, and then (one hopes!) only O(min(Q,D)) to do the subset test -- assuming of course that Python set look-up is O(1) -- this is worst case (when the answer is True).The generator solution of hughdbrown (et al?) all(k in my_dict for k in queries) is worst-case O(Q).Complicating factors:

(1) the loops in the set-based gadget are all done at C-speed whereas the any-based gadget is looping over bytecode.

(2) The caller of the any-based gadget may be able to use any knowledge of probability of failure to order the query items accordingly whereas the set-based gadget allows no such control.As always, if speed is important, benchmarking under operational conditions is a good idea.How about using lambda?In case you want to: then:Not to suggest that this isn't something that you haven't thought of, but I find that the simplest thing is usually the best:Jason, () aren't necessary in Python.You can use .issubset() as wellJust my take on this, there are two methods that are easy to understand of all the given options. So my main criteria is have very readable code, not exceptionally fast code. To keep code understandable, i prefer to given possibilities:The fact that "var <= var2.keys()" executes faster in my testing below, i prefer this one.This seems to work

How do I get the object if it exists, or None if it does not exist?

TIMEX

[How do I get the object if it exists, or None if it does not exist?](https://stackoverflow.com/questions/3090302/how-do-i-get-the-object-if-it-exists-or-none-if-it-does-not-exist)

When I ask the model manager to get an object, it raises DoesNotExist when there is no matching object.Instead of DoesNotExist, how can I have go be None instead?

2010-06-22 04:38:42Z

When I ask the model manager to get an object, it raises DoesNotExist when there is no matching object.Instead of DoesNotExist, how can I have go be None instead?There is no 'built in' way to do this. Django will raise the DoesNotExist exception every time.

The idiomatic way to handle this in python is to wrap it in a try catch:What I did do, is to subclass models.Manager, create a safe_get like the code above and use that manager for my models. That way you can write: SomeModel.objects.safe_get(foo='bar').Since django 1.6 you can use first() method like so:From django docsYou can catch the exception and assign None to go.You can create a generic function for this.Use this like below:go will be None if no entry matches else will return the Content entry.Note:It will raises exception MultipleObjectsReturned if more than one entry returned for name="baby"You can do it this way:Now go variable could be either the object you want or NoneRef: https://docs.djangoproject.com/en/1.8/ref/models/querysets/#django.db.models.query.QuerySet.firstTo make things easier, here is a snippet of the code I wrote, based on inputs from the wonderful replies here:And then in your model:That's it.

Now you have MyModel.objects.get() as well as MyModel.objetcs.get_or_none()you could use exists with a filter:just an alternative for if you only want to know if it existsHandling exceptions at different points in your views could really be cumbersome..What about defining a custom Model Manager, in the models.py file, likeand then including it in the content Model classIn this way it can be easily dealt in the views i.e.It's one of those annoying functions that you might not want to re-implement:If you want a simple one-line solution that doesn't involve exception handling, conditional statements or a requirement of Django 1.6+, do this instead:I think it isn't bad idea to use get_object_or_404()This example is equivalent to:You can read more about get_object_or_404() in django online documentation.From django 1.7 onwards you can do like:The advantage of "MyQuerySet.as_manager()" is that both of the following will work:Here's a variation on the helper function that allows you to optionally pass in a QuerySet instance, in case you want to get the unique object (if present) from a queryset other than the model's all objects queryset (e.g. from a subset of child items belonging to a parent instance):This can be used in two ways, e.g.:Without exception:Using an exception:There is a bit of an argument about when one should use an exception in python. On the one hand, "it is easier to ask for forgiveness than for permission". While I agree with this, I believe that an exception should remain, well, the exception, and  the "ideal case" should run without hitting one. We can use Django builtin exception which attached to the models named as .DoesNotExist. So, we don't have to import ObjectDoesNotExist exception.Instead doing: We can do this:This is a copycat from Django's get_object_or_404 except that the method returns None. This is extremely useful when we have to use only() query to retreive certain fields only. This method can accept a model or a queryset.

How do I load a file into the python console?

sybind

[How do I load a file into the python console?](https://stackoverflow.com/questions/5280178/how-do-i-load-a-file-into-the-python-console)

I have some lines of python code that I'm continuously copying/pasting into the python console. Is there a load command or something I can run? e.g. load file.py

2011-03-12 01:26:11Z

I have some lines of python code that I'm continuously copying/pasting into the python console. Is there a load command or something I can run? e.g. load file.pyFor Python 2 (see other answers for Python 3) give this a try:Example usage:From the man page:So this should do what you want:Python 3: new exec (execfile dropped) !The execfile solution is valid only for Python 2. Python 3 dropped the execfile function - and promoted the exec statement to a builtin universal function. As the comment in Python 3.0's changelog and Hi-Angels comment suggest:useinstead ofFrom the shell command line:From the Python command lineorYou can just use an import statement:So, for example, if you had a file named my_script.py you'd load it like so:Open command prompt in the folder in which you files to be imported are present. when you type 'python', python terminal will be opened. Now you can use  If you're using IPython, you can simply run:See http://ipython.org/ipython-doc/rel-1.1.0/interactive/tutorial.htmlIf your path environment variable contains Python (eg. C:\Python27\) you can run your py file simply from Windows command line (cmd).

Howto here.

How to get a string after a specific substring?

havox

[How to get a string after a specific substring?](https://stackoverflow.com/questions/12572362/how-to-get-a-string-after-a-specific-substring)

How can I get a string after a specific substring?For example, I want to get the string after "world" in my_string="hello python world , i'm a beginner "

2012-09-24 20:24:26Z

How can I get a string after a specific substring?For example, I want to get the string after "world" in my_string="hello python world , i'm a beginner "The easiest way is probably just to split on your target wordsplit takes the word(or character) to split on and optionally a limit to the number of splits.In this example split on "world" and limit it to only one split.If you want to deal with the case where s2 is not present in s1, then use s1.find(s2) as opposed to index. If the return value of that call is -1, then s2 is not in s1.I'm surprised nobody mentioned partition.IMHO, this solution is more readable than @arshajii's.  Other than that, I think @arshajii's is the best for being the fastest -- it does not create any unnecessary copies/substrings.If you want to do this using regex, you could simply use a non-capturing group, to get the word "world" and then grab everything after, like soThe example string is tested hereYou want to use str.partition():because this option is faster than the alternatives.Note that this produces an empty string if the delimiter is missing:If you want to have the original string, then test if the second value returned from str.partition() is non-empty:You could also use str.split() with a limit of 1:However, this option is slower. For a best-case scenario, str.partition() is easily about 15% faster compared to str.split():This shows timings per execution with inputs here the delimiter is either missing (worst-case scenario), placed first (best case scenario), or in the lower half, upper half or last position. The fastest time is marked with [...] and <...> marks the worst.The above table is produced by a comprehensive time trial for all three options, produced below. I ran the tests on Python 3.7.4 on a 2017 model 15" Macbook Pro with 2.9 GHz Intel Core i7 and 16 GB ram.This script generates random sentences with and without the randomly selected delimiter present, and if present, at different positions in the generated sentence, runs the tests in random order with repeats (producing the fairest results accounting for random OS events taking place during testing), and then prints a table of the results:It's an old question but i faced a very same scenario, i need to split a string using as demiliter the word "low" the problem for me was that i have in the same string the word below and lower. I solved it using the re module this wayuse re.split with regex to match the exact wordthe generic code is:Hope this can help someone!You can use this package called "substring". Just type "pip install substring". You can get the substring by just mentioning the start and end characters/indices.For example:Output:Try this general approach:

How do I replace whitespaces with underscore and vice versa?

Lucas

[How do I replace whitespaces with underscore and vice versa?](https://stackoverflow.com/questions/1007481/how-do-i-replace-whitespaces-with-underscore-and-vice-versa)

I want to replace whitespace with underscore in a string to create nice URLs. So that for example:I am using Python with Django. Can this be solved using regular expressions?  

2009-06-17 14:41:36Z

I want to replace whitespace with underscore in a string to create nice URLs. So that for example:I am using Python with Django. Can this be solved using regular expressions?  You don't need regular expressions. Python has a built-in string method that does what you need:Replacing spaces is fine, but I might suggest going a little further to handle other URL-hostile characters like question marks, apostrophes, exclamation points, etc.  Also note that the general consensus among SEO experts is that dashes are preferred to underscores in URLs.Django has a 'slugify' function which does this, as well as other URL-friendly optimisations. It's hidden away in the defaultfilters module.This isn't exactly the output you asked for, but IMO it's better for use in URLs.This takes into account blank characters other than space and I think it's faster than using re module:Using the re module:Unless you have multiple spaces or other whitespace possibilities as above, you may just wish to use string.replace as others have suggested.use string's replace method:"this should be connected".replace(" ", "_")"this_should_be_disconnected".replace("_", " ")I'm using the following piece of code for my friendly urls:It works fine with unicode characters as well.Surprisingly this library not mentioned yetpython package named python-slugify, which does a pretty good job of slugifying:Works like this:Python has a built in method on strings called replace which is used as so:So you would use:I had this problem a while ago and I wrote code to replace characters in a string. I have to start remembering to check the python documentation because they've got built in functions for everything.if you assign this value to any variable, it will workby default mystring wont have thisYou can try this instead:OP is using python, but in javascript (something to be careful of since the syntaxes are similar.Match et replace space > underscore of all files in current directory

Django:「projects」vs「apps」

Dolph

[Django:「projects」vs「apps」](https://stackoverflow.com/questions/4879036/django-projects-vs-apps)

I have a fairly complex "product" I'm getting ready to build using Django. I'm going to avoid using the terms "project" and "application" in this context, because I'm not clear on their specific meaning in Django.Projects can have many apps. Apps can be shared among many projects. Fine.I'm not reinventing the blog or forum - I don't see any portion of my product being reusable in any context. Intuitively, I would call this one "application." Do I then do all my work in a single "app" folder?If so... in terms of Django's project.app namespace, my inclination is to use myproduct.myproduct, but of course this isn't allowed (but the application I'm building is my project, and my project is an application!). I'm therefore lead to believe that perhaps I'm supposed to approach Django by building one app per "significant" model, but I don't know where to draw the boundaries in my schema to separate it into apps - I have a lot of models with relatively complex relationships.I'm hoping there's a common solution to this...

2011-02-02 19:41:49Z

I have a fairly complex "product" I'm getting ready to build using Django. I'm going to avoid using the terms "project" and "application" in this context, because I'm not clear on their specific meaning in Django.Projects can have many apps. Apps can be shared among many projects. Fine.I'm not reinventing the blog or forum - I don't see any portion of my product being reusable in any context. Intuitively, I would call this one "application." Do I then do all my work in a single "app" folder?If so... in terms of Django's project.app namespace, my inclination is to use myproduct.myproduct, but of course this isn't allowed (but the application I'm building is my project, and my project is an application!). I'm therefore lead to believe that perhaps I'm supposed to approach Django by building one app per "significant" model, but I don't know where to draw the boundaries in my schema to separate it into apps - I have a lot of models with relatively complex relationships.I'm hoping there's a common solution to this...What is to stop you using myproduct.myproduct? What you need to achieve that roughly consists of doing this:and so on. Would it help if I said views.py doesn't have to be called views.py? Provided you can name, on the python path, a function (usually package.package.views.function_name) it will get handled. Simple as that. All this "project"/"app" stuff is just python packages.Now, how are you supposed to do it? Or rather, how might I do it? Well, if you create a significant piece of reusable functionality, like say a markup editor, that's when you create a "top level app" which might contain widgets.py, fields.py, context_processors.py etc - all things you might want to import.Similarly, if you can create something like a blog in a format that is pretty generic across installs, you can wrap it up in an app, with its own template, static content folder etc, and configure an instance of a django project to use that app's content.There are no hard and fast rules saying you must do this, but it is one of the goals of the framework. The fact that everything, templates included, allows you to include from some common base means your blog should fit snugly into any other setup, simply by looking after its own part.However, to address your actual concern, yes, nothing says you can't work with the top level project folder. That's what apps do and you can do it if you really want to. I tend not to, however, for several reasons:In short, the reason there is a convention is the same as any other convention - it helps when it comes to others working with your project. If I see fields.py I immediately expect code in it to subclass django's field, whereas if I see inputtypes.py I might not be so clear on what that means without looking at it.Once you graduate from using startproject and startapp, there's nothing to stop you from combining a "project" and "app" in the same Python package. A project is really nothing more than a settings module, and an app is really nothing more than a models module—everything else is optional.For small sites, it's entirely reasonable to have something like:I read this thought somewhere soon after I've started to work with django and I find that I ask this question of myself quite often and it helps me. Your apps don't have to be reusable, they can depend on each other, but they should do one thing.I've found the following blog posts very useful about django applications and projects: In principle, you have a lot of freedom with django for organizing the source code of your product. There is nothing like not allowed. Its your project, no one is restricting you. It is advisable to keep a reasonable name.In a general django project there are many apps (contrib apps) which are used really in every project. Let us say that your project does only one task and has only a single app (I name it main as thethe project revolves around it and is hardly pluggable). This project too still uses some other apps generally.   Now if you say that your project is using just the one app (INSTALLED_APPS='myproduct') so what is use of project defining the project as project.app, I think you should consider some points: As far as most of the work being done in the app is concerned, I think that is the case with most of django projects.Here Django creators points out that difference themselves.

I think that thinking about Apps as they have to be reusable in other projects is good. Also a good way of thinking about Apps in Django provide modern web applications. Imagine that you are creating big dynamic web app basing on JavaScript. You can create then in django App named e.g "FrontEnd" <-- in thins app you will display content.Then you create some backend Apps. E.g App named "Comments" that will store user comments. And "Comments" App will not display anything itself. It will be just API for AJAX requests of your dynamic JS website.In this way you can always reuse your "Comments" app. You can make it open source without opening source of whole project. And you keep clean logic of your project.

how to pick just one item from a generator (in python)?

Alexandros

[how to pick just one item from a generator (in python)?](https://stackoverflow.com/questions/4741243/how-to-pick-just-one-item-from-a-generator-in-python)

I have a generator function like the following:The usual way to call this function would be:My question, is there a way to get just one element from the generator whenever I like?

For example, I'd like to do something like:

2011-01-19 21:55:28Z

I have a generator function like the following:The usual way to call this function would be:My question, is there a way to get just one element from the generator whenever I like?

For example, I'd like to do something like:Create a generator usingEverytime you would like an item, use(or g.next() in Python 2.5 or below).If the generator exits, it will raise StopIteration.  You can either catch this exception if necessary, or use the default argument to next():For picking just one element of a generator use break in a for statement, or list(itertools.islice(gen, 1))According to your example (litterally) you can do like:If you want "get just one element from the [once generated] generator whenever I like"  (I suppose 50% thats the original intention, and the most common intention)  then:This way explicit use of generator.next() can be avoided, and end-of-input handling doesn't require (cryptic) StopIteration exception handling or extra default value comparisons.The else: of for statement section is only needed if you want do something special in case of end-of-generator.In Python3 the .next() method was renamed to .__next__() for good reason: its considered low-level (PEP 3114). Before Python 2.6 the builtin function next() did not exist. And it was even discussed to move next() to the operator module (which would have been wise), because of its rare need and questionable inflation of builtin names.Using next() without default is still very low-level practice - throwing the cryptic StopIteration like a bolt out of the blue in normal application code openly. And using next() with default sentinel - which best should be the only option for a next() directly in builtins - is limited and often gives reason to odd non-pythonic logic/readablity.Bottom line: Using next() should be very rare - like using functions of operator module. Using for x in iterator , islice, list(iterator) and other functions accepting an iterator seamlessly is the natural way of using iterators on application level - and quite always possible. next() is low-level, an extra concept, unobvious - as the question of this thread shows. While e.g. using break in for is conventional.I don't believe there's a convenient way to retrieve an arbitrary value from a generator.  The generator will provide a next() method to traverse itself, but the full sequence is not produced immediately to save memory.  That's the functional difference between a generator and a list.make sure to catch the exception thrown after the last element is takenGenerator is a function that produces an iterator. Therefore, once you have iterator instance, use next() to fetch the next item from the iterator.

As an example, use next() function to fetch the first item, and later use for in to process remaining items:For those of you scanning through these answers for a complete working example for Python3... well here ya go:This outputs:I believe the only way is to get a list from the iterator then get the element you want from that list.

Python in Xcode 4+?

Tyler Crompton

[Python in Xcode 4+?](https://stackoverflow.com/questions/5276967/python-in-xcode-4)

How does one create a Python friendly environment in Xcode 4, 5, 6 or 7?

2011-03-11 18:40:01Z

How does one create a Python friendly environment in Xcode 4, 5, 6 or 7?I figured it out! The steps make it look like it will take more effort than it actually does.These instructions are for creating a project from scratch. If you have existing Python scripts that you wish to include in this project, you will obviously need to slightly deviate from these instructions.If you find that these instructions no longer work or are unclear due to changes in Xcode updates, please let me know. I will make the necessary corrections.Note that if you open the "Utilities" panel, with the "Show the File inspector" tab active, the file type is automatically set to "Default - Python script". Feel free to look through all the file type options it has, to gain an idea as to what all it is capable of doing. The method above can be applied to any interpreted language. As of right now, I have yet to figure out exactly how to get it to work with Java; then again, I haven't done too much research. Surely there is some documentation floating around on the web about all of this.If you do not have administrative privileges or are not in the Developer group, you can still use Xcode for Python programming (but you still won't be able to develop in languages that require compiling). Instead of using the play button, in the menu bar, click "Product" → "Perform Action" → "Run Without Building" or simply use the keyboard shortcut ^⌘R.To change the text encoding, line endings, and/or indentation settings, open the "Utilities" panel and click "Show the File inspector" tab active. There, you will find these settings.For more information about Xcode's build settings, there is no better source than this. I'd be interested in hearing from somebody who got this to work with unsupported compiled languages. This process should work for any other interpreted language. Just be sure to change Step 5 and Step 16 accordingly.I've created Xcode 4 templates to simplify the steps provided by Tyler.

The result is Python Project Template for Xcode 4.Now what you need to do is download the templates, move it to /Developer/Library/Xcode/Templates/Project Templates/Mac/Others/ and then new a Python project with Xcode 4.It still needs manual Scheme setup (you can refer to steps 12–20 provided by Tyler.)Procedure to get Python Working in XCode 7Step 1: Setup your Project with a External Build SystemStep 1.1: Edit the Project SchemeStep 2: Specify Python as the executable for the project (shift-command-g)  the path should be /usr/bin/pythonStep 3: Specify your custom working directoryStep 4: Specify your command line arguments to be the name of your python file. (in this example "test.py")Step 5: Thankfully thats it!(debugging can't be added until OSX supports a python debugger?)You should try PyDev plug in for Eclipse. I tried alot of editors/IDE's to use with python, but the only one i liked the most is the PyDev plugin for Eclipse. It has code completion, debugger and many other nice features. Plus both are free.Another way, which I've been using for awhile in XCode3:See steps 1-15 above.The nice thing about this way is it will use the same environment to develop in that you would use to run in outside of XCode (as setup from your bash .profile).It's also generic enough to let you develop/run any type of file, not just python.This Technical Note TN2328 from Apple Developer Library helped me a lot about Changes To Embedding Python Using Xcode 5.0.This thread is old, but to chime in for Xcode Version 8.3.3, Tyler Crompton's method in the accepted answer still works (some of the names are very slightly different, but not enough to matter).2 points where I struggled slightly:Step 16: If the python executable you want is greyed out, right click it and select quick look. Then close the quick look window, and it should now be selectable.Step 19: If this isn’t working for you, you can enter the name of just the python file in the Arguments tab, and then enter the project root directory explicitly in the Options tab under Working Directory--check the「Use custom working directory」box, and type in your project root directory in the field below it.Try Editra It's free, has a lot of cool features and plug-ins, it runs on most platforms, and it is written in Python. I use it for all my non-XCode development at home and on Windows/Linux at work.

Why does Python code use len() function instead of a length method?

fuentesjr

[Why does Python code use len() function instead of a length method?](https://stackoverflow.com/questions/237128/why-does-python-code-use-len-function-instead-of-a-length-method)

I know that python has a len() function that is used to determine the size of a string, but I was wondering why it's not a method of the string object.Ok, I realized I was embarrassingly mistaken. __len__() is actually a method of a string object. It just seems weird to see object oriented code in Python using the len function on string objects. Furthermore, it's also weird to see __len__ as the name instead of just len.

2008-10-25 22:37:03Z

I know that python has a len() function that is used to determine the size of a string, but I was wondering why it's not a method of the string object.Ok, I realized I was embarrassingly mistaken. __len__() is actually a method of a string object. It just seems weird to see object oriented code in Python using the len function on string objects. Furthermore, it's also weird to see __len__ as the name instead of just len.Strings do have a length method: __len__()The protocol in Python is to implement this method on objects which have a length and use the built-in len() function, which calls it for you, similar to the way you would implement __iter__() and use the built-in iter() function (or have the method called behind the scenes for you) on objects which are iterable.See Emulating container types for more information.Here's a good read on the subject of protocols in Python: Python and the Principle of Least AstonishmentJim's answer to this question may help; I copy it here. Quoting Guido van Rossum:There is a len method:Python is a pragmatic programming language, and the reasons for len() being a function and not a method of str, list, dict etc. are pragmatic.The len() built-in function deals directly with built-in types: the CPython implementation of len() actually returns the value of the ob_size field in the PyVarObject C struct that represents any variable-sized built-in object in memory. This is much faster than calling a method -- no attribute lookup needs to happen. Getting the number of items in a collection is a common operation and must work efficiently for such basic and diverse types as str, list, array.array etc. However, to promote consistency, when applying len(o) to a user-defined type, Python calls o.__len__() as a fallback.  __len__, __abs__ and all the other special methods documented in the Python Data Model make it easy to create objects that behave like the built-ins, enabling the expressive and highly consistent APIs we call "Pythonic". By implementing special methods your objects can support iteration, overload infix operators, manage contexts in with blocks etc. You can think of the Data Model as a way of using the Python language itself as a framework where the objects you create can be integrated seamlessly.A second reason, supported by quotes from Guido van Rossum like this one, is that it is easier to read and write len(s) than s.len().The notation len(s) is consistent with unary operators with prefix notation, like abs(n). len() is used way more often than abs(), and it deserves to be as easy to write. There may also be a historical reason: in the ABC language which preceded Python (and was very influential in its design), there was a unary operator written as #s which meant len(s).There are some great answers here, and so before I give my own I'd like to highlight a few of the gems (no ruby pun intended) I've read here.If you don't like the way this works in your own code, it's trivial for you to re-implement the containers using your preferred method (see example below).You can also say Using Python 2.7.3.Something missing from the rest of the answers here: the len function checks that the __len__ method returns a non-negative int. The fact that len is a function means that classes cannot override this behaviour to avoid the check. As such, len(obj) gives a level of safety that obj.len() cannot.Example:Of course, it is possible to "override" the len function by reassigning it as a global variable, but code which does this is much more obviously suspicious than code which overrides a method in a class.It doesn't?

Repeat string to certain length

John Howard

[Repeat string to certain length](https://stackoverflow.com/questions/3391076/repeat-string-to-certain-length)

What is an efficient way to repeat a string to a certain length? Eg: repeat('abc', 7) -> 'abcabca'Here is my current code:Is there a better (more pythonic) way to do this? Maybe using list comprehension?

2010-08-02 19:26:44Z

What is an efficient way to repeat a string to a certain length? Eg: repeat('abc', 7) -> 'abcabca'Here is my current code:Is there a better (more pythonic) way to do this? Maybe using list comprehension?For python3:Jason Scheirer's answer is correct but could use some more exposition.First off, to repeat a string an integer number of times, you can use overloaded multiplication:So, to repeat a string until it's at least as long as the length you want, you calculate the appropriate number of repeats and put it on the right-hand side of that multiplication operator:Then, you can trim it to the exact length you want with an array slice:Alternatively, as suggested in pillmod's answer that probably nobody scrolls down far enough to notice anymore, you can use divmod to compute the number of full repetitions needed, and the number of extra characters, all at once:Which is better? Let's benchmark it:So, pillmod's version is something like 40% slower, which is too bad, since personally I think it's much more readable.  There are several possible reasons for this, starting with its compiling to about 40% more bytecode instructions.Note: these examples use the new-ish // operator for truncating integer division.  This is often called a Python 3 feature, but according to PEP 238, it was introduced all the way back in Python 2.2.  You only have to use it in Python 3 (or in modules that have from __future__ import division) but you can use it regardless.This is pretty pythonic:Perhaps not the most efficient solution, but certainly short & simple:Gives "foobarfoobarfo".  One thing about this version is that if length < len(string) then the output string will be truncated.  For example:Gives "foo".Edit: actually to my surprise, this is faster than the currently accepted solution (the 'repeat_to_length' function), at least on short strings:Presumably if the string was long, or length was very high (that is, if the wastefulness of the string * length part was high) then it would perform poorly.  And in fact we can modify the above to verify this:How about string * (length / len(string)) + string[0:(length % len(string))]i use this: Not that there haven't been enough answers to this question, but there is a repeat function; just need to make a list of and then join the output:Yay recursion!Won't scale forever, but it's fine for smaller strings. And it's pretty.I admit I just read the Little Schemer and I like recursion right now.This is one way to do it using a list comprehension, though it's increasingly wasteful as the length of the rpt string increases.Another FP aproach:

How to uninstall a package installed with pip install --user

Serjik

[How to uninstall a package installed with pip install --user](https://stackoverflow.com/questions/33412974/how-to-uninstall-a-package-installed-with-pip-install-user)

There is a --user option for pip which can install a Python package per user:I used this option to install a package on a server for which I do not have root access. What I need now is to uninstall the installed package on the current user. I tried to execute this command:But I got:How can I uninstall a package that I installed with pip install --user, other than manually finding and deleting the package?I've found this article pip cannot uninstall from per-user site-packages directorywhich describes that uninstalling packages from user directory does not supported. According to the article if it was implemented correctly then withthe package that was installed will be also searched in user directories. But a problem still remains for me. What if the same package was installed both system-wide and per-user?

What if someone needs to target a specific user directory?

2015-10-29 11:27:43Z

There is a --user option for pip which can install a Python package per user:I used this option to install a package on a server for which I do not have root access. What I need now is to uninstall the installed package on the current user. I tried to execute this command:But I got:How can I uninstall a package that I installed with pip install --user, other than manually finding and deleting the package?I've found this article pip cannot uninstall from per-user site-packages directorywhich describes that uninstalling packages from user directory does not supported. According to the article if it was implemented correctly then withthe package that was installed will be also searched in user directories. But a problem still remains for me. What if the same package was installed both system-wide and per-user?

What if someone needs to target a specific user directory?Having tested this using Python 3.5 and pip 7.1.2 on Linux, the situation appears to be this:example to uninstall package 'oauth2client' on MacOS:Be careful though, for those who using pip install --user some_pkg inside a virtual environment.In this case, you have to deactivate the current virtual environment, then use the corresponding python/pip executable to list or uninstall the user site packages:Note that this issue was reported few years ago. And it seems that the current conclusion is: --user is not valid inside a virtual env's pip, since a user location doesn't really make sense for a virtual environment.The answer is Not possible yet. You have to remove it manually.As @thomas-lotze has mentioned, currently pip tooling does not do that as there is no corresponding --user option.  But what I find is that I can check in ~/.local/bin and look for the specific pip#.# which looks to me like it corresponds to the --user option.In my case:And then just uninstall with the specific pip version.I think it's possible to uninstall packages installed with --user flag. This one worked for me;pip freeze --user | xargs pip uninstall -yFor python 3;pip3 freeze --user | xargs pip3 uninstall -yBut somehow these commands don't install setuptools and pip. After those commands (if you really want clean python) you may delete them with;pip uninstall setuptools && pip uninstall pipI am running Anaconda version 4.3.22 and a python3.6.1 environment, and had this problem. Here's the history and the fix:I did this into my python3.6 environment and got this error.Next, I tried downloading python3.6 and putting the python3.dll in the folder and in various folders. nothing changed. finally, this fixed it:    (the other conda-forge version is still installed) This left only the conda version, and that works in 3.6. working!

Make virtualenv inherit specific packages from your global site-packages

TheMeaningfulEngineer

[Make virtualenv inherit specific packages from your global site-packages](https://stackoverflow.com/questions/12079607/make-virtualenv-inherit-specific-packages-from-your-global-site-packages)

I'm looking for a way to make a virtualenv which will contain just some libraries (which i chose) of the base python installation.To be more concrete, I'm trying to import my matplotlib to virtualenv during the creation of virtualenv. It can't be installed efficiently with pip or easy_install since it misses some fortran compiler libs. The way i did it till now was to manually copy fromhowever this prevents the manully imported links to be registerd by yolk (which prints all currently available libs in virtualenv).So, is there a way to do a selective variant of the

2012-08-22 18:54:52Z

I'm looking for a way to make a virtualenv which will contain just some libraries (which i chose) of the base python installation.To be more concrete, I'm trying to import my matplotlib to virtualenv during the creation of virtualenv. It can't be installed efficiently with pip or easy_install since it misses some fortran compiler libs. The way i did it till now was to manually copy fromhowever this prevents the manully imported links to be registerd by yolk (which prints all currently available libs in virtualenv).So, is there a way to do a selective variant of theCreate the environment with virtualenv --system-site-packages . Then, activate the virtualenv and when you want things installed in the virtualenv rather than the system python, use pip install --ignore-installed or pip install -I . That way pip will install what you've requested locally even though a system-wide version exists. Your python interpreter will look first in the virtualenv's package directory, so those packages should shadow the global ones.You can use the --system-site-packages and then "overinstall" the specific stuff for your virtualenv. That way, everything you install into your virtualenv will be taken from there, otherwise it will be taken from your system.Install virtual env withand use pip install -U to install matplotlibYou can use virtualenv --clear. which won't install any packages, then install the ones you want.

Is there a difference between using a dict literal and a dict constructor?

maligree

[Is there a difference between using a dict literal and a dict constructor?](https://stackoverflow.com/questions/6610606/is-there-a-difference-between-using-a-dict-literal-and-a-dict-constructor)

Using PyCharm, I noticed it offers to convert a dict literal:into a dict constructor:Do these different approaches differ in some significant way?(While writing this question I noticed that using dict() it seems impossible to specify a numeric key .. d = {1: 'one', 2: 'two'} is possible, but, obviously, dict(1='one' ...) is not. Anything else?)

2011-07-07 12:29:41Z

Using PyCharm, I noticed it offers to convert a dict literal:into a dict constructor:Do these different approaches differ in some significant way?(While writing this question I noticed that using dict() it seems impossible to specify a numeric key .. d = {1: 'one', 2: 'two'} is possible, but, obviously, dict(1='one' ...) is not. Anything else?)I think you have pointed out the most obvious difference. Apart from that, the first doesn't need to lookup dict which should make it a tiny bit faster the second looks up dict in locals() and then globals() and the finds the builtin, so you can switch the behaviour by defining a local called dict for example although I can't think of anywhere this would be a good idea apart from maybe when debuggingLiteral is much faster, since it uses optimized BUILD_MAP and STORE_MAP opcodes rather than generic CALL_FUNCTION:They look pretty much the same on Python 3.2.As gnibbler pointed out, the first doesn't need to lookup dict, which should make it a tiny bit faster.These two approaches produce identical dictionaries, except, as you've noted, where the lexical rules of Python interfere.Dictionary literals are a little more obviously dictionaries, and you can create any kind of key, but you need to quote the key names.  On the other hand, you can use variables for keys if you need to for some reason:The dict() constructor gives you more flexibility because of the variety of forms of input it takes.  For example, you can provide it with an iterator of pairs, and it will treat them as key/value pairs.I have no idea why PyCharm would offer to convert one form to the other.One big difference with python 3.4 + pycharm is that the dict() constructor

produces a "syntax error" message if the number of keys exceeds 256.I prefer using the dict literal now.From python 2.7 tutorial:While:So both {} and dict() produce dictionary but provide a bit different ways of dictionary data initialization.I find the dict literal d = {'one': '1'} to be much more readable, your defining data, rather than assigning things values and sending them to the dict() constructor.On the other hand i have seen people mistype the dict literal as d = {'one', '1'} which in modern python 2.7+ will create a set.Despite this i still prefer to all-ways use the set literal because i think its more readable, personal preference i suppose.There is no dict literal to create dict-inherited classes, custom dict classes with additional methods. In such case custom dict class constructor should be used, for example:the dict() literal is nice when you are copy pasting values from something else (none python)

For example a list of environment variables.

if you had a bash file, say you can easily paste then into a dict() literal and add comments. It also makes it easier to do the opposite, copy into something else. Whereas the {'FOO': 'bar'} syntax is pretty unique to python and json. So if you use json a lot, you might want to use {} literals with double quotes.Also consider the fact that tokens that match for operators can't be used in the constructor syntax, i.e. dasherized keys.

Apply pandas function to column to create multiple new columns?

smci

[Apply pandas function to column to create multiple new columns?](https://stackoverflow.com/questions/16236684/apply-pandas-function-to-column-to-create-multiple-new-columns)

How to do this in pandas:I have a function extract_text_features on a single text column, returning multiple output columns. Specifically, the function returns 6 values.The function works, however there doesn't seem to be any proper return type (pandas DataFrame/ numpy array/ Python list) such that the output can get correctly assigned df.ix[: ,10:16] = df.textcol.map(extract_text_features)So I think I need to drop back to iterating with df.iterrows(), as per this?UPDATE: 

Iterating with df.iterrows() is at least 20x slower, so I surrendered and split out the function into six distinct .map(lambda ...) calls.UPDATE 2: this question was asked back around v0.11.0. Hence much of the question and answers are not too relevant.

2013-04-26 12:38:33Z

How to do this in pandas:I have a function extract_text_features on a single text column, returning multiple output columns. Specifically, the function returns 6 values.The function works, however there doesn't seem to be any proper return type (pandas DataFrame/ numpy array/ Python list) such that the output can get correctly assigned df.ix[: ,10:16] = df.textcol.map(extract_text_features)So I think I need to drop back to iterating with df.iterrows(), as per this?UPDATE: 

Iterating with df.iterrows() is at least 20x slower, so I surrendered and split out the function into six distinct .map(lambda ...) calls.UPDATE 2: this question was asked back around v0.11.0. Hence much of the question and answers are not too relevant.Building off of user1827356 's answer, you can do the assignment in one pass using df.merge:EDIT:

Please be aware of the huge memory consumption and low speed: https://ys-l.github.io/posts/2015/08/28/how-not-to-use-pandas-apply/ !I usually do this using zip:This is what I've done in the pastEditing for completenessThis is the correct and easiest way to accomplish this for 95% of use cases:Summary: If you only want to create a few columns, use df[['new_col1','new_col2']] = df[['data1','data2']].apply( function_of_your_choosing(x), axis=1)For this solution, the number of new columns you are creating must be equal to the number columns you use as input to the .apply() function. If you want to do something else, have a look at the other answers.Details

Let's say you have two-column dataframe. The first column is a person's height when they are 10; the second is said person's height when they are 20. Suppose you need to calculate both the mean of each person's heights and sum of each person's heights. That's two values per each row.You could do this via the following, soon-to-be-applied function:You might use this function like so:(To be clear: this apply function takes in the values from each row in the subsetted dataframe and returns a list.)However, if you do this:you'll create 1 new column that contains the [mean,sum] lists, which you'd presumably want to avoid, because that would require another Lambda/Apply.Instead, you want to break out each value into its own column. To do this, you can create two columns at once:For me this worked:Input dfFunctionCreate 2 new columns:   Output:In 2018, I use apply() with argument result_type='expand'Just use result_type="expand"I've looked several ways of doing this and the method shown here (returning a pandas series) doesn't seem to be most efficient.If we start with a largeish dataframe of random data:The example shown here:An alternative method:By my reckoning it's far more efficient to take a series of tuples and then convert that to a DataFrame. I'd be interested to hear people's thinking though if there's an error in my working.The accepted solution is going to be extremely slow for lots of data. The solution with the greatest number of upvotes is a little difficult to read and also slow with numeric data. If each new column can be calculated independently of the others, I would just assign each of them directly without using apply.Create 100,000 strings in a DataFrameLet's say we wanted to extract some text features as done in the original question. For instance, let's extract the first character, count the occurrence of the letter 'e' and capitalize the phrase.TimingsSurprisingly, you can get better performance by looping through each valueCreate 1 million random numbers and test the powers function from above.Assigning each column is 25x faster and very readable:I made a similar response with more details here on why apply is typically not the way to go.Have posted the same answer in two other similar questions. The way I prefer to do this is to wrap up the return values of the function in a series:And then use apply as follows to create separate columns:you can return the entire row instead of values:where the function returns the rowThis worked for me. New Column will be created with processed old column data.

Plot correlation matrix using pandas

Gaurav Singh

[Plot correlation matrix using pandas](https://stackoverflow.com/questions/29432629/plot-correlation-matrix-using-pandas)

I have a data set with huge number of features, so analysing the correlation matrix has become very difficult. I want to plot a correlation matrix which we get using dataframe.corr() function from pandas library. Is there any built-in function provided by the pandas library to plot this matrix?

2015-04-03 12:57:22Z

I have a data set with huge number of features, so analysing the correlation matrix has become very difficult. I want to plot a correlation matrix which we get using dataframe.corr() function from pandas library. Is there any built-in function provided by the pandas library to plot this matrix?You can use pyplot.matshow()  from matplotlib:Edit: In the comments was a request for how to change the axis tick labels. Here's a deluxe version that is drawn on a bigger figure size, has axis labels to match the dataframe, and a colorbar legend to interpret the color scale. I'm including how to adjust the size and rotation of the labels, and I'm using a figure ratio that makes the colorbar and the main figure come out the same height. If your main goal is to visualize the correlation matrix, rather than creating a plot per se, the convenient pandas styling options is a viable built-in solution:Note that this needs to be in a backend that supports rendering HTML, such as the JupyterLab Notebook. (The automatic light text on dark backgrounds is from an existing PR and not the latest released version, pandas 0.23).You can easily limit the digit precision:Or get rid of the digits altogether if you prefer the matrix without annotations:The styling documentation also includes instructions of more advanced styles, such as how to change the display of the cell the mouse pointer is hovering over. To save the output you could return the HTML by appending the render() method and then write it to a file (or just take a screenshot for less formal purposes).In my testing, style.background_gradient() was 4x faster than plt.matshow() and 120x faster than sns.heatmap() with a 10x10 matrix. Unfortunately it doesn't scale as well as plt.matshow(): the two take about the same time for a 100x100 matrix, and plt.matshow() is 10x faster for a 1000x1000 matrix. There are a few possible ways to save the stylized dataframe:By setting axis=None, it is now possible to compute the colors based on the entire matrix rather than per column or per row:Try this function, which also displays variable names for the correlation matrix:Seaborn's heatmap version:You can observe the relation between features either by drawing a heat map from seaborn or scatter matrix from pandas. Scatter Matrix:If you want to visualize each feature's skewness as well - use seaborn pairplots. Sns Heatmap:The output will be a correlation map of the features. i.e. see the below example. The correlation between grocery and detergents is high. Similarly:From Pairplots: You can observe same set of relations from pairplots or scatter matrix. But from these we can say that whether the data is normally distributed or not. Note: The above is same graph taken from the data, which is used to draw heatmap.You can use imshow() method from matplotlibIf you dataframe is df you can simply use:statmodels graphics also gives a nice view of correlation matrixFor completeness, the simplest solution i know with seaborn as of late 2019, if one is using Jupyter:Along with other methods it is also good to have pairplot which will give scatter plot for all the cases-Form correlation matrix, in my case zdf is the dataframe which i need perform correlation matrix.Then we can take screenshot. or convert html to an image file.

How to open a file for both reading and writing?

bigredhat

[How to open a file for both reading and writing?](https://stackoverflow.com/questions/6648493/how-to-open-a-file-for-both-reading-and-writing)

Is there a way to open a file for both reading and writing?As a workaround, I open the file for writing, close it, then open it again for reading. But is there a way to open a file for both reading and writing?

2011-07-11 09:59:26Z

Is there a way to open a file for both reading and writing?As a workaround, I open the file for writing, close it, then open it again for reading. But is there a way to open a file for both reading and writing?Here's how you read a file, and then write to it (overwriting any existing data), without closing and reopening:r+ is the canonical mode for reading and writing at the same time. This is not different from using the fopen() system call since file() / open() is just a tiny wrapper around this operating system call.I have tried something like this and it works as expected:Where:And:Also if you open Python tutorial about reading and writing files you will find that:Summarize the I/O behaviorsand the decision branch 

Is there shorthand for returning a default value if None in Python? [duplicate]

nfw

[Is there shorthand for returning a default value if None in Python? [duplicate]](https://stackoverflow.com/questions/13710631/is-there-shorthand-for-returning-a-default-value-if-none-in-python)

In C#, I can say x ?? "", which will give me x if x is not null, and the empty string if x is null. I've found it useful for working with databases. Is there a way to return a default value if Python finds None in a variable?

2012-12-04 19:42:31Z

In C#, I can say x ?? "", which will give me x if x is not null, and the empty string if x is null. I've found it useful for working with databases. Is there a way to return a default value if Python finds None in a variable?You could use the or operator:Note that this also returns "default" if x is any falsy value, including an empty list, 0, empty string, or even datetime.time(0) (midnight).try the above.You can use a conditional expression:Example:works best — i can even use a function call inline, without executing it twice or using extra variable:I use it when opening Qt's dialog.getExistingDirectory() and canceling, which returns empty string.You've got the ternary syntax x if x else '' - is that what you're after?

Is it possible to set a number to NaN or infinity?

Bob

[Is it possible to set a number to NaN or infinity?](https://stackoverflow.com/questions/5438745/is-it-possible-to-set-a-number-to-nan-or-infinity)

Is it possible to set an element of an array to NaN in Python?Additionally, is it possible to set a variable to +/- infinity? If so, is there any function to check whether a number is infinity or not?

2011-03-25 22:24:07Z

Is it possible to set an element of an array to NaN in Python?Additionally, is it possible to set a variable to +/- infinity? If so, is there any function to check whether a number is infinity or not?Cast from string using float():Yes, you can use numpy for that.Yes, in fact there are several ways. A few work without any imports, while others require import, however for this answer I'll limit the libraries in the overview to standard-library and NumPy (which isn't standard-library but a very common third-party library).The following table summarizes the ways how one can create a not-a-number or a positive or negative infinity float:A couple remarks to the table:Yes there is - in fact there are several functions for NaN, Infinity, and neither Nan nor Inf. However these predefined functions are not built-in, they always require an import:Again a couple of remarks:To summarize the expected results for these functions (assuming the input is a float):In a list it's no problem, you can always include NaN (or Infinity) there:However if you want to include it in an array (for example array.array or numpy.array) then the type of the array must be float or complex because otherwise it will try to downcast it to the arrays type!When using Python 2.4, tryI am facing the issue when I was porting the simplejson to an embedded device which running the Python 2.4, float("9e999") fixed it. Don't use inf = 9e999, you need convert it from string.

-inf gives the -Infinity.

How to execute Python code from within Visual Studio Code

RPiAwesomeness

[How to execute Python code from within Visual Studio Code](https://stackoverflow.com/questions/29987840/how-to-execute-python-code-from-within-visual-studio-code)

Visual Studio Code was recently released and I liked the look of it and the features it offered, so I figured I would give it a go. I downloaded the application from the downloads page

 fired it up, messed around a bit with some of the features ... and then realized I had no idea how to actually execute any of my Python code!I really like the look and feel/usability/features of Visual Studio Code, but I can't seem to find out how to run my Python code, a real killer because that's what I program primarily in.Does anyone know if there is a way to execute Python code in Visual Studio Code?

2015-05-01 13:35:17Z

Visual Studio Code was recently released and I liked the look of it and the features it offered, so I figured I would give it a go. I downloaded the application from the downloads page

 fired it up, messed around a bit with some of the features ... and then realized I had no idea how to actually execute any of my Python code!I really like the look and feel/usability/features of Visual Studio Code, but I can't seem to find out how to run my Python code, a real killer because that's what I program primarily in.Does anyone know if there is a way to execute Python code in Visual Studio Code?Here is how to Configure Task Runner in Visual Studio Code to run a py file.In your console press Ctrl+Shift+P (Windows) or Cmd+Shift+P (Apple) and this brings up a search box where you search for "Configure Task Runner"

EDIT: If this is the first time you open the "Task: Configure Task Runner", you need to select "other" at the bottom of the next selection list.This will bring up the properties which you can then change to suit your preference. In this case you want to change the following properties;You can now open your py file and run it nicely with the shortcut Ctrl+Shift+B (Windows) or Cmd+Shift+B (Apple) Enjoy!There is a much easier way to run Python, no any configuration needed:If you want to add Python path, you could Go to File->Preference->Settings, and add Python path like below:In case you have installed Python extension and manually set your interpreter already, you could config your settings.json file as following:You can add a custom task to do this. Here is a basic custom task for Python.You add this to tasks.json and press CTRL + SHIFT + B to run it.All these answers are obsolete now.Currently you have to:No additional extensions or manual launch.json editing is required now.To extend @vlad2135's answer (read his first); that is how you set up python debugging in VSCode with Don Jayamanne's great python extension (Which is a pretty full featured IDE for python these days, and arguably one of VS code's best language extensions IMO).Basically when you click the gear icon, it creates a launch.json in your .vscode directory in your workspace. You can also make this yourself, but it's probably just simpler to let VSCode do the heavy lifting. Here's an example file:You'll notice something cool after you generate it. It automatically created a bunch of configurations (most of mine are cut off, just scroll to see them all) with different settings and extra features for different libraries or environments (like django). The one you'll probably end up using the most is python; which is a plain (in my case C) Python debugger, and easiest to work with settings wise. I'll make a short walkthrough of the json attributes for this one, since the others use the pretty much same configuration with only different interpreter paths and one or two different other features there.You can go here for more information on the VSCode file variables you can use to configure your debuggers and paths.You can go here for the extension's own documentation on launch options, with both optional and required attributes. You can click the "Add Configuration" button at the bottom right if you don't see the config template already in the file. It'll give you a list to auto generate a configuration for most of the common debug processes out there.Now, as per vlad's answer, you may add any breakpoints you need as per normal visual debuggers, choose which run configuration you want in the top left dropdown menu and you can tap the green arrow to the left to the configuration name to start your program.Pro tip: Different people on your team use different IDE's and they probably don't need your configurations files. VSCode nearly always puts it's IDE files in one place (by design for this purpose; I assume), launch or otherwise so make sure to add .vscode/to your .gitignore if this is your first time generating a VSCode file (This process will create the folder in your workspace if you don't have it already)!There is a Run Python File in Terminal command available in the Python for VS Code extension.As stated in Visualstudio Code Documentation, just right-click anywhere in the editor and select Run Python File in Terminal.So there're 4 ways to run Python in VSCode so far:  Now this is the additional step Actually I got irritated of clicking again and again so I setup the Keyboard Shortcut.You no longer need any additional extensions. You can simply switch the output of the debugger to the integrated terminal.Ctrl+Shift+D, then select Integrated Terminal/Console from the dropdown at the top.Here's the current (September 2018) extensions for running python:Official python extension: This is a must install.Code Runner: Increadibly useful for all sorts of languages, not just python. Would highly reccomend installing.AREPL: Real-time python scratchpad that displays your variables in a side window.  I'm the creator of this so obviously I think it's great but I can't give a unbiased opinion ¯\_(ツ)_/¯Wolf: Real-time python scratchpad that displays results inlineAnd of course if you use the integrated terminal you can run python in there and not have to install any extensions.Super simple:

Press F5 key and the code will run. 

If a breakpoint is set, pressing F5 will stop at the breakpoint and run the code in Debug mode.In the latest version (1.36) of VS Code (Python):Press F5 then hit Enter to run your code in the integrated terminal.CTRL+A then hit SHIFT+Enter to run your code in interactive IPython Shell.If you are using the latest version of vs code (version 1.21.1). The task.json format has changed, see here. So the answer by @Fenton and @python_starter may no longer be valid.Before you start configuring vs code for running your python file. Now you can configure the task. The following steps will help you run your python file correctly:

For a more complete tutorial about task configuration, go to vs code official documentation.There is a lot of confusion around VSCode Tasks and Debugger. Let's discuss about it first so that we understand when to use Tasks and when to use Debugger. The official documentation says -So, Tasks are not for debugging, compiling or executing our programs.If we check the debugger documentation, we will find there is something called run mode. It says -So, Press F5 and VS Code will try to debug your currently active file.Press Ctrl+F5 and VSCode will ignore your breakpoints and run the code.To configure the debugger, go through the documentation. In summary it says, you should modify the launch.json file. For starters, to run the code in integrated terminal (inside VS Code), use -To run the code in external terminal (outside of VS Code), use -N.B. If all documentations were easy to search and understand then we probably would not need stackoverflow. Fortunately, The documentations I mentioned in this post are really easy to understand. Please feel free to read, ponder and enjoy.A simple and direct Python extension would save both time and efforts.

Linting, debugging, code completion are the available features once installation is done. After this, to run the code proper Python installation path needs to be configured in order to run the code. General settings are available in User scope and Workspace can be configured for Python language– "python.pythonPath": "c:/python27/python.exe" 

With above steps at least the basic Python programs can be executed.From Extension install Code Runner. After that you can use the shortcuts to run your source code in Visual Studio Code.First: To run code:Second: To stop the running code:I use Python 3.7 (32 bit). To run a program in Visual Studio Code, I right-click on the program and select "Run Current File in Python Interactive Window". If you do not have Jupyter, you may be asked to install it.If you are running a code and want to take input via running your program in the terminal. best thing to do is to run it in terminal directly by just right click and choose "Run python file in terminal".

 in order to launch the current file with respective venv i added this to launch.jsonin the bin folder resides the source .../venv/bin/activate script regularly sourced when running from regular terminalI had installed python via Anaconda. 

By starting VS code via anaconda I was able to run python programs. 

However, I couldn't find any shortcut way (hotkey) to directly run .py files.(using the latest version as of Feb 21st 2019 with the Python extension which came with VS Code.

Link: https://marketplace.visualstudio.com/items?itemName=ms-python.python )Following worked: The below is similar to what @jdhao did.This is what I did to get the hotkey:I made the code look like this:After saving it, the file changed to this:Now every time that you press CTRL + SHIFT + B, the python file will automatically run and show you the output :)If you have a project consisting of multiple python files and you want to start running/debugging with the main program independent of which file is current you create the following launch configuration (change MyMain.py to your main file)If I just want to run the Python file in the terminal, I'll make a keyboard shortcut for the command because there isn't one by default (you need to have python in your path):I use ctrl+alt+n

Clear variable in python

Bnicholas

[Clear variable in python](https://stackoverflow.com/questions/8237647/clear-variable-in-python)

Is there a way to clear the value of a variable in python?For example if I was implementing a binary tree:If I wanted to remove some node from the tree, I would need to set self.left to empty.

2011-11-23 05:17:26Z

Is there a way to clear the value of a variable in python?For example if I was implementing a binary tree:If I wanted to remove some node from the tree, I would need to set self.left to empty.What's wrong with self.left = None?The del keyword would do.But in this case I vote for self.left = Nonevar = None "clears the value", setting the value of the variable to "null" like value of "None", however the pointer to the variable remains. del var removes the definition for the variable totally. In case you want to use the variable later, e.g. set a new value for it, i.e. retain the variable, None would be better.Actually, that does not delete the variable/property. All it will do is set its value to None, therefore the variable will still take up space in memory. If you want to completely wipe all existence of the variable from memory, you can just type:Then your_variable will be emptyI used a few options mentioned above : or setting value to None using It's important to know the differences and put a few exception handlers in place when you use set the value to None. If you're printing the value of the conditional statements using a template, say, you might see the value of the variable printing "The value of the variable is None". Thus, you'd have to put a few exception handlers there :The above command will only print values if self.left is not None

TypeError: Missing 1 required positional argument: 'self'

DominicM

[TypeError: Missing 1 required positional argument: 'self'](https://stackoverflow.com/questions/17534345/typeerror-missing-1-required-positional-argument-self)

I am new to python and have hit a wall. I followed several tutorials but cant get past the error:I examined several tutorials but there doesn't seem to be anything different from my code. The only thing I can think of is that python 3.3 requires different syntax.main scipt:Pump class:If I understand correctly "self" is passed to the constructor and methods automatically. What am I doing wrong here?I am using windows 8 with python 3.3.2

2013-07-08 19:21:42Z

I am new to python and have hit a wall. I followed several tutorials but cant get past the error:I examined several tutorials but there doesn't seem to be anything different from my code. The only thing I can think of is that python 3.3 requires different syntax.main scipt:Pump class:If I understand correctly "self" is passed to the constructor and methods automatically. What am I doing wrong here?I am using windows 8 with python 3.3.2You need to instantiate a class instance here.UseSmall example - You need to initialize it first:Works and is simpler than every other solution I see here :This is great if you don't need to reuse a class instance. Tested on Python 3.7.3.You can also get this error by prematurely taking PyCharm's advice to annotate a method @staticmethod.  Remove the annotation.The 'self' keyword in python is analogous to 'this' keyword in c++ / java / c#.In python 2 it is done implicitly by the compiler (yes python does compilation internally). 

It's just that in python 3 you need to mention it explicitly in the constructor and member functions. example:

How bad is shadowing names defined in outer scopes?

Framester

[How bad is shadowing names defined in outer scopes?](https://stackoverflow.com/questions/20125172/how-bad-is-shadowing-names-defined-in-outer-scopes)

I just switched to Pycharm and I am very happy about all the warnings and hints it provides me to improve my code. Except for this one which I don't understand:This inspection detects shadowing names defined in outer scopes.I know it is bad practice to access variable from the outer scope but what is the problem with shadowing the outer scope?Here is one example, where Pycharm gives me the warning message:

2013-11-21 15:35:11Z

I just switched to Pycharm and I am very happy about all the warnings and hints it provides me to improve my code. Except for this one which I don't understand:This inspection detects shadowing names defined in outer scopes.I know it is bad practice to access variable from the outer scope but what is the problem with shadowing the outer scope?Here is one example, where Pycharm gives me the warning message:No big deal in your above snippet, but imagine a function with a few more arguments and quite a few more lines of code. Then you decide to rename your data argument as yadda but miss one of the places it is used in the function's body... Now data refers to the global, and you start having weird behaviour - where you would have a much more obvious NameError if you didn't have a global name data. Also remember that in Python everything is an object (including modules, classes and functions) so there's no distinct namespaces for functions, modules or classes. Another scenario is that you import function foo at the top of your module, and use it somewhere in your function body. Then you add a new argument to your function and named it - bad luck - foo. Finally, built-in functions and types also live in the same namespace and can be shadowed the same way.None of this is much of a problem if you have short functions, good naming and a decent unittest coverage, but well, sometimes you have to maintain less than perfect code and being warned about such possible issues might help.The currently most up-voted and accepted answer and most answers here miss the point.It doesn't matter how long your function is, or how you name your variable descriptively (to hopefully minimize the chance of potential name collision).The fact that your function's local variable or its parameter happens to share a name in the global scope is completely irrelevant. And in fact, no matter how carefully you choose you local variable name, your function can never foresee "whether my cool name yadda will also be used as a global variable in future?". The solution? Simply don't worry about that! The correct mindset is to design your function to consume input from and only from its parameters in signature, that way you don't need to care what is (or will be) in global scope, and then shadowing becomes not an issue at all.In other words, shadowing problem only matters when your function need to use the same name local variable AND the global variable. But you should avoid such design in the first place. The OP's code does NOT really have such design problem. It is just that PyCharm is not smart enough and it gives out a warning just in case. So, just to make PyCharm happy, and also make our code clean, see this solution quoting from silyevsk 's answer to remove the global variable completely.This is the proper way to "solve" this problem, by fixing/removing your global thing, not adjusting your current local function.A good workaround in some cases may be to move the vars + code to another function:Do this:It depends how long the function is. The longer the function, the more chance that someone modifying it in future will write data thinking that it means the global. In fact it means the local but because the function is so long it's not obvious to them that there exists a local with that name.For your example function, I think that shadowing the global is not bad at all.It looks like it 100% pytest code patternsee:https://docs.pytest.org/en/latest/fixture.html#conftest-py-sharing-fixture-functionsI had the same problem with, this is why I found this post ;)And it will warn with This inspection detects shadowing names defined in outer scopes.To fix that just move your twitter fixture into ./tests/conftest.pyAnd remove twitter fixture like in ./tests/test_twitter2.pyThis will be make happy QA, Pycharm and everyoneI like to see a green tick in the top right corner in pycharm. I append the variable names with an underscore just to clear this warning so I can focus on the important warnings.

Check if item is in an array / list

SomeKittens

[Check if item is in an array / list](https://stackoverflow.com/questions/11251709/check-if-item-is-in-an-array-list)

If I've got an array of strings, can I check to see if a string is in the array without doing a for loop?  Specifically, I'm looking for a way to do it within an if statement, so something like this:

2012-06-28 19:39:16Z

If I've got an array of strings, can I check to see if a string is in the array without doing a for loop?  Specifically, I'm looking for a way to do it within an if statement, so something like this:Assuming you mean "list" where you say "array", you can doThis works for any collection, not just for lists. For dictionaries, it checks whether the given key is present in the dictionary.I'm also going to assume that you mean "list" when you say "array." Sven Marnach's solution is good. If you are going to be doing repeated checks on the list, then it might be worth converting it to a set or frozenset, which can be faster for each check. Assuming your list of strs is called subjects:Use a lambda function.Let's say you have an array:Check whether 5 is in nums:This solution is more robust. You can now check whether any number satisfying a certain condition is in your array nums.For example, check whether any number that is greater than or equal to 5 exists in nums:You have to use .values for arrays. 

for example say you have dataframe which has a column name ie, test['Name'], you can dofor a normal list you dont have to use .valuesYou can also use the same syntax for an array. For example, searching within a Pandas series:

How to install Python package from GitHub? [duplicate]

Colonel Panic

[How to install Python package from GitHub? [duplicate]](https://stackoverflow.com/questions/15268953/how-to-install-python-package-from-github)

I want to use a new feature of httpie. This feature is in the github repo https://github.com/jkbr/httpie but not in the release on the python package index https://pypi.python.org/pypi/httpie How can I install the httpie package from the github repo? I triedBut I got  an error 'could not unpack'In Nodejs, I can install packages from github like this

2013-03-07 10:39:33Z

I want to use a new feature of httpie. This feature is in the github repo https://github.com/jkbr/httpie but not in the release on the python package index https://pypi.python.org/pypi/httpie How can I install the httpie package from the github repo? I triedBut I got  an error 'could not unpack'In Nodejs, I can install packages from github like thisYou need to use the proper git URL:orAlso see the VCS Support section of the pip documentation.Don’t forget to include the egg=<projectname> part to explicitly name the project; this way pip can track metadata for it without having to have run the setup.py script.To install Python package from github, you need to clone that repository.Then just run the setup.py file from that directory,

How do I get Flask to run on port 80?

quantumtremor

[How do I get Flask to run on port 80?](https://stackoverflow.com/questions/20212894/how-do-i-get-flask-to-run-on-port-80)

I have a Flask server running through port 5000, and it's fine. I can access it at http://example.com:5000But is it possible to simply access it at http://example.com? I'm assuming that means I have to change the port from 5000 to 80. But when I try that on Flask, I get this error message when I run it.Running lsof -i :80 returns Do I need to kill these processes first? Is that safe? Or is there another way to keep Flask running on port 5000 but have the main website domain redirect somehow?

2013-11-26 09:22:46Z

I have a Flask server running through port 5000, and it's fine. I can access it at http://example.com:5000But is it possible to simply access it at http://example.com? I'm assuming that means I have to change the port from 5000 to 80. But when I try that on Flask, I get this error message when I run it.Running lsof -i :80 returns Do I need to kill these processes first? Is that safe? Or is there another way to keep Flask running on port 5000 but have the main website domain redirect somehow?So it's throwing up that error message because you have apache2 running on port 80.If this is for development, I would just leave it as it is on port 5000.If it's for production either:Not RecommendedNot recommended as it states in the documentation:RecommendedThis way, apache2 can handle all your static files (which it's very good at - much better than the debug server built into Flask) and act as a reverse proxy for your dynamic content, passing those requests to Flask.Here's a link to the official documentation about setting up Flask with Apache + mod_wsgi.Edit 1 - Clarification for @DjackWhen a request comes to the server on port 80 (HTTP) or port 443 (HTTPS) a web server like Apache or Nginx handles the connection of the request and works out what to do with it. In our case a request received should be configured to be passed through to Flask on the WSGI protocol and handled by the Python code. This is the "dynamic" part. There are a few advantages to configuring your web server like the above; 1- Stop other applications that are using port 80.

2- run application with port 80 :For externally visible server, where you don't use apache or other web server you just type If you use the following to change the port or host:use the following code to start the server (my main entrance for flask is app.py):instead of using:If you want your application on same port i.e port=5000 then just in your terminal run this command:and then run:If you want to run on a specified port, e.g. if you want to run on port=80, in your main file just mention this:Your issue is, that you have an apache webserver already running that is already using port 80. So, you can either:A convinient way is using the package python-dotenv:

It reads out a .flaskenv file where you can store environment variables for flask.Inside the file you specify:After that you just have to run your app with flask run and can access your app at that port.Please note that FLASK_RUN_HOST defaults to 127.0.0.1 and FLASK_RUN_PORT defaults to 5000.I had to set FLASK_RUN_PORT in my environment to the specified port number. Next time you start your app, Flask will load that environment variable with the port number you selected.You don't need to change port number for your application, just configure your www server (nginx or apache) to proxy queries to flask port. Pay attantion on uWSGI.set the port with app.run(port=80,debug=True)

you should set debug to true when on devEasiest and Best SolutionSave your .py file in a folder. This case my folder name is test. In the command prompt run the following----------------- Following will be returned ----------------Now on your browser type: http://127.0.0.1:8000. ThanksOn my scenario the following steps worked like a charm:

Python argparse ignore unrecognised arguments

jdborg

[Python argparse ignore unrecognised arguments](https://stackoverflow.com/questions/12818146/python-argparse-ignore-unrecognised-arguments)

Optparse, the old version just ignores all unrecognised arguments and carries on.  In most situations, this isn't ideal and was changed in argparse.  But there are a few situations where you want to ignore any unrecognised arguments and parse the ones you've specified.For example:Is there anyway to overwrite this? 

2012-10-10 11:22:37Z

Optparse, the old version just ignores all unrecognised arguments and carries on.  In most situations, this isn't ideal and was changed in argparse.  But there are a few situations where you want to ignore any unrecognised arguments and parse the ones you've specified.For example:Is there anyway to overwrite this? Replacewith For example,You can puts the remaining parts into a new argument with parser.add_argument('args', nargs=argparse.REMAINDER) if you want to use them.Actually argparse does still "ignore" _unrecognized_args. As long as these "unrecognized" arguments don't use the default prefix you will hear no complaints from the parser.Using @anutbu's configuration but with the standard parse.parse_args(), if we were to execute our program with the following arguments.We will have this Namespaced data collection to work with. If we wanted the default prefix - ignored we could change the ArgumentParser and decide we are going to use a + for our "recognized" arguments instead.The same command will produce Put that in your pipe and smoke it =)nJoy!

What are the differences between Pandas and NumPy+SciPy in Python? [closed]

Wes McKinney

[What are the differences between Pandas and NumPy+SciPy in Python? [closed]](https://stackoverflow.com/questions/11077023/what-are-the-differences-between-pandas-and-numpyscipy-in-python)

They both seem exceedingly similar and I'm curious as to which package would be more beneficial for financial data analysis. 

2012-06-18 04:45:48Z

They both seem exceedingly similar and I'm curious as to which package would be more beneficial for financial data analysis. Indeed, pandas provides high level data manipulation tools built on top of NumPy. NumPy by itself is a fairly low-level tool, and will be very much similar to using MATLAB. pandas on the other hand provides rich time series functionality, data alignment, NA-friendly statistics, groupby, merge and join methods, and lots of other conveniences. It has become very popular in recent years in financial applications. I will have a chapter dedicated to financial data analysis using pandas in my upcoming book. Numpy is required by pandas (and by virtually all numerical tools for Python).  Scipy is not strictly required for pandas but is listed as an "optional dependency".  I wouldn't say that pandas is an alternative to Numpy and/or Scipy.  Rather, it's an extra tool that provides a more streamlined way of working with numerical and tabular data in Python.  You can use pandas data structures but freely draw on Numpy and Scipy functions to manipulate them.Pandas offer a great way to manipulate tables, as you can make binning easy (binning a dataframe in pandas in Python) and calculate statistics. Other thing that is great in pandas is the Panel class that you can join series of layers with different properties and combine it using groupby function.

How to create a custom string representation for a class object?

Björn Pollex

[How to create a custom string representation for a class object?](https://stackoverflow.com/questions/4932438/how-to-create-a-custom-string-representation-for-a-class-object)

Consider this class:The default string representation looks something like this:How can I make this display a custom string?

2011-02-08 11:27:11Z

Consider this class:The default string representation looks something like this:How can I make this display a custom string?Implement __str__() or __repr__() in the class's metaclass.Use __str__ if you mean a readable stringification, use __repr__ for unambiguous representations.If you have to choose between __repr__ or __str__ go for the first one, as by default implementation __str__ calls __repr__ when it wasn't defined. Custom Vector3 example:In this example, repr returns again a string that can be directly consumed/executed, whereas str is more useful as a debug output. Ignacio Vazquez-Abrams' approved answer is quite right. It is, however, from the Python 2 generation. An update for the now-current Python 3 would be:If you want code that runs across both Python 2 and Python 3, the six module has you covered:Finally, if you have one class that you want to have a custom static repr, the class-based approach above works great. But if you have several, you'd have to generate a metaclass similar to MC for each, and that can get tiresome. In that case, taking your metaprogramming one step further and creating a metaclass factory makes things a bit cleaner:prints:Metaprogramming isn't something you generally need everyday—but when you need it, it really hits the spot!Just adding to all the fine answers, my version with decoration:stdout:The down sides:

Python None comparison: should I use「is」or ==?

Clay Wardell

[Python None comparison: should I use「is」or ==?](https://stackoverflow.com/questions/14247373/python-none-comparison-should-i-use-is-or)

I am using Python 2.x.My editor warns me when I compare my_var == None, but no warning when I use my_var is None.  I did a test in the Python shell and determined both are valid syntax, but my editor seems to be saying that my_var is None is preferred. Is this the case, and if so, why?

2013-01-09 22:06:58Z

I am using Python 2.x.My editor warns me when I compare my_var == None, but no warning when I use my_var is None.  I did a test in the Python shell and determined both are valid syntax, but my editor seems to be saying that my_var is None is preferred. Is this the case, and if so, why?Use is when you want to check against an object's identity (e.g. checking to see if var is None).  Use == when you want to check equality (e.g. Is var equal to 3?).You can have custom classes where my_var == None will return Truee.g:is checks for object identity.  There is only 1 object None, so when you do my_var is None, you're checking whether they actually are the same object (not just equivalent objects) In other words, == is a check for equivalence (which is defined from object to object) whereas is checks for object identity:is is generally preferred when comparing arbitrary objects to singletons like None because it is faster and more predictable. is always compares by object identity, whereas what == will do depends on the exact type of the operands and even on their ordering.This recommendation is supported by PEP 8, which explicitly states that "comparisons to singletons like None should always be done with is or is not, never the equality operators."PEP 8 defines that it is better to use the is operator when comparing singletons.

How to remove specific elements in a numpy array

Daniel Thaagaard Andreasen

[How to remove specific elements in a numpy array](https://stackoverflow.com/questions/10996140/how-to-remove-specific-elements-in-a-numpy-array)

How can I remove some specific elements from a numpy array? Say I haveI then want to remove 3,4,7 from a. All I know is the index of the values (index=[2,3,6]).

2012-06-12 11:54:37Z

How can I remove some specific elements from a numpy array? Say I haveI then want to remove 3,4,7 from a. All I know is the index of the values (index=[2,3,6]).Use numpy.delete() - returns a new array with sub-arrays along an axis deletedFor your specific question:Note that numpy.delete() returns a new array since array scalars are immutable, similar to strings in Python, so each time a change is made to it, a new object is created. I.e., to quote the delete() docs:If the code I post has output, it is the result of running the code.There is a numpy built-in function to help with that.A Numpy array is immutable, meaning you technically cannot delete an item from it. However, you can construct a new array without the values you don't want, like this:To delete by value : Not being a numpy person, I took a shot with:According to my tests, this outperforms numpy.delete(). I don't know why that would be the case, maybe due to the small size of the initial array?That's a pretty significant difference (in the opposite direction to what I was expecting), anyone have any idea why this would be the case?Even more weirdly, passing numpy.delete() a list performs worse than looping through the list and giving it single indices.Edit: It does appear to be to do with the size of the array. With large arrays, numpy.delete() is significantly faster.Obviously, this is all pretty irrelevant, as you should always go for clarity and avoid reinventing the wheel, but I found it a little interesting, so I thought I'd leave it here.If you don't know the index, you can't use logical_andUsing np.delete is the way to do it, as was answered. However, for completes, let me add another way of removing array elements, by specifying the elements or their indices:Remove by indices:Or remove by elements:Remove specific index(i removed 16 and 21 from matrix)Output:You can also use sets:

In pytest, what is the use of conftest.py files?

Aviv Cohn

[In pytest, what is the use of conftest.py files?](https://stackoverflow.com/questions/34466027/in-pytest-what-is-the-use-of-conftest-py-files)

I recently discovered pytest. It seems great. However, I feel the documentation could be better.I'm trying to understand what conftest.py files are meant to be used for.In my (currently small) test suite I have one conftest.py file at the project root. I use it to define the fixtures that I inject into my tests.I have two questions:More generally, how would you define the purpose and correct use of conftest.py file(s) in a py.test test suite?

2015-12-25 20:08:20Z

I recently discovered pytest. It seems great. However, I feel the documentation could be better.I'm trying to understand what conftest.py files are meant to be used for.In my (currently small) test suite I have one conftest.py file at the project root. I use it to define the fixtures that I inject into my tests.I have two questions:More generally, how would you define the purpose and correct use of conftest.py file(s) in a py.test test suite?Yes it is. Fixtures are a potential and common use of conftest.py. The 

fixtures that you will define will be shared among all tests in your test suite. However, defining fixtures in the root conftest.py might be useless and it would slow down testing if such fixtures are not used by all tests. Yes it does.Yes you can and it is strongly recommended if your test structure is somewhat complex. conftest.py files have directory scope. Therefore, creating targeted fixtures and helpers is good practice.Several cases could fit:Creating a set of tools or hooks for a particular group of tests.root/mod/conftest.pyLoading a set of fixtures for some tests but not for others.root/mod/conftest.pyroot/mod2/conftest.pyroot/mod2/test.pyWill print "some other stuff".Overriding hooks inherited from the root conftest.py.root/mod/conftest.pyroot/conftest.pyBy running any test inside root/mod, only "I am mod" is printed. You can read more about conftest.py here.EDIT:You can use conftest.py to define your helpers. However, you should follow common practice. Helpers can be used as fixtures at least in pytest. For example in my tests I have a mock redis helper which I inject into my tests this way.root/helper/redis/redis.pyroot/tests/stuff/conftest.pyroot/tests/stuff/test.pyThis will be a test module that you can freely import in your tests. NOTE that you could potentially name redis.py as conftest.py if your module redis contains more tests. However, that practice is discouraged because of ambiguity.If you want to use conftest.py, you can simply put that helper in your root conftest.py and inject it when needed. root/tests/conftest.pyroot/tests/stuff/test.pyAnother thing you can do is to write an installable plugin. In that case your helper can be written anywhere but it needs to define an entry point to be installed in your and other potential test frameworks. See this.If you don't want to use fixtures, you could of course define a simple helper and just use the plain old import wherever it is needed.root/tests/helper/redis.pyroot/tests/stuff/test.pyHowever, here you might have problems with the path since the module is not in a child folder of the test. You should be able to overcome this (not tested) by adding an __init__.py to your helper root/tests/helper/__init__.pyOr simply adding the helper module to your PYTHONPATH.In a wide meaning conftest.py is a local per-directory plugin. Here you define directory-specific hooks and fixtures. In my case a have a root directory containing project specific tests directories. Some common magic is stationed in 'root' conftest.py. Project specific - in their own ones. Can't see anything bad in storing fixtures in conftest.py unless they are not used widely (In that case I prefer to define them in test files directly)Yes, a fixture is usually used to get data ready for multiple tests.Yes, a fixture is a function that is run by pytest before, and sometimes

after, the actual test functions. The code in the fixture can do whatever you

want it to. For instance, a fixture can be used to get a data set for the tests to work on, or a fixture can also be used to get a system into a known state before running a test.First, it is possible to put fixtures into individual test files. However, to share fixtures among multiple test files, you need to use a conftest.py file somewhere centrally located for all of the tests. Fixtures can be shared by any test. They can be put in individual test files if you want the fixture to only be used by tests in that file.Second, yes, you can have other conftest.py files in subdirectories of the top tests directory. If you do, fixtures defined in these lower-level conftest.py files will be available to tests in that directory and subdirectories.Finally, putting fixtures in the conftest.py file at the test root will make them available in all test files.

Does Conda replace the need for virtualenv?

Johan

[Does Conda replace the need for virtualenv?](https://stackoverflow.com/questions/34398676/does-conda-replace-the-need-for-virtualenv)

I recently discovered Conda after I was having trouble installing SciPy, specifically on a Heroku app that I am developing.With Conda you create environments, very similar to what virtualenv does. My questions are:

2015-12-21 15:03:54Z

I recently discovered Conda after I was having trouble installing SciPy, specifically on a Heroku app that I am developing.With Conda you create environments, very similar to what virtualenv does. My questions are:For example:lists all installed packages in your current environment.

Conda-installed packages show up like this:and the ones installed via pip have the <pip> marker:Short answer is, you only need conda.Here is a link to the conda page comparing conda, pip and virtualenv: https://docs.conda.io/projects/conda/en/latest/commands.html#conda-vs-pip-vs-virtualenv-commands.Virtual Environments and pipI will add that creating and removing conda environments is simple with Anaconda.In an activated environment, install packages via conda or pip:These environments are strongly tied to conda's pip-like package management, so it is simple to create environments and install both Python and non-Python packages.JupyterIn addition, installing ipykernel in an environment adds a new listing in the Kernels dropdown menu of Jupyter notebooks, extending reproducible environments to notebooks.  As of Anaconda 4.1, nbextensions were added, adding extensions to notebooks more easily.ReliabilityIn my experience, conda is faster and more reliable at installing large libraries such as numpy and pandas.  Moreover, if you wish to transfer your the preserved state of an environment, you can do so by sharing or cloning an env.Installing Conda will enable you to create and remove python environments as you wish, therefore providing you with same functionality as virtualenv would.In case of both distributions you would be able to create an isolated filesystem tree, where you can install and remove python packages (probably, with pip) as you wish. Which might come in handy if you want to have different versions of same library for different use cases or you just want to try some distribution and remove it afterwards conserving your disk space.License agreement. While virtualenv comes under most liberal MIT license, Conda uses 3 clause BSD license.Conda provides you with their own package control system. This package control system often provides precompiled versions (for most popular systems) of popular non-python software, which can easy ones way getting some machine learning packages working. Namely you don't have to compile optimized C/C++ code for you system. While it is a great relief for most of us, it might affect performance of such libraries.Unlike virtualenv, Conda duplicating some system libraries at least on Linux system. This libraries can get out of sync leading to inconsistent behavior of your programs.Conda is great and should be your default choice while starting your way with machine learning. It will save you some time messing with gcc and numerous packages. Yet, Conda does not replace virtualenv. It introduces some additional complexity which might not always be desired. It comes under different license. You might want to avoid using conda on a distributed environments or on HPC hardware. Another new option and my current preferred method of getting an environment up and running is PipenvIt is currently the officially recommended Python packaging tool from Python.orgYes, conda is a lot easier to install than virtualenv, and pretty much replaces the latter.I use both and (as of Jan, 2020) they have some superficial differences that lend themselves to different usages for me.  By default Conda prefers to manage a list of environments for you in a central location, whereas virtualenv makes a folder in the current directory.  The former (centralized) makes sense if you are e.g. doing machine learning and just have a couple of broad environments that you use across many projects and want to jump into them from anywhere.  The latter (per project folder) makes sense if you are doing little one-off projects that have completely different sets of lib requirements that really belong more to the project itself.The empty environment that Conda creates is about 122MB whereas the virtualenv's is about 12MB, so that's another reason you may prefer not to scatter Conda environments around everywhere.Finally, another superficial indication that Conda prefers its centralized envs is that (again, by default) if you do create a Conda env in your own project folder and activate it the name prefix that appears in your shell is the (way too long) absolute path to the folder.  You can fix that by giving it a name, but virtualenv does the right thing by default.I expect this info to become stale rapidly as the two package managers vie for dominance, but these are the trade-offs as of today :)I work in corporate, behind several firewall  with machine on which I have no admin accesIn my limited experience with python (2 years) i have come  across few libraries (JayDeBeApi,sasl) which when installing via pip threw C++ dependency errors 

error: Microsoft Visual C++ 14.0 is required. Get it with "Microsoft Visual C++ Build Tools": http://landinghub.visualstudio.com/visual-cpp-build-toolsthese installed fine with conda, hence since those days i started working with conda env.

however it isnt easy to stop conda from installing dependency inside c.programfiles where i dont have write access.

Python, compute list difference

Mike

[Python, compute list difference](https://stackoverflow.com/questions/6486450/python-compute-list-difference)

In Python, what is the best way to compute the difference between two lists?example

2011-06-26 20:14:17Z

In Python, what is the best way to compute the difference between two lists?exampleUse set if you don't care about items order or repetition. Use list comprehensions if you do:If the order does not matter, you can simply calculate the set difference:You can do a andOne liner:Or:The above examples trivialized the problem of calculating differences.  Assuming sorting or de-duplication definitely make it easier to compute the difference, but if your comparison cannot afford those assumptions then you'll need a non-trivial implementation of a diff algorithm.  See difflib in the python standard library.A - B = [[1, 3, 4]]Python 2.7.3 (default, Feb 27 2014, 19:58:35) - IPython 1.1.0 - timeit: (github gist)Results:@roman-bodnarchuk list comprehensions function def diff(a, b) seems to be faster.You would want to use a set instead of a list.In case you want the difference recursively going deep into items of your list, I have written a package for python: https://github.com/erasmose/deepdiffInstall from PyPi:If you are Python3 you need to also install:Same object returns emptyType of an item has changedValue of an item has changedItem added and/or removedString differenceString difference 2        Type changeList differenceList difference 2: Note that it DOES NOT take order into accountList that contains dictionary:most simple way,use set().difference(set())answer is set([1])In case of a list of dictionaries, the full list comprehension solution works while the set solution raisesWhen having a look at TimeComplexity of In-operator, in worst case it works with O(n). Even for Sets.So when comparing two arrays we'll have a TimeComplexity of O(n) in best case and O(n^2) in worst case.An alternative (but unfortunately more complex) solution, which works with O(n) in best and worst case is this one:e.g.Simple code that gives you the difference with multiple items if you want that:    

Converting Dictionary to List? [duplicate]

Federer

[Converting Dictionary to List? [duplicate]](https://stackoverflow.com/questions/1679384/converting-dictionary-to-list)

I'm trying to convert a Python dictionary into a Python list, in order to perform some calculations.That's my attempt at it... but I can't work out what's wrong?

2009-11-05 09:34:18Z

I'm trying to convert a Python dictionary into a Python list, in order to perform some calculations.That's my attempt at it... but I can't work out what's wrong?Your problem is that you have key and value in quotes making them strings, i.e. you're setting aKey to contain the string "key" and not the value of the variable key.  Also, you're not clearing out the temp list, so you're adding to it each time, instead of just having two items in it.To fix your code, try something like:You don't need to copy the loop variables key and value into another variable before using them so I dropped them out.  Similarly, you don't need to use append to build up a list, you can just specify it between square brackets as shown above.  And we could have done dictlist.append([key,value]) if we wanted to be as brief as possible.Or just use dict.items() as has been suggested.Does the trick.Converting from dict to list is made easy in Python. Three examples:You should use dict.items().Here is a one liner solution for your problem:and result:or you can dofor:To explain: a.items() returns a list of tuples. Adding two tuples together makes one tuple containing all elements. Thus the reduction creates one tuple containing all keys and values and then the list(...) makes a list from that.Probably you just want this:Your approach has two problems. For one you use key and value in quotes, which are strings with the letters "key" and "value", not related to the variables of that names. Also you keep adding elements to the "temporary" list and never get rid of old elements that are already in it from previous iterations. Make sure you have a new and empty temp list in each iteration and use the key and value variables:Also note that this could be written shorter without the temporary variables (and in Python 3 with items() instead of iteritems()):If you're making a dictionary only to make a list of tuples, as creating dicts like you are may be a pain, you might look into using zip()Its especialy useful if you've got one heading, and multiple rows. For instance if I assume that you want Olympics stats for countries:givesDon't know if thats the end goal, and my be off topic, but it could be something to keep in mind.

Extracting just Month and Year separately from Pandas Datetime column

monkeybiz7

[Extracting just Month and Year separately from Pandas Datetime column](https://stackoverflow.com/questions/25146121/extracting-just-month-and-year-separately-from-pandas-datetime-column)

I have a Dataframe, df, with the following column:The elements of the column are pandas.tslib.Timestamp.I want to just include the year and month.  I thought there would be simple way to do it, but I can't figure it out.Here's what I've tried:I got the following error:Then I tried:I got the following error:Any suggestions?Edit: I sort of figured it out.  Then, I can resample another column using the index.But I'd still like a method for reconfiguring the entire column.  Any ideas?

2014-08-05 18:44:30Z

I have a Dataframe, df, with the following column:The elements of the column are pandas.tslib.Timestamp.I want to just include the year and month.  I thought there would be simple way to do it, but I can't figure it out.Here's what I've tried:I got the following error:Then I tried:I got the following error:Any suggestions?Edit: I sort of figured it out.  Then, I can resample another column using the index.But I'd still like a method for reconfiguring the entire column.  Any ideas?You can directly access the year and month attributes, or request a datetime.datetime:One way to combine year and month is to make an integer encoding them, such as: 201408 for August, 2014. Along a whole column, you could do this as:or many variants thereof.I'm not a big fan of doing this, though, since it makes date alignment and arithmetic painful later and especially painful for others who come upon your code or data without this same convention. A better way is to choose a day-of-month convention, such as final non-US-holiday weekday, or first day, etc., and leave the data in a date/time format with the chosen date convention.The calendar module is useful for obtaining the number value of certain days such as the final weekday. Then you could do something like:If you happen to be looking for a way to solve the simpler problem of just formatting the datetime column into some stringified representation, for that you can just make use of the strftime function from the datetime.datetime class, like this:If you want new columns showing year and month separately you can do this:or...Then you can combine them or work with them just as they are.Best way found!!the df['date_column'] has to be in date time format.You could also use D for Day, 2M for 2 Months etc. for different sampling intervals, and in case one has time series data with time stamp, we can go for granular sampling intervals such as 45Min for 45 min, 15Min for 15 min sampling etc.If you want the month year unique pair, using apply is pretty sleek.Outputs month-year in one column.Don't forget to first change the format to date-time before, I generally forget.Extracting the Year say from ['2018-03-04']  The df['Year'] creates a new column. While if you want to extract the month just use .month  You can first convert your date strings with pandas.to_datetime, which gives you access to all of the numpy datetime and timedelta facilities. For example:Thanks to jaknap32, I wanted to aggregate the results according to Year and Month, so this worked:Output was neat:@KieranPC's solution is the correct approach for Pandas, but is not easily extendible for arbitrary attributes. For this, you can use getattr within a generator comprehension and combine using pd.concat:This worked fine for me, didn't think pandas would interpret the resultant string date as date, but when i did the plot, it knew very well my agenda and the string year_month where ordered properly... gotta love pandas!There is two steps to extract year for all the dataframe without using method apply.Step1convert the column to datetime :Step2extract the year or the month using DatetimeIndex() method

BeautifulSoup getting href [duplicate]

dkgirl

[BeautifulSoup getting href [duplicate]](https://stackoverflow.com/questions/5815747/beautifulsoup-getting-href)

I have the following soup:From this I want to extract the href, "some_url"I can do it if I only have one tag, but here there are two tags. I can also get the text 'next' but that's not what I want.Also, is there a good description of the API somewhere with examples. I'm using the standard documentation, but I'm looking for something a little more organized. 

2011-04-28 08:25:21Z

I have the following soup:From this I want to extract the href, "some_url"I can do it if I only have one tag, but here there are two tags. I can also get the text 'next' but that's not what I want.Also, is there a good description of the API somewhere with examples. I'm using the standard documentation, but I'm looking for something a little more organized. You can use find_all in the following way to find every a element that has an href attribute, and print each one:The output would be:Note that if you're using an older version of BeautifulSoup (before version 4) the name of this method is findAll. In version 4, BeautifulSoup's method names were changed to be PEP 8 compliant, so you should use find_all instead.If you want all tags with an href, you can omit the name parameter:

The tilde operator in Python

clwen

[The tilde operator in Python](https://stackoverflow.com/questions/8305199/the-tilde-operator-in-python)

What's the usage of the tilde operator in Python?One thing I can think about is do something in both sides of a string or list, such as check if a string is palindromic or not:Any other good usage?

2011-11-29 02:43:56Z

What's the usage of the tilde operator in Python?One thing I can think about is do something in both sides of a string or list, such as check if a string is palindromic or not:Any other good usage?It is a unary operator (taking a single argument) that is borrowed from C, where all data types are just different ways of interpreting bytes.  It is the "invert" or "complement" operation, in which all the bits of the input data are reversed.In Python, for integers, the bits of the twos-complement representation of the integer are reversed (as in b <- b XOR 1 for each individual bit), and the result interpreted again as a twos-complement integer.  So for integers, ~x is equivalent to (-x) - 1.The reified form of the ~ operator is provided as operator.invert.  To support this operator in your own class, give it an __invert__(self) method.Any class in which it is meaningful to have a "complement" or "inverse" of an instance that is also an instance of the same class is a possible candidate for the invert operator.  However, operator overloading can lead to confusion if misused, so be sure that it really makes sense to do so before supplying an __invert__ method to your class.  (Note that byte-strings [ex: '\xff'] do not support this operator, even though it is meaningful to invert all the bits of a byte-string.)~ is the bitwise complement operator in python which essentially calculates -x - 1So a table would look likeSo for i = 0 it would compare s[0] with s[len(s) - 1], for i = 1, s[1] with s[len(s) - 2].As for your other question, this can be useful for a range of bitwise hacks.Besides being a bitwise complement operator, ~ can also help revert a boolean value, though it is not the conventional bool type here, rather you should use numpy.bool_.This is explained in,Reversing logical value can be useful sometimes, e.g., below ~ operator is used to cleanse your dataset and return you a column without NaN.One should note that in the case of array indexing, array[~i] amounts to reversed_array[i]. It can be seen as indexing starting from the end of the array:The only time I've ever used this in practice is with numpy/pandas.  For example, with the .isin() dataframe method.In the docs they show this basic exampleBut what if instead you wanted all the rows not in [0, 2]?  I was solving this leetcode problem and I came across this beautiful solution by a user named Zitao Wang.The problem goes like this for each element in the given array find the product of all the remaining numbers without making use of divison and in O(n) timeThe standard solution is:His solution uses only one for loop by making use of. He computes the left product and right product on the fly using ~This is minor usage is tilde...the code above is from "Hands On Machine Learning"you use tilde (~ sign) as alternative to - sign index markerjust like you use minus - is for integer indexex) is the samething asprint(array[~1])

HTTP requests and JSON parsing in Python

Arun

[HTTP requests and JSON parsing in Python](https://stackoverflow.com/questions/6386308/http-requests-and-json-parsing-in-python)

I want to dynamically query Google Maps through the Google Directions API. As an example, this request calculates the route from Chicago, IL to Los Angeles, CA via two waypoints in Joplin, MO and Oklahoma City, OK:http://maps.googleapis.com/maps/api/directions/json?origin=Chicago,IL&destination=Los+Angeles,CA&waypoints=Joplin,MO|Oklahoma+City,OK&sensor=falseIt returns a result in the JSON format.How can I do this in Python? I want to send such a request, receive the result and parse it.

2011-06-17 13:17:43Z

I want to dynamically query Google Maps through the Google Directions API. As an example, this request calculates the route from Chicago, IL to Los Angeles, CA via two waypoints in Joplin, MO and Oklahoma City, OK:http://maps.googleapis.com/maps/api/directions/json?origin=Chicago,IL&destination=Los+Angeles,CA&waypoints=Joplin,MO|Oklahoma+City,OK&sensor=falseIt returns a result in the JSON format.How can I do this in Python? I want to send such a request, receive the result and parse it.I recommend using the awesome requests library:JSON Response Content: https://requests.readthedocs.io/en/master/user/quickstart/#json-response-contentThe requests Python module takes care of both retrieving JSON data and decoding it, due to its builtin JSON decoder. Here is an example taken from the module's documentation:So there is no use of having to use some separate module for decoding JSON.requests has built-in .json() methodUse the requests library, pretty print the results so you can better locate the keys/values you want to extract, and then use nested for loops to parse the data. In the example I extract step by step driving directions.Try this:

Django Server Error: port is already in use

Ashish Kumar Saxena

[Django Server Error: port is already in use](https://stackoverflow.com/questions/20239232/django-server-error-port-is-already-in-use)

Restarting the Django server displays the following error:This problem occurs specifically on Ubuntu and not other operating systems. How can I free up the port to restart the server?

2013-11-27 10:00:43Z

Restarting the Django server displays the following error:This problem occurs specifically on Ubuntu and not other operating systems. How can I free up the port to restart the server?A more simple solution just type sudo fuser -k 8000/tcp.

This should kill all the processes associated with port 8000.EDIT:For osx users you can use sudo lsof -t -i tcp:8000 | xargs kill -9It will show something like this.So now just close the port in which Django/python running already by killing the process associated with it.in my caseNow run your Django app.We don't use this command { sudo lsof -t -i tcp:8000 | xargs kill -9 } Because it's close all tabs...You should use to ps -ef | grep python (show all process with id)kill -9 11633 

(11633 is a process id to :- /bin/python manage.py runserver)By default, the runserver command starts the development server on the internal IP at port 8000.If you want to change the server’s port, pass it as a command-line argument. For instance, this command starts the server on port 8080:This is an expansion on Mounir's answer. I've added a bash script that covers this for you. Just run ./scripts/runserver.sh instead of ./manage.py runserver and it'll work exactly the same way.For me, this happens because my API request in Postman is being intercepted by a debugger breakpoint in my app... leaving the request hanging. If I cancel the request in Postman before killing my app's server, the error does not happen in the first place.--> So try cancelling any open requests you are making in other programs.On macOS, I have been using sudo lsof -t -i tcp:8000 | xargs kill -9 when I forget to cancel the open http request in order to solve error = That port is already in use. This also, complete closes my Postman app, which is why my first solution is better.Sorry for comment in an old post but It may help peopleJust type this on your terminal It will kill all python3 running on your machine and it will free your all port. Greatly help me when to work in Django project.ps aux | grep manageubuntu    3438  127.0.0  2.3  40256 14064 pts/0    T    06:47   0:00 python manage.py runserverkill -9 3438It seems that IDEs, VSCode, Puppeteer, nodemon, express, etc. causes this problem, you ran a process in the background or just closed the debugging area [browser, terminal, etc. ] or whatever , anyway, i have answered same question before,

Here you are it's linkhttps://stackoverflow.com/a/49797588/2918720Type 'fg' as command after that ctl-c.

Command:

Fg will show which is running on background. After that ctl-c will stop it.if you have face this problem in mac you just need to open activity monitor and force quite python then try againlsof -t -i tcp:8000 | xargs kill -9

Django Admin - change header 'Django administration' text

samurailawngnome

[Django Admin - change header 'Django administration' text](https://stackoverflow.com/questions/4938491/django-admin-change-header-django-administration-text)

How does one change the 'Django administration' text in the django admin header?It doesn't seem to be covered in the "Customizing the admin" documentation.

2011-02-08 21:10:16Z

How does one change the 'Django administration' text in the django admin header?It doesn't seem to be covered in the "Customizing the admin" documentation.Update: If you are using Django 1.7+, see the answer below.Original answer from 2011:

You need to create your own admin base_site.html template to do this. The easiest way is to create the file:This should be a copy of the original base_site.html, except putting in your custom title:For this to work, you need to have the correct settings for your project, namely in settings.py:See docs for more information on settings.py.As of Django 1.7 you don't need to override templates. You can now implement site_header, site_title, and index_title attributes on a custom AdminSite in order to easily change the admin site’s page title and header text. Create an AdminSite subclass and hook your instance into your URLconf:admin.py:urls.py:Update: As pointed out by oxfn you can simply set the site_header in your urls.py or admin.py directly without subclassing AdminSite:There is an easy way to set admin site header - assign it to current admin instance in urls.py like thisOr one can implement some header-building magic in separate methodThus, in simple cases there's no need to subclass AdminSiteIn urls.py you can override the 3 most important variables:Reference: Django documentation on these attributes.A simple complete solution in Django 1.8.3 based on answers in this question.In settings.py add:In urls.py add:The easiest way of doing it 

make sure you have and then just add these at bottom of url.py of you main application For Django 2.1.1 add following lines to urls.pyAs you can see in the templates, the text is delivered via the localization framework (note the use of the trans template tag). You can make changes to the translation files to override the text without making your own copy of the templates.See https://docs.djangoproject.com/en/1.3/topics/i18n/localization/#message-filesadmin.py:First of all, you should add templates/admin/base_site.html to your project. This file can safely be overwritten since it’s a file that the Django devs have intended for the exact purpose of customizing your admin site a bit. Here’s an example of what to put in the file:This is common practice. But I noticed after this that I was still left with an annoying「Site Administration」on the main admin index page. And this string was not inside any of the templates, but rather set inside the admin view. Luckily it’s quite easy to change. Assuming your language is set to English, run the following commands from your project directory:Now open up the file locale/en/LC_MESSAGES/django.po and add two lines after the header information (the last two lines of this example)After this, remember to run the following command and reload your project’s server:source: http://overtag.dk/wordpress/2010/04/changing-the-django-admin-site-title/you do not need to change any template for this work you just need to update the settings.py of your project. Go to the bottom of the settings.py and define this.In this way you would be able to change the header of the  of the Django admin. Moreover you can read more about Django Admin customization and settings on the following link.Django Admin DocumentationYou can use AdminSite.site_header to change that text. Here is the docsSince I only use admin interface in my app, I put this in the admin.py :You just override the admin/base_site.html template (copy the template from django.contrib.admin.templates and put in your own admin template dir) and replace the branding block.There are two methods to do this:1] By overriding base_site.html in django/contrib/admin/templates/admin/base_site.html:

Following is the content of base_site.html: Edit the site_title & site_header in the above code snippet. This method works but it is not recommendable since its a static change.2] By adding following lines in urls.py of project's directory:This method is recommended one since we can change the site-header, site-title & index-title without editing base_site.html.From Django 2.0 you can just add a single line in the url.py and change the name.For older versions of Django. (<1.11 and earlier) you need to edit admin/base_site.html Change this lineto You can check your django version by 

Upgrade python in a virtualenv

Simon Walker

[Upgrade python in a virtualenv](https://stackoverflow.com/questions/10218946/upgrade-python-in-a-virtualenv)

Is there a way to upgrade the version of python used in a virtualenv (e.g. if a bugfix release comes out)?I could pip freeze --local > requirements.txt, then remove the directory and pip install -r requirements.txt, but this requires a lot of reinstallation of large libraries, for instance, numpy, which I use a lot. I can see this is an advantage when upgrading from, e.g., 2.6 -> 2.7, but what about 2.7.x -> 2.7.y?

2012-04-18 22:21:43Z

Is there a way to upgrade the version of python used in a virtualenv (e.g. if a bugfix release comes out)?I could pip freeze --local > requirements.txt, then remove the directory and pip install -r requirements.txt, but this requires a lot of reinstallation of large libraries, for instance, numpy, which I use a lot. I can see this is an advantage when upgrading from, e.g., 2.6 -> 2.7, but what about 2.7.x -> 2.7.y?Did you see this? If I haven't misunderstand that answer, you may try to create a new virtualenv on top of the old one. You just need to know which python is going to use your virtualenv (you will need to see your virtualenv version). If your virtualenv is installed with the same python version of the old one and upgrading your virtualenv package is not an option, you may want to read this in order to install a virtualenv with the python version you want.EDITI've tested this approach (the one that create a new virtualenv on top of the old one) and it worked fine for me. I think you may have some problems if you change from python 2.6 to 2.7 or 2.7 to 3.x but if you just upgrade inside the same version (staying at 2.7 as you want) you shouldn't have any problem, as all the packages are held in the same folders for both python versions (2.7.x and 2.7.y packages are inside your_env/lib/python2.7/).If you change your virtualenv python version, you will need to install all your packages again for that version (or just link the packages you need into the new version packages folder, i.e: your_env/lib/python_newversion/site-packages)If you happen to be using the venv module that comes with Python 3.3+, it supports an --upgrade option. 

Per the docs:Updated again:

The following method might not work in newer versions of virtualenv. Before you try to make modifications to the old virtualenv, you should save the dependencies in a requirement file (pip freeze > requirements.txt) and make a backup of it somewhere else. If anything goes wrong, you can still create a new virtualenv and install the old dependencies in it (pip install -r requirements.txt).Updated: I changed the answer 5 months after I originally answered. The following method is more convenient and robust.Side effect: it also fixes the Symbol not found: _SSLv2_method exception when you do import ssl in a virtual environment after upgrading Python to v2.7.8.Notice: Currently, this is for Python 2.7.x only.If you're using Homebrew Python on OS X, first deactivate all virtualenv, then upgrade Python:Run the following commands (<EXISTING_ENV_PATH> is path of your virtual environment):Finally, re-create your virtual environment:By doing so, old Python core files and standard libraries (plus setuptools and pip) are removed, while the custom libraries installed in site-packages are preserved and working, as soon as they are in pure Python. Binary libraries may or may not need to be reinstalled to function properly.This worked for me on 5 virtual environments with Django installed.BTW, if ./manage.py compilemessages is not working afterwards, try this:I wasn't able to create a new virtualenv on top of the old one.  But there are tools in pip which make it much faster to re-install requirements into a brand new venv.  Pip can build each of the items in your requirements.txt into a wheel package, and store that in a local cache.  When you create a new venv and run pip install in it, pip will automatically use the prebuilt wheels if it finds them.  Wheels install much faster than running setup.py for each module.My ~/.pip/pip.conf looks like this:I install wheel (pip install wheel), then run pip wheel -r requirements.txt.  This stores the built wheels in the wheel-dir in my pip.conf.From then on, any time I pip install any of these requirements, it installs them from the wheels, which is pretty quick.  I'm adding an answer for anyone using Doug Hellmann's excellent virtualenvwrapper specifically since the existing answers didn't do it for me.Some context:Directions:Let's say your existing project is named foo and is currently running Python 2 (mkproject -p python2 foo), though the commands are the same whether upgrading from 2.x to 3.x, 3.6.0 to 3.6.1, etc.  I'm also assuming you're currently inside the activated virtual environment.1. Deactivate and remove the old virtual environment:Note that if you've added any custom commands to the hooks (e.g., bin/postactivate) you'd need to save those before removing the environment.2. Stash the real project in a temp directory:3. Create the new virtual environment (and project dir) and activate:4. Replace the empty generated project dir with real project, change back into project dir:5. Re-install dependencies, confirm new Python version, etc:If this is a common use case, I'll consider opening a PR to add something like $ upgradevirtualenv / $ upgradeproject to virtualenvwrapper.I moved my home directory from one mac to another (Mountain Lion to Yosemite) and didn't realize about the broken virtualenv until I lost hold of the old laptop. I had the virtualenv point to Python 2.7 installed by brew and since Yosemite came with Python 2.7, I wanted to update my virtualenv to the system python. When I ran virtualenv on top of the existing directory, I was getting OSError: [Errno 17] File exists: '/Users/hdara/bin/python2.7/lib/python2.7/config' error. By trial and error, I worked around this issue by removing a few links and fixing up a few more manually. This is what I finally did (similar to what @Rockalite did, but simpler):After this, I was able to just run virtualenv on top of the existing directory.On OS X or macOS using Homebrew to install and upgrade Python3 I had to delete symbolic links before python -m venv --upgrade ENV_DIR would work.I saved the following in upgrade_python3.sh so I would remember how months from now when I need to do it again:UPDATE: while this seemed to work well at first, when I ran py.test it gave an error.  In the end I just re-created the environment from a requirements file.If you're using pipenv, I don't know if it's possible to upgrade an environment in place, but at least for minor version upgrades it seems to be smart enough not to rebuild packages from scratch when it creates a new environment. E.g., from 3.6.4 to 3.6.5:I just want to clarify, because some of the answers refer to venv and others refer to virtualenv.Use of the -p or --python flag is supported on virtualenv, but not on venv. If you have more than one Python version and you want to specify which one to create the venv with, do it on the command line, like this:You can of course upgrade with venv as others have pointed out, but that assumes you have already upgraded the Python that was used to create that venv in the first place. You can't upgrade to a Python version you don't already have on your system somewhere, so make sure to get the version you want, first, then make all the venvs you want from it.This approach always works for me: Taken from:For everyone with the problemYou have to install python3.6-venv

matplotlib error - no module named tkinter

noamgot

[matplotlib error - no module named tkinter](https://stackoverflow.com/questions/36327134/matplotlib-error-no-module-named-tkinter)

I tried to use the matplotlib package via Pycharm IDE on windows 10.

when I run this code:I get the following error:I know that in python 2.x it was called Tkinter, but that is not the problem - I just installed a brand new python 3.5.1.EDIT: in addition, I also tried to import 'tkinter' and 'Tkinter' - neither of these worked (both returned the error message I mentioned).

2016-03-31 07:42:34Z

I tried to use the matplotlib package via Pycharm IDE on windows 10.

when I run this code:I get the following error:I know that in python 2.x it was called Tkinter, but that is not the problem - I just installed a brand new python 3.5.1.EDIT: in addition, I also tried to import 'tkinter' and 'Tkinter' - neither of these worked (both returned the error message I mentioned).Then,Edit:For Windows, I think the problem is you didn't install complete Python package. Since Tkinter should be shipped with Python out of box. See: http://www.tkdocs.com/tutorial/install.htmlI suggest install ipython, which provides powerful shell and necessary packages as well.you can use  if you dont want to use tkinter at all.Also dont forget to use %matplotlib inline at the top of your notebook if using one.EDIT: agg is a different backend like tkinter for matplotlib.On Centos, the package names and commands are different. You'll need to do:To fix the problem.Almost all answers I searched for this issue say that Python on Windows comes with tkinter and tcl already installed, and I had no luck trying to download or install them using pip, or actviestate.com site. I eventually found that when I was installing python using the binary installer, I had unchecked the module related to TCL and tkinter. So, I ran the binary installer again and chose to modify my python version by this time selecting this option. No need to do anything manually then. If you go to your python terminal, then the following commands should show you version of tkinter installed with your Python:If you are using fedora then first install tkinterI don't think you need to import tkinter afterwards

I also suggest you to use virtualenv And add the necessary packages using pipFor Windows users, there's no need to download the installer again. Just do the following:Wait for installation and you're done.On CentOS 7 and Python 3.4, the command is sudo yum install python34-tkinterOn Redhat 7.4 with Python 3.6, the command is sudo yum install rh-python36-python-tkinterOn Ubuntu, early 2018, there is no python3.6-tk on ubuntu's (xenial/16.04) normal distributions, so even if you have earlier versions of python-tk this won't work.My solution was to use set everything up with python 3.5:And now matplotlib can find tkinter.EDIT:I just needed 3.6 afterall, and the trick was to:and then rebuild python3.6, after tk-dev, eg:For windows users, re-run the installer. Select Modify. Check the box for tcl/tk and IDLE. The description for this says "Installs tkinter"If you are using python 3.6, this worked for me:instead ofWhich works for other versions of python3For the poor guys like me using python 3.7. You need the python3.7-tk package.sudo apt install python3.7-tkNote. python3-tk is installed. But not python3.7-tk.After installing it, all good.On CentOS 6.5 with python 2.7 I needed to do: yum install python27-tkinterSometimes (for example in osgeo4w distribution) tkinter is removed.Try changing matplotlib backend editing matplotlibrc file located in [python install dir]/matplotlib/mpl-data/matplotlibrc changing The backend parameter from backend: TkAgg to something other like backend: Qt4Aggas described here: http://matplotlib.org/faq/usage_faq.html#what-is-a-backendSince I'm using Python 3.7 on Ubuntu I had to use:Maybe you installed python from source. In this case, you can recompile python with tcl/tk supported.

What is the syntax to insert one list into another list in python?

Compuser7

[What is the syntax to insert one list into another list in python?](https://stackoverflow.com/questions/3748063/what-is-the-syntax-to-insert-one-list-into-another-list-in-python)

Given two lists:What is the syntax to:

2010-09-20 00:41:13Z

Given two lists:What is the syntax to:Do you mean append?Or merge?The question does not make clear what exactly you want to achieve.List has the append method, which appends its argument to the list:There's also the extend method, which appends items from the list you pass as an argument:And of course, there's the insert method which acts similarly to append but allows you to specify the insertion point:To extend a list at a specific insertion point you can use list slicing (thanks, @florisla):List slicing is quite flexible as it allows to replace a range of entries in a list with a range of entries from another list:http://docs.python.org/tutorial/datastructures.htmlYou can also just do...If you want to add the elements in a list (list2) to the end of other list (list), then you can use the list extend methodOr if you want to concatenate two list then you can use + signIf we just do x.append(y), y gets referenced into x such that any changes made to y will affect appended x as well. So if we need to insert only elements, we should do following:x = [1,2,3]

y = [4,5,6]

x.append(y[:])

Changing a specific column name in pandas DataFrame

Mark Graph

[Changing a specific column name in pandas DataFrame](https://stackoverflow.com/questions/20868394/changing-a-specific-column-name-in-pandas-dataframe)

I was looking for an elegant way to change a specified column name in a DataFrame.play data ...The most elegant solution I have found so far ...I was hoping for a simple one-liner ... this attempt failed ...Any hints gratefully received.

2014-01-01 11:59:54Z

I was looking for an elegant way to change a specified column name in a DataFrame.play data ...The most elegant solution I have found so far ...I was hoping for a simple one-liner ... this attempt failed ...Any hints gratefully received.A one liner does exist:Following is the docstring for the rename method.Since inplace argument is available, you don't need to copy and assign the original data frame back to itself, but do as follows:What about?The rename method has gained an axis parameter to match most of the rest of the pandas API.So, in addition to this:You can do:orIf you know which column # it is (first / second / nth) then this solution posted on a similar question works regardless of whether it is named or unnamed, and in one line: https://stackoverflow.com/a/26336314/4355695For renaming the columns here is the simple one which will work for both Default(0,1,2,etc;) and existing columns but not much useful for a larger data sets(having many columns).For a larger data set we can slice the columns that we need and apply the below code:Following short code can help:Remove spaces from columns.pandas version 0.23.4For the record:Another option would be to simply copy & drop the column:After that you get the result:

Including non-Python files with setup.py

Ram Rachum

[Including non-Python files with setup.py](https://stackoverflow.com/questions/1612733/including-non-python-files-with-setup-py)

How do I make setup.py include a file that isn't part of the code? (Specifically, it's a license file, but it could be any other thing.)I want to be able to control the location of the file. In the original source folder, the file is in the root of the package. (i.e. on the same level as the topmost __init__.py.) I want it to stay exactly there when the package is installed, regardless of operating system. How  do I do that?

2009-10-23 11:04:57Z

How do I make setup.py include a file that isn't part of the code? (Specifically, it's a license file, but it could be any other thing.)I want to be able to control the location of the file. In the original source folder, the file is in the root of the package. (i.e. on the same level as the topmost __init__.py.) I want it to stay exactly there when the package is installed, regardless of operating system. How  do I do that?Probably the best way to do this is to use the setuptools package_data directive.  This does mean using setuptools (or distribute) instead of distutils, but this is a very seamless "upgrade".Here's a full (but untested) example:Note the specific lines that are critical here:package_data is a dict of package names (empty = all packages) to a list of patterns (can include globs).  For example, if you want to only specify files within your package, you can do that too:The solution here is definitely not to rename your non-py files with a .py extension.See Ian Bicking's presentation for more info.Another approach that works well if you just want to control the contents of the source distribution (sdist) and have files outside of the package (e.g. top-level directory) is to add a MANIFEST.in file.  See the Python documentation for the format of this file.Since writing this response, I have found that using MANIFEST.in is typically a less frustrating approach to just make sure your source distribution (tar.gz) has the files you need. For example, if you wanted to include the requirements.txt from top-level, recursively include the top-level "data" directory:Nevertheless, in order for these files to be copied at install time to the package’s folder inside site-packages, you’ll need to supply include_package_data=True to the setup() function. See Adding Non-Code Files for more information.To accomplish what you're describing will take two steps...Step 1: To add the file to the source tarball, include it in the MANIFESTCreate a MANIFEST template in the folder that contains setup.pyThe MANIFEST is basically a text file with a list of all the files that will be included in the source tarball.Here's what the MANIFEST for my project look like:Note: While sdist does add some files automatically, I prefer to explicitly specify them to be sure instead of predicting what it does and doesn't.Step 2: To install the data file to the source folder, modify setup.pySince you're looking to add a data file (LICENSE.txt) to the source install folder you need to modify the data install path to match the source install path. This is necessary because, by default, data files are installed to a different location than source files.To modify the data install dir to match the source install dir...Pull the install dir info from distutils with:Modify the data install dir to match the source install dir:And, add the data file and location to setup():Note: The steps above should accomplish exactly what you described in a standard manner without requiring any extension libraries. create MANIFEST.in in the project root with recursive-include to the required directory or include with the file name.documentation can be found hereIn setup.py under setup(  :Step 1: create a MANIFEST.in file in the same folder with setup.pyStep 2: include the relative path to the files you want to add in MANIFEST.inStep 3: set include_package_data=True in the setup() function to copy these files to site-packageReference is here.It is 2019, and here is what is working - 

despite advice here and there, what I found on the internet halfway documented is using setuptools_scm, passed as options to setuptools.setup. This will include any data files that are versioned on your VCS, be it git or any other, to the wheel package, and will make "pip install" from the git repository to bring those files along.So, I just added these two lines to the setup call on "setup.py". No extra installs or import requireds:No need to manually list package_data, or in a MANIFEST.in file - if it is versioned, it is included in the package. The docs on "setuptools_scm" put emphasis on creating a version number from the commit position, and disregard the really important part of adding the data files. (I can't care less if my intermediate wheel file is named "*0.2.2.dev45+g3495a1f" or will use the hardcoded version number "0.3.0dev0" I've typed in - but leaving crucial files for the program to work behind is somewhat important)Here is a simpler answer that worked for me.  First, per a Python Dev's comment above, setuptools is not required:That's great because putting a setuptools requirement on your package means you will have to install it also.  In short:I wanted to post a comment to one of the questions but I don't enough reputation to do that >.>Here's what worked for me (came up with it after referring the docs):The last line was, strangely enough, also crucial for me (you can also omit this keyword argument - it works the same).What this does is it copies all text files in your top-level or root directory (one level up from the package mypkg you want to distribute).Hope this helps!I just wanted to follow up on something I found working with Python 2.7 on Centos 6.  Adding the package_data or data_files as mentioned above did not work for me.  I added a MANIFEST.IN with the files I wanted which put the non-python files into the tarball, but did not install them on the target machine via RPM.  In the end, I was able to get the files into my solution using the "options" in the setup/setuptools.  The option files let you modify various sections of the spec file from setup.py.  As follows.file - MANIFEST.in:file - filewithinstallcommands:Figured out a workaround: I renamed my lgpl2.1_license.txt to lgpl2.1_license.txt.py, and put some triple quotes around the text. Now I don't need to use the data_files option nor to specify any absolute paths. Making it a Python module is ugly, I know, but I consider it less ugly than specifying absolute paths.

Take the content of a list and append it to another list

user1006198

[Take the content of a list and append it to another list](https://stackoverflow.com/questions/8177079/take-the-content-of-a-list-and-append-it-to-another-list)

I am trying to understand if it makes sense to take the content of a list and append it to another list.I have the first list created through a loop function, that will get specific lines out of a file and will save them in a list.Then a second list is used to save these lines, and start a new cycle over another file.My idea was to get the list once that the for cycle is done, dump it into the second list, then start a new cycle, dump the content of the first list again into the second but appending it, so the second list will be the sum of all the smaller list files created in my loop. The list has to be appended only if certain conditions met. It looks like something similar to this:Does this makes sense or should I go for a different route?I need something efficient that would not take up too many cycles, since the list of logs is long and each text file is pretty big; so I thought that the lists would fit the purpose.

2011-11-18 02:31:23Z

I am trying to understand if it makes sense to take the content of a list and append it to another list.I have the first list created through a loop function, that will get specific lines out of a file and will save them in a list.Then a second list is used to save these lines, and start a new cycle over another file.My idea was to get the list once that the for cycle is done, dump it into the second list, then start a new cycle, dump the content of the first list again into the second but appending it, so the second list will be the sum of all the smaller list files created in my loop. The list has to be appended only if certain conditions met. It looks like something similar to this:Does this makes sense or should I go for a different route?I need something efficient that would not take up too many cycles, since the list of logs is long and each text file is pretty big; so I thought that the lists would fit the purpose.You probably wantinstead ofHere's the difference:Since list.extend() accepts an arbitrary iterable, you can also replacebyTake a look at itertools.chain for a fast way to treat many small lists as a single big list (or at least as a single big iterable) without copying the smaller lists:That seems fairly reasonable for what you're trying to do.A slightly shorter version which leans on Python to do more of the heavy lifting might be:The (True for line in list1 if "string" in line) iterates over list and emits True whenever a match is found. any() uses short-circuit evaluation to return True as soon as the first True element is found. list2.extend() appends the contents of list1 to the end.To recap on the previous answers. If you have a list with [0,1,2] and another one with [3,4,5] and you want to merge them, so it becomes [0,1,2,3,4,5], you can either use chaining or extending and should know the differences to use it wisely for your needs.Using the list classes extend method, you can do a copy of the elements from one list onto another. However this will cause extra memory usage, which should be fine in most cases, but might cause problems if you want to be memory efficient.Contrary you can use itertools.chain to wire many lists, which will return a so called iterator that can be used to iterate over the lists. This is more memory efficient as it is not copying elements over but just pointing to the next list.Using the map() and reduce() built-in functionsMinimal "for looping" and elegant coding pattern :)You can also combine two lists (say a,b) using the '+' operator.

For example,If we have list like below: two ways to copy it into another list.1.2.

Using global variables between files?

Hai Vu

[Using global variables between files?](https://stackoverflow.com/questions/13034496/using-global-variables-between-files)

I'm bit confused about how the global variables work. I have a large project, with around 50 files, and I need to define global variables for all those files.What I did was define them in my projects main.py file, as following:I'm trying to use myList in subfile.py, as followingAn other way I tried, but didn't work eitherAnd inside subfile.py I had this:But again, it didn't work. How should I implement this? I understand that it cannot work like that, when the two files don't really know each other (well subfile doesn't know main), but I can't think of how to do it, without using io writing or pickle, which I don't want to do.

2012-10-23 15:54:30Z

I'm bit confused about how the global variables work. I have a large project, with around 50 files, and I need to define global variables for all those files.What I did was define them in my projects main.py file, as following:I'm trying to use myList in subfile.py, as followingAn other way I tried, but didn't work eitherAnd inside subfile.py I had this:But again, it didn't work. How should I implement this? I understand that it cannot work like that, when the two files don't really know each other (well subfile doesn't know main), but I can't think of how to do it, without using io writing or pickle, which I don't want to do.The problem is you defined myList from main.py, but subfile.py needs to use it. Here is a clean way to solve this problem: move all globals to a file, I call this file settings.py. This file is responsible for defining globals and initializing them:Next, your subfile can import globals:Note that subfile does not call init()— that task belongs to main.py:This way, you achieve your objective while avoid initializing global variables more than once.See Python's document on sharing global variables across modules:You can think of Python global variables as "module" variables - and as such they are much more useful than the traditional "global variables" from C.A global variable is actually defined in a module's __dict__ and can be accessed from outside that module as a module attribute.So, in your example:And:Using from your_file import * should fix your problems. It defines everything so that it is globally available (with the exception of local variables in the imports of course).for example:and:Hai Vu answer works great, just one comment: In case you are using the global in other module and you want to set the global dynamically, pay attention to import the other modules after you set the global variables, for example:Your 2nd attempt will work perfectly, and is actually a really good way to handle variable names that you want to have available globally.  But you have a name error in the last line.  Here is how it should be:See the last line?  myList is an attr of globfile, not subfile.  This will work as you want.Mike

Virtualenv Command Not Found

Arial

[Virtualenv Command Not Found](https://stackoverflow.com/questions/31133050/virtualenv-command-not-found)

I couldn't get virtualenv to work despite various attempts. I installed virtualenv on MAC OS X using:and have also added the PATH into my .bash_profile. Every time I try to run the virtualenv command, it returns:Every time I run pip install virtualenv, it returns:I understand that in mac, the virtualenv should be correctly installed in The virtualenv is indeed installed in /usr/local/bin, but whenever I try to run the virtualenv command, the command is not found. I've also tried to run the virtualenv command in the directory /usr/local/bin, and it gives me the same result: These are the PATHs I added to my .bash_profileAny workarounds for this? Why is this the case? 

2015-06-30 08:17:41Z

I couldn't get virtualenv to work despite various attempts. I installed virtualenv on MAC OS X using:and have also added the PATH into my .bash_profile. Every time I try to run the virtualenv command, it returns:Every time I run pip install virtualenv, it returns:I understand that in mac, the virtualenv should be correctly installed in The virtualenv is indeed installed in /usr/local/bin, but whenever I try to run the virtualenv command, the command is not found. I've also tried to run the virtualenv command in the directory /usr/local/bin, and it gives me the same result: These are the PATHs I added to my .bash_profileAny workarounds for this? Why is this the case? If you installed it withYou need to runwhich puts it in /usr/local/bin/.The above directory by default should be in your PATH; otherwise, edit your .zshrc (or .bashrc) accordingly. I faced the same issue and this is how I solved it:The simplest answer. Just:and then:Or you maybe installed virtualenv with sudo, in that case:On Ubuntu 18.04 LTS I also faced same error.

Following command worked:I had same problem on Mac OS X El Capitan.When I installed virtualenv like that sudo pip3 install virtualenv I didn't have virtualenv under my command line.I solved this problem by following those steps:Figure out the problemTry installing with the --verbose flagOutput will look something like thisFrom the output we can see that it's installed at /home/manos/.local/bin/virtualenv so let's ensure PATH includes that.In my case we can clearly see that /home/manos/.local/bin is totally missing and that's why the shell can't find the program.SolutionsWe can solve this in many ways:The two last options are probably the most sensible. The last solution is the simplest so therefore I will just show solution 3.Add this to ~/.profile:Logout out and in again and it should work.In my case, I ran pip show virtualenv to get the information about virtualenv package. I will look similar to this and will also show location of the package:From that grab the part of location up to the .local part, which in this case is /home/user/.local/. You can find virtualenv command under /home/user/.local/bin/virtualenv.You can then run commands like /home/user/.local/bin/virtualenv newvirtualenv.You said that every time you run the pip install you get Requirement already satisfied (use --upgrade to upgrade): virtualenv in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages. What you need to do is the following:Hope this helps. My advice would be to research venvs more. Here is a good resource: https://www.dabapps.com/blog/introduction-to-pip-and-virtualenv-python/I had troubles because I used apt to install python-virtualenv package.

To get it working I had to remove this package with apt-get remove python-virtualenv and install it with pip install virtualenv.python -m virtualenv virtualenv_name   Ensure that virtualenv is executable.If virtualenv is not found, running the full path (/usr/local/bin/virtualenv) should work.I had the same issue. I used the following steps to make it workThat is it. It started working.Usage of sudo -H----> sudo -H:  set HOME variable to target user's home dir.I think your problem can be solved using a simple symbolic link, but you are creating the symbolic link to the wrong file. As far as I know virtualenv is installed to /Library/Frameworks/Python.framework/Versions/2.7/bin/virtualenv, (you can change the numbers for your Python version) so the command for creating the symbolic link should be:On ubuntu 18.4 on AWS installation with pip don't work correctly. 

Using apt-get install the problem was solved for me.and to check Same problem:

So I just did pip uninstall virtualenv

Then  pip install virtualenvCollecting virtualenv

  Using cached https://files.pythonhosted.org/packages/b6/30/96a02b2287098b23b875bc8c2f58071c35d2efe84f747b64d523721dc2b5/virtualenv-16.0.0-py2.py3-none-any.whl

Installing collected packages: virtualenvThen I got this :which clearly says where it is installed and what to do to get itIf you're using Linux, open your terminal and type virtualenv halfway and autocomplete with tab key. If there's no auto-completion install virtualenv on your system by running:  You can now navigate to where you want to create your project and do:Follow these basic steps to setup the virtual envwe need to update our ~/.bashrcThe ~/.bashrc file is simply a shell script that Bash runs whenever you launch a new terminal. You normally use this file to set various configurations. In this case, we are setting an environment variable called WORKON_HOME  to point to the directory where our Python virtual environments live. We then load any necessary configurations from virtualenvwrapper .To update your ~/.bashrc file simply use a standard text editor, nano  is likely the easiest to operate.

A more simple solution is to use the cat  command and avoid editors entirely:After editing our ~/.bashrc  file, we need to reload the changes:Now that we have installed virtualenv  and virtualenvwrapper , the next step is to actually create the Python virtual environment — we do this using the mkvirtualenv  command.For me it was installed in this path (python 2.7 on MacOS):

$HOME/Library/Python/2.7/bin3 commands and everything working!

Proper way to handle multiple forms on one page in Django

Adam Nelson

[Proper way to handle multiple forms on one page in Django](https://stackoverflow.com/questions/1395807/proper-way-to-handle-multiple-forms-on-one-page-in-django)

I have a template page expecting two forms.  If I just use one form, things are fine as in this typical example:If I want to work with multiple forms however, how do I let the view know that I'm submitting only one of the forms and not the other (i.e. it's still request.POST but I only want to process the form for which the submit happened)?This is the solution based on the answer where expectedphrase and bannedphrase are the names of the submit buttons for the different forms and expectedphraseform and bannedphraseform are the forms.

2009-09-08 19:17:09Z

I have a template page expecting two forms.  If I just use one form, things are fine as in this typical example:If I want to work with multiple forms however, how do I let the view know that I'm submitting only one of the forms and not the other (i.e. it's still request.POST but I only want to process the form for which the submit happened)?This is the solution based on the answer where expectedphrase and bannedphrase are the names of the submit buttons for the different forms and expectedphraseform and bannedphraseform are the forms.You have a few options:A method for future reference is something like this.  bannedphraseform is the first form and expectedphraseform is the second.  If the first one is hit, the second one is skipped (which is a reasonable assumption in this case):Django's class based views provide a generic FormView but for all intents and purposes it is designed to only handle one form. One way to handle multiple forms with same target action url using Django's generic views is to extend the 'TemplateView' as shown below; I use this approach often enough that I have made it into an Eclipse IDE template.The html template is to the following effect:I needed multiple forms that are independently validated on the same page. The key concepts I was missing were 1) using the form prefix for the submit button name and 2) an unbounded form does not trigger validation. If it helps anyone else, here is my simplified example of two forms AForm and BForm using TemplateView based on the answers by @adam-nelson and @daniel-sokolowski and comment by @zeraien (https://stackoverflow.com/a/17303480/2680349):This is a bit late, but this is the best solution I found. You make a look-up dictionary for the form name and its class, you also have to add an attribute to identify the form, and in your views you have to add it as a hidden field, with the form.formlabel.I hope this will help in the future.If you are using approach with class-based views and different 'action' attrs i mean You can easily handle errors from different forms using overloaded get_context_data method, e.x: views.py:template:view:template:Wanted to share my solution where Django Forms are not being used.

I have multiple form elements on a single page and I want to use a single view to manage all the POST requests from all the forms. What I've done is I have introduced an invisible input tag so that I can pass a parameter to the views to check which form has been submitted.views.pyHere is simple way to handle the above.In Html Template we put PostIn View In URL

Give needed info like 

How do I check (at runtime) if one class is a subclass of another?

snakile

[How do I check (at runtime) if one class is a subclass of another?](https://stackoverflow.com/questions/4912972/how-do-i-check-at-runtime-if-one-class-is-a-subclass-of-another)

Let's say that I have a class Suit and four subclasses of suit: Heart, Spade, Diamond, Club.I have a method which receives a suit as a parameter, which is a class object, not an instance. More precisely, it may receive only one of the four values: Heart, Spade, Diamond, Club. How can I make an assertion which ensures such a thing? Something like:I'm using Python 3.

2011-02-06 11:26:01Z

Let's say that I have a class Suit and four subclasses of suit: Heart, Spade, Diamond, Club.I have a method which receives a suit as a parameter, which is a class object, not an instance. More precisely, it may receive only one of the four values: Heart, Spade, Diamond, Club. How can I make an assertion which ensures such a thing? Something like:I'm using Python 3.You can use issubclass() like this assert issubclass(suit, Suit).issubclass(class, classinfo)Excerpt:You can use isinstance if you have an instance, or issubclass if you have a class. Normally thought its a bad idea. Normally in Python you work out if an object is capable of something by attempting to do that thing to it.The issubclass(sub, sup) boolean function returns true if the given subclass sub is indeed a subclass of the superclass sup.issubclass minimal runnable exampleHere is a more complete example with some assertions:GitHub upstream.Tested in Python 3.5.2.You can use the builtin issubclass. But type checking is usually seen as unneccessary because you can use duck-typing.Using issubclass seemed like a clean way to write loglevels. It kinda feels odd using it... but it seems cleaner than other options. According to the Python doc, we can also use class.__mro__ attribute or class.mro() method:

Pythonic way to add datetime.date and datetime.time objects

jb.

[Pythonic way to add datetime.date and datetime.time objects](https://stackoverflow.com/questions/8474670/pythonic-way-to-add-datetime-date-and-datetime-time-objects)

I have two objects that represent the same event instance --- one holds the date, the other the time of this event, and I want to create a datetime object. Since one can't simply add date and time objects (following call fails): 

2011-12-12 12:57:52Z

I have two objects that represent the same event instance --- one holds the date, the other the time of this event, and I want to create a datetime object. Since one can't simply add date and time objects (following call fails): It's in the python docs.returns

How to keep a Python script output window open?

rohitmishra

[How to keep a Python script output window open?](https://stackoverflow.com/questions/1000900/how-to-keep-a-python-script-output-window-open)

I have just started with Python. When I execute a python script file on Windows, the output window appears but instantaneously goes away. I need it to stay there so I can analyze my output. How can I keep it open?

2009-06-16 11:31:16Z

I have just started with Python. When I execute a python script file on Windows, the output window appears but instantaneously goes away. I need it to stay there so I can analyze my output. How can I keep it open?You have a few options:cmd /k is the typical way to open any console application (not only Python) with a console window that will remain after the application closes. The easiest way I can think to do that, is to press Win+R, type cmd /k and then drag&drop the script you want to the Run dialog.Start the script from already open cmd window or

at the end of script add something like this, in Python 2:Or, in Python 3:To keep your window open in case of exception (yet, while printing the exception)To keep the window open in any case:For Python3 you'll have to use input() in place of raw_input(), and of course adapt the print statements.To keep the window open in any case:you can combine the answers before: (for Notepad++ User)press F5 to run current script and type in command: in this way you stay in interactive mode after executing your Notepad++ python script and you are able to play around with your variables and so on :)Create a Windows batch file with these 2 lines:In python 2 you can do it with: raw_input()In python 3 you can do it with: input()  Also, you can do it with the time.sleep(time)Using atexit, you can pause the program right when it exits. If an error/exception is the reason for the exit, it will pause after printing the stacktrace.In my program, I put the call to atexit.register in the except clause, so that it will only pause if something went wrong.I had a similar problem. With Notepad++ I used to use the command : C:\Python27\python.exe "$(FULL_CURRENT_PATH)" which closed the cmd window immediately after the code terminated.

Now I am using cmd /k c:\Python27\python.exe "$(FULL_CURRENT_PATH)" which keeps the cmd window open.On Python 3Will do the trick. To just keep the window open I agree with Anurag and this is what I did to keep my windows open for short little calculation type programs.This would just show a cursor with no text:This next example would give you a clear message that the program is done and not waiting on another input prompt within the program:In this next example, I use double quotes and it won't work because it thinks there is a break in the quotes between "the" and "function" even though when you read it, your own mind can make perfect sense of it:Hopefully this helps others who might be starting out and still haven't figured out how the computer thinks yet. It can take a while. :o)If you want to run your script from a desktop shortcut, right click your python file and select Send to|Desktop (create shortcut). Then right click the shortcut and select Properties. On the Shortcut tab select the Target: text box and add cmd /k  in front of the path and click OK. The shortcut should now run your script without closing and you don't need the input('Hit enter to close')Note, if you have more than one version of python on your machine, add the name of the required python executable between cmd /k and the scipt path like this:Apart from input and raw_input, you could also use an infinite while loop, like this:

while True: pass (Python 2.5+/3) or while 1: pass (all versions of Python 2/3). This might use computing power, though.You could also run the program from the command line. Type python into the command line (Mac OS X Terminal) and it should say Python 3.?.? (Your Python version) It it does not show your Python version, or says python: command not found, look into changing PATH values (enviromentl values, listed above)/type C:\(Python folder\python.exe. If that is successful, type python or C:\(Python installation)\python.exe and the full directory of your program.A very belated answer, but I created a Windows Batch file called pythonbat.bat containing the following:and then specified pythonbat.bat as the default handler for .py files.Now, when I double-click a .py file in File Explorer, it opens a new console window, runs the Python script and then pauses (remains open), until I press any key...No changes required to any Python scripts.I can still open a console window and specify python myscript.py if I want to...(I just noticed @maurizio already posted this exact answer)You can open PowerShell and type "python".

After Python has been imported, you can copy paste the source code from your favourite text-editor to run the code.The window won't close.If you want to stay cmd-window open AND be in running-file directory this works at Windows 10:The simplest way:this will leave the code up for 1 minute then close it.A simple hack to keep the window open:The counter is so the code won’t repeat itself.On windows 10 insert at beggining this:Strange, but it work for me!(Together with  input() at the end, of course)

Convert a list to a dictionary in Python

Mike

[Convert a list to a dictionary in Python](https://stackoverflow.com/questions/4576115/convert-a-list-to-a-dictionary-in-python)

Let's say I have a list a in Python whose entries conveniently map to a dictionary. Each even element represents the key to the dictionary, and the following odd element is the valuefor example,and I'd like to convert it to a dictionary b, where What is the syntactically cleanest way to accomplish this?

2011-01-01 22:29:00Z

Let's say I have a list a in Python whose entries conveniently map to a dictionary. Each even element represents the key to the dictionary, and the following odd element is the valuefor example,and I'd like to convert it to a dictionary b, where What is the syntactically cleanest way to accomplish this?If a is large, you will probably want to do something like the following, which doesn't make any temporary lists like the above.In Python 3 you could also use a dict comprehension, but ironically I think the simplest way to do it will be with range() and len(), which would normally be a code smell.So the iter()/izip() method is still probably the most Pythonic in Python 3, although as EOL notes in a comment, zip() is already lazy in Python 3 so you don't need izip().If you want it on one line, you'll have to cheat and use a semicolon.  ;-)Another option (courtesy of Alex Martelli https://stackoverflow.com/a/2597178/104264):If you have this:and you want this (each element of the list keying a given value (2 in this case)):you can use:You can use a dict comprehension for this pretty easily:This is equivalent to the for loop below:Something i find pretty cool, which is that if your list is only 2 items long:Remember, dict accepts any iterable containing an iterable where each item in the iterable must itself be an iterable with exactly two objects.May not be the most pythonic, butYou can do it pretty fast without creating extra arrays, so this will work even for very large arrays:If you have a generator a, even better:Here's the rundown:You can also do it like this (string to list conversion here, then conversion to a dictionary)I am also very much interested to have a one-liner for this conversion, as far such a list is the default initializer for hashed in Perl.Exceptionally comprehensive answer is given in this thread -Mine one I am newbie in Python), using Python 2.7 Generator Expressions, would be:dict((a[i], a[i + 1]) for i in range(0, len(a) - 1, 2))I am not sure if this is pythonic, but seems to worktry below code:You can also try this approach save the keys and values in different list and then use dict methodoutput:

ImportError: No module named PIL

Asma Gheisari

[ImportError: No module named PIL](https://stackoverflow.com/questions/8863917/importerror-no-module-named-pil)

I use this command in the shell to install PIL:then I run python and type this: import PIL. But I get this error:I've never had such problem, what do you think?

2012-01-14 17:24:27Z

I use this command in the shell to install PIL:then I run python and type this: import PIL. But I get this error:I've never had such problem, what do you think?On some installs of PIL, You must doinstead of import PIL (PIL is in fact not always imported this way).  Since import Image works for you, this means that you have in fact installed PIL.Having a different name for the library and the Python module is unusual, but this is what was chosen for (some versions of) PIL.You can get more information about how to use this module from the official tutorial.PS: In fact, on some installs, import PIL does work, which adds to the confusion. This is confirmed by an example from the documentation, as @JanneKarila found out, and also by some more recent versions of the MacPorts PIL package (1.1.7).In shell, run:Attention: PIL is deprecated, and pillow is the successor.On a different note, I can highly recommend the use of Pillow which is backwards compatible with PIL and is better maintained/will work on newer systems.When that is installed you can do oretc..At first install Pillow with or as followsThen in python code you may call"Pillow is a fork of PIL, the Python Imaging Library, which is no longer maintained. However, to maintain backwards compatibility, the old module name is used." From pillow installed, but "no module named pillow" - python2.7 - Windows 7 - python -m install pillowSometimes I get this type of error running a Unitest in python. The solution is  to uninstall and install the same package  on your  virtual environment.  Using this commands: andIf for any reason  you get an error, add  sudo at the beginning of the command  and after hitting enter  type your password. This worked for me on Ubuntu 16.04:I found this on Wikibooks after searching for about half an hour.I used:and pip installed PIL in Lib\site-packages. When I moved PIL to Lib everything worked fine. I'm on Windows 10.you have to install Image and pillow with your python package.type or run command prompt (in windows), then navigate to the scripts folderthen run below commandif you use anaconda:On windows, try checking the path to the location of the PIL library.  On my system, I noticed the path was after  renaming the pil folder to PIL, I was able to load the PIL module.You will need to install Image and pillow with your python package. 

Rest assured, the command line will take care of everything for you. Hit python -m pip install imageOn windows 10 I managed to get there with:after which in python (python 3.7 in my case) this works fine...instead of PIL use Pillow it worksorOn  Windows, you need to download it and install the .exehttps://pypi.python.org/pypi/Pillow/2.7.0I used conda-forge to install pillow version 5, and that seemed to work for me: the normal conda install pillow did NOT work for me.I had the same issue while importing PIL and further importing the ImageTk and Image modules. I also tried installing PIL directly through pip. but not success could be achieved. As in between it has been suggested that PIL has been deprectaed, thus, tried to install pillow through pip only. pillow was successfully installed, further, the PIL package was made under the path : python27/Lib/site-packages/. Now both Image and ImageTk could be imported. I recently installed Leap. I Tried openshot and it didn't start. So came here and found a suggestion to start from the Terminal to see if there were any error. The error I had was error missing mlt. So I installed the python-mlt module from Yast and imported it, tried to start but next openshot said missing pil.I Followed the Pillow suggestion to install because Yast couldn't find any pil and imported pil. That went ok but did not start and showed Error missing goocanvas. The I installed goocanvas with Yast, imported it in python, and Openshot fired up !!With a lot of errors in the terminal like missing Vimeoclient and lots of attributeerrors. Well, will see if it is of any influence working with it.I had the same problem and i fixed it by checking what version pip (pip3 --version) is, then realizing I'm typing python<uncorrect version> filename.py instead of python<correct version> filename.py You are probably missing the python headers to build pil. If you're using ubuntu or the likes it'll be something like 

Python 3: UnboundLocalError: local variable referenced before assignment [duplicate]

Eden Crow

[Python 3: UnboundLocalError: local variable referenced before assignment [duplicate]](https://stackoverflow.com/questions/10851906/python-3-unboundlocalerror-local-variable-referenced-before-assignment)

The following code gives the error UnboundLocalError: local variable 'Var1' referenced before assignment:How can I fix this? Thanks for any help!

2012-06-01 14:13:28Z

The following code gives the error UnboundLocalError: local variable 'Var1' referenced before assignment:How can I fix this? Thanks for any help!You can fix this by passing parameters rather than relying on GlobalsThis is because, even though Var1 exists, you're also using an assignment statement on the name Var1 inside of the function (Var1 -= 1 at the bottom line). Naturally, this creates a variable inside the function's scope called Var1 (truthfully, a -= or += will only update (reassign) an existing variable, but for reasons unknown (likely consistency in this context), Python treats it as an assignment). The Python interpreter sees this at module load time and decides (correctly so) that the global scope's Var1 should not be used inside the local scope, which leads to a problem when you try to reference the variable before it is locally assigned.Using global variables, outside of necessity, is usually frowned upon by Python developers, because it leads to confusing and problematic code. However, if you'd like to use them to accomplish what your code is implying, you can simply add:inside the top of your function. This will tell Python that you don't intend to define a Var1 or Var2 variable inside the function's local scope. The Python interpreter sees this at module load time and decides (correctly so) to look up any references to the aforementioned variables in the global scope.If you set the value of a variable inside the function, python understands it as creating a local variable with that name. This local variable masks the global variable.In your case, Var1 is considered as a local variable, and it's used before being set, thus the error.To solve this problem, you can explicitly say it's a global by putting global Var1 in you function.I don't like this behavior, but this is how Python works. The question has already been answered by others, but for completeness, let me point out that Python 2 has more such quirks.Python 2.7.6 returns an error:Python sees the f is used as a local variable in [f for f in [1, 2, 3]], and decides that it is also a local variable in f(3). You could add a global f statement:It does work; however, f becomes 3 at the end... That is, print [f for f in [1, 2, 3]] now changes the global variable f to 3, so it is not a function any more.Fortunately, it works fine in Python3 after adding the parentheses to print.Why not simply return your calculated value and let the caller modify the global variable. It's not a good idea to manipulate a global variable within a function, as below:or even make local copies of the global variables and work with them and return the results which the caller can then assign appropriately

Create numpy matrix filled with NaNs

devoured elysium

[Create numpy matrix filled with NaNs](https://stackoverflow.com/questions/1704823/create-numpy-matrix-filled-with-nans)

I have the following code:It creates a width x height x 9 matrix filled with zeros. Instead, I'd like to know if there's a function or way to initialize them instead to NaNs in an easy way.

2009-11-10 00:10:07Z

I have the following code:It creates a width x height x 9 matrix filled with zeros. Instead, I'd like to know if there's a function or way to initialize them instead to NaNs in an easy way.You rarely need loops for vector operations in numpy.

You can create an uninitialized array and assign to all entries at once:I have timed the alternatives a[:] = numpy.nan here and a.fill(numpy.nan) as posted by Blaenk:The timings show a preference for ndarray.fill(..) as the faster alternative. OTOH, I like numpy's convenience implementation where you can assign values to whole slices at the time, the code's intention is very clear.Another option is to use numpy.full, an option available in NumPy 1.8+This is pretty flexible and you can fill it with any other number that you want.I compared the suggested alternatives for speed and found that, for large enough vectors/matrices to fill, all alternatives except val * ones and array(n * [val]) are equally fast.Code to reproduce the plot:Are you familiar with numpy.nan?You can create your own method such as:Thenwould outputI found this code in a mailing list thread.You can always use multiplication if you don't immediately recall the .empty or .full methods:Of course it works with any other numerical value as well:But the @u0b34a0f6ae's accepted answer is 3x faster (CPU cycles, not brain cycles to remember numpy syntax ;):As said, numpy.empty() is the way to go. However, for objects, fill() might not do exactly what you think it does:One way around can be e.g.:Another alternative is numpy.broadcast_to(val,n) which returns in constant time regardless of the size and is also the most memory efficient (it returns a view of the repeated element). The caveat is that the returned value is read-only.Below is a comparison of the performances of all the other methods that have been proposed using the same benchmark as in Nico Schlömer's answer.Yet another possibility not yet mentioned here is to use NumPy tile:Also givesI don't know about speed comparison.

Count unique values with pandas per groups [duplicate]

Arseniy Krupenin

[Count unique values with pandas per groups [duplicate]](https://stackoverflow.com/questions/38309729/count-unique-values-with-pandas-per-groups)

I need to count unique ID values in every domain

I have dataI try df.groupby(['domain', 'ID']).count()

But I want to get

2016-07-11 14:38:25Z

I need to count unique ID values in every domain

I have dataI try df.groupby(['domain', 'ID']).count()

But I want to getYou need nunique:If you need to strip ' characters:Or as Jon Clements commented:You can retain the column name like this:The difference is that nunique() returns a Series and agg() returns a DataFrame.Generally to count distinct values in single column, you can use Series.value_counts:To see how many unique values in a column, use Series.nunique:To get all these distinct values, you can use unique or drop_duplicates, the slight difference between the two functions is that unique return a numpy.array while drop_duplicates returns a pandas.Series:As for this specific problem, since you'd like to count distinct value with respect to another variable, besides groupby method provided by other answers here, you can also simply drop duplicates firstly and then do value_counts():df.domain.value_counts()IIUC you want the number of different ID for every domain, then you can try this:output:You could also use value_counts, which is slightly less efficient.But the best is Jezrael's answer using nunique:

How to declare array of zeros in python (or an array of a certain size) [duplicate]

user491880

[How to declare array of zeros in python (or an array of a certain size) [duplicate]](https://stackoverflow.com/questions/4056768/how-to-declare-array-of-zeros-in-python-or-an-array-of-a-certain-size)

I am trying to build a histogram of counts... so I create buckets.

I know I could just go through and append a bunch of zeros i.e something along these lines:Is there a  more elegant way to do it? I feel like there should be a way to just declare an array of a certain size.I know numpy has numpy.zeros but I want the more general solution

2010-10-30 00:44:58Z

I am trying to build a histogram of counts... so I create buckets.

I know I could just go through and append a bunch of zeros i.e something along these lines:Is there a  more elegant way to do it? I feel like there should be a way to just declare an array of a certain size.I know numpy has numpy.zeros but I want the more general solutionCareful - this technique doesn't generalize to multidimensional arrays or lists of lists. Which leads to the List of lists changes reflected across sublists unexpectedly problemJust for completeness: To declare a multidimensional list of zeros in python you have to use a list comprehension like this:to avoid reference sharing between the rows.This looks more clumsy than chester1000's code, but is essential if the values are supposed to be changed later. See the Python FAQ for more details.You can multiply a list by an integer n to repeat the list n times:Use this:ORBoth of them will output proper empty multidimensional bucket list 100x100use numpyAnd then use the Histogram library functionThe question says "How to declare array of zeros ..." but then the sample code references the Python list:However, if someone is actually wanting to initialize an array, I suggest:The Python purist might claim this is not pythonic and suggest:The pythonic version is very slow and when you have a few hundred arrays to be initialized with thousands of values, the difference is quite noticeable.The simplest solution would beIn general you should use more pythonic code like list comprehension (in your example: [0 for unused in xrange(100)]) or using string.join for buffers.Depending on what you're actually going to do with the data after it's collected, collections.defaultdict(int) might be useful.Well I would like to help you by posting a sample program and its outputProgram :-Output :-I hope this clears some very basic concept of yours regarding their declaration. To initialize them with some other specific values,like initializing them with 0..you can declare them as :x=[0]*10Hope it helps..!! ;)If you need more columns:

How to split a column into two columns?

a k

[How to split a column into two columns?](https://stackoverflow.com/questions/14745022/how-to-split-a-column-into-two-columns)

I have a data frame with one column and I'd like to split it into two columns, with one column header as 'fips' and the other 'row'My dataframe df looks like this:I do not know how to use df.row.str[:] to achieve my goal of splitting the row cell. I can use df['fips'] = hello to add a new column and populate it with hello. Any ideas? 

2013-02-07 06:30:11Z

I have a data frame with one column and I'd like to split it into two columns, with one column header as 'fips' and the other 'row'My dataframe df looks like this:I do not know how to use df.row.str[:] to achieve my goal of splitting the row cell. I can use df['fips'] = hello to add a new column and populate it with hello. Any ideas? There might be a better way, but this here's one approach:For the simple case of:The simplest solution is:Or you can create create a DataFrame with one column for each entry of the split automatically with:You must use expand=True if your strings have a non-uniform number of splits and you want None to replace the missing values.Notice how, in either case, the .tolist() method is not necessary. Neither is zip().Andy Hayden's solution is most excellent in demonstrating the power of the str.extract() method.But for a simple split over a known separator (like, splitting by dashes, or splitting by whitespace), the .str.split() method is enough1. It operates on a column (Series) of strings, and returns a column (Series) of lists:1: If you're unsure what the first two parameters of .str.split() do,

 I recommend the docs for the plain Python version of the method.But how do you go from:to:Well, we need to take a closer look at the .str attribute of a column.It's a magical object that is used to collect methods that treat each element in a column as a string, and then apply the respective method in each element as efficient as possible:But it also has an "indexing" interface for getting each element of a string by its index:Of course, this indexing interface of .str doesn't really care if each element it's indexing is actually a string, as long as it can be indexed, so:Then, it's a simple matter of taking advantage of the Python tuple unpacking of iterables to doOf course, getting a DataFrame out of splitting a column of strings is so useful that the .str.split() method can do it for you with the expand=True parameter:So, another way of accomplishing what we wanted is to do:The expand=True version, although longer, has a distinct advantage over the tuple unpacking method. Tuple unpacking doesn't deal well with splits of different lengths:But expand=True handles it nicely by placing None in the columns for which there aren't enough "splits":You can extract the different parts out quite neatly using a regex pattern:To explain the somewhat long regex:The next part:Does either (|) one of two things:orIn the example:

Note that the first two rows hit the "state" (leaving NaN in  the county and state_code columns), whilst the last three hit the county, state_code (leaving NaN in the state column).If you don't want to create a new dataframe, or if your dataframe has more columns than just the ones you want to split, you could:You can use str.split by whitespace (default separator) and parameter expand=True for DataFrame with assign to new columns:Modification if need remove original column with DataFrame.popWhat is same like:If get error:You can check and it return 4 column DataFrame, not only 2:Then solution is append new DataFrame by join:With remove original column (if there are also another columns):If you want to split a string into more than two columns based on a delimiter you can omit the 'maximum splits' parameter.

You can use: This will automatically create as many columns as the maximum number of fields included in any of your initial strings. Surprised I haven't seen this one yet. If you only need two splits, I highly recommend. . .partition performs one split on the separator, and is generally quite performant.If you need to rename the rows, If you need to join this back to the original, use join or concat:I prefer exporting the corresponding pandas series (i.e. the columns I need), using the apply function to split the column content into multiple series and then join the generated columns to the existing DataFrame. Of course, the source column should be removed.e.g. To split two words strings function should be something like that:I saw that no one had used the slice method, so here I put my 2 cents here.This method will create two new columns.Use df.assgin to create a new df. See http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

pandas three-way joining multiple dataframes on columns

lollercoaster

[pandas three-way joining multiple dataframes on columns](https://stackoverflow.com/questions/23668427/pandas-three-way-joining-multiple-dataframes-on-columns)

I have 3 CSV files. Each has the first column as the (string) names of people, while all the other columns in each dataframe are attributes of that person. How can I "join" together all three CSV documents to create a single CSV with each row having all the attributes for each unique value of the person's string name?The join() function in pandas specifies that I need a multiindex, but I'm confused about what a hierarchical indexing scheme has to do with making a join based on a single index. 

2014-05-15 02:51:40Z

I have 3 CSV files. Each has the first column as the (string) names of people, while all the other columns in each dataframe are attributes of that person. How can I "join" together all three CSV documents to create a single CSV with each row having all the attributes for each unique value of the person's string name?The join() function in pandas specifies that I need a multiindex, but I'm confused about what a hierarchical indexing scheme has to do with making a join based on a single index. Assumed imports:John Galt's answer is basically a reduce operation.  If I have more than a handful of dataframes, I'd put them in a list like this (generated via list comprehensions or loops or whatnot):Assuming they have some common column, like name in your example, I'd do the following:That way, your code should work with whatever number of dataframes you want to merge.Edit August 1, 2016: For those using Python 3: reduce has been moved into functools. So to use this function, you'll first need to import that module:You could try this if you have 3 dataframesalternatively, as mentioned by cwharland The join method is built exactly for these types of situations. You can join any number of DataFrames together with it. The calling DataFrame joins with the index of the collection of passed DataFrames. To work with multiple DataFrames, you must put the joining columns in the index.The code would look something like this:With @zero's data, you could do this:This can also be done as follows for a list of dataframes df_list:or if the dataframes are in a generator object (e.g. to reduce memory consumption):In python 3.6.3 with pandas 0.22.0 you can also use concat as long as you set as index the columns you want to use for the joiningwhere df1, df2, and df3 are defined as in John Galt's answerOne does not need a multiindex to perform join operations.

One just need to set correctly the index column on which to perform the join operations (which command df.set_index('Name') for example)The join operation is by default performed on index.

In your case, you just have to specify that the Name column corresponds to your index. 

Below is an exampleA tutorial may be useful.Here is a method to merge a dictionary of data frames while keeping the column names in sync with the dictionary. Also it fills in missing values if needed:There is another solution from the pandas documentation (that I don't see here),using the .appendThe ignore_index=True is used to ignore the index of the appended dataframe, replacing it with the next index available in the source one.If there are different column names, Nan will be introduced.Simple Solution:If the column names are similar:If the column names are different:

How is Pythons glob.glob ordered?

Martin Thoma

[How is Pythons glob.glob ordered?](https://stackoverflow.com/questions/6773584/how-is-pythons-glob-glob-ordered)

I have written the following Python code:Now I get this:In which way is it ordered?It might help you to get my ls -l output:It is not ordered by filename or size.Other links: glob, ls

2011-07-21 08:59:21Z

I have written the following Python code:Now I get this:In which way is it ordered?It might help you to get my ls -l output:It is not ordered by filename or size.Other links: glob, lsIt is probably not sorted at all and uses the order at which entries appear in the filesystem, i.e. the one you get when using ls -U. (At least on my machine this produces the same order as listing glob matches).Order is arbitrary, but you can sort them yourselfIf you want sorted by name:sorted by modification time:sorted by size:etc.By checking the source code of glob.glob you see that it internally calls os.listdir, described here:http://docs.python.org/library/os.html?highlight=os.listdir#os.listdirKey sentence: os.listdir(path)

Return a list containing the names of the entries in the directory given by path. The list is in arbitrary order. It does not include the special entries '.' and '..' even if they are present in the directory.Arbitrary order. :)glob.glob() is a wrapper around os.listdir() so the underlaying OS is in charge for delivering the data. In general: you can not make an assumption on the ordering here. The basic assumption is: no ordering. If you need some sorting: sort on the application level.I had a similar issue, glob was returning a list of file names in an arbitrary order but I wanted to step through them in numerical order as indicated by the file name. This is how I achieved it:My files were returned by glob something like:I sorted the list in place, to do this I created a function:This function returns the numeric part of the file name and converts to an integer.I then called the sort method on the list as such:This returned a list as such:Order is arbitrary, but there are several ways to sort them. One of them is as following:If you're wondering about what glob.glob has done on your system in the past and cannot add a sorted call, the ordering will be consistent on Mac HFS+ filesystems and will be traversal order on other Unix systems. So it will likely have been deterministic unless the underlying filesystem was reorganized which can happen if files were added, removed, renamed, deleted, moved, etc...Please try this code:That's how I did my particular case. Hope it's helpful.

How can I use a DLL file from Python?

Tom Hennen

[How can I use a DLL file from Python?](https://stackoverflow.com/questions/252417/how-can-i-use-a-dll-file-from-python)

What is the easiest way to use a DLL file from within Python?Specifically, how can this be done without writing any additional wrapper C++ code to expose the functionality to Python?Native Python functionality is strongly preferred over using a third-party library.

2008-10-31 02:01:48Z

What is the easiest way to use a DLL file from within Python?Specifically, how can this be done without writing any additional wrapper C++ code to expose the functionality to Python?Native Python functionality is strongly preferred over using a third-party library.For ease of use, ctypes is the way to go.The following example of ctypes is from actual code I've written (in Python 2.5).  This has been, by far, the easiest way I've found for doing what you ask.The ctypes stuff has all the C-type data types (int, char, short, void*, and so on) and can pass by value or reference.  It can also return specific data types although my example doesn't do that (the HLL API returns values by modifying a variable passed by reference).In terms of the specific example shown above, IBM's EHLLAPI is a fairly consistent interface.All calls pass four void pointers (EHLLAPI sends the return code back through the fourth parameter, a pointer to an int so, while I specify int as the return type, I can safely ignore it) as per IBM's documentation here. In other words, the C variant of the function would be:This makes for a single, simple ctypes function able to do anything the EHLLAPI library provides, but it's likely that other libraries will need a separate ctypes function set up per library function.The return value from WINFUNCTYPE is a function prototype but you still have to set up more  parameter information (over and above the types). Each tuple in hllApiParams has a parameter "direction" (1 = input, 2 = output and so on), a parameter name and a default value - see the ctypes doco for detailsOnce you have the prototype and parameter information, you can create a Python "callable" hllApi with which to call the function. You simply create the needed variable (p1 through p4 in my case) and call the function with them.This page has a very simple example of calling functions from a DLL file.Paraphrasing the details here for completeness:ctypes can be used to access dlls, here's a tutorial:http://docs.python.org/library/ctypes.html#module-ctypesMaybe with Dispatch:Where zkemkeeper is a registered DLL file on the system...

After that, you can access functions just by calling them:ctypes will be the easiest thing to use but (mis)using it makes Python subject to crashing. If you are trying to do something quickly, and you are careful, it's great.I would encourage you to check out Boost Python. Yes, it requires that you write some C++ code and have a C++ compiler, but you don't actually need to learn C++ to use it, and you can get a free (as in beer) C++ compiler from Microsoft.I present a fully worked example on how building a shared library and using it under Python by means of ctypes. I consider the Windows case and deal with DLLs. Two steps are needed:The shared library I consider is the following and is contained in the testDLL.cpp file. The only function testDLL just receives an int and prints it.To build a DLL with Visual Studio from the command line runto set the include path and then runto build the DLL.Alternatively, the DLL can be build using Visual Studio as follows:Under Python, do the following

I'm getting Key error in python

David Liaw

[I'm getting Key error in python](https://stackoverflow.com/questions/10116518/im-getting-key-error-in-python)

In my python program I am getting this error:From this code:Can anyone please explain why this is happening?

2012-04-12 02:11:27Z

In my python program I am getting this error:From this code:Can anyone please explain why this is happening?A KeyError generally means the key doesn't exist. So, are you sure the path key exists?From the official python docs:exception KeyErrorFor example:So, try to print the content of meta_entry and check whether path exists or not.Or, you can do:I fully agree with the Key error comments.  You could also use the dictionary's get() method as well to avoid the exceptions.  This could also be used to give a default path rather than None as shown below.For dict, just useif key in dictand not useif key in dict.keys()It will be time-consumingYes, it is most likely caused by non-exsistent key.In my program, I used setdefault to mute this error, for efficiency concern. 

depending on how efficient is this line I am new to Python too. In fact I have just learned it today. So forgive me on the ignorance of efficiency.In Python 3, you can also use this function,It is said that it will never raise a key error.This means your array is missing the key you're looking for. I handle this with a function which either returns the value if it exists or it returns a default value instead.Output:I received this error when I was parsing dict with nested for:Traceback:Because in second loop should be cats[cat] instead just cat (what is just a key)So:GivesFor example, if this is a number :It's work perfectly, but  if you use for example :it's doesn't work, because your input return string '1'. So you need to use int()

How can I convert an RGB image into grayscale in Python?

waspinator

[How can I convert an RGB image into grayscale in Python?](https://stackoverflow.com/questions/12201577/how-can-i-convert-an-rgb-image-into-grayscale-in-python)

I'm trying to use matplotlib to read in an RGB image and convert it to grayscale.In matlab I use this:In the matplotlib tutorial they don't cover it. They just read in the imageand then they slice the array, but that's not the same thing as converting RGB to grayscale from what I understand.I find it hard to believe that numpy or matplotlib doesn't have a built-in function to convert from rgb to gray. Isn't this a common operation in image processing?I wrote a very simple function that works with the image imported using imread in 5 minutes. It's horribly inefficient, but that's why I was hoping for a professional implementation built-in. Sebastian has improved my function, but I'm still hoping to find the built-in one.matlab's (NTSC/PAL) implementation:

2012-08-30 16:37:37Z

I'm trying to use matplotlib to read in an RGB image and convert it to grayscale.In matlab I use this:In the matplotlib tutorial they don't cover it. They just read in the imageand then they slice the array, but that's not the same thing as converting RGB to grayscale from what I understand.I find it hard to believe that numpy or matplotlib doesn't have a built-in function to convert from rgb to gray. Isn't this a common operation in image processing?I wrote a very simple function that works with the image imported using imread in 5 minutes. It's horribly inefficient, but that's why I was hoping for a professional implementation built-in. Sebastian has improved my function, but I'm still hoping to find the built-in one.matlab's (NTSC/PAL) implementation:How about doing it with Pillow:Using matplotlib and the formulayou could do:You can also use scikit-image, which provides some functions to convert an image in ndarray, like rgb2gray.Notes: The weights used in this conversion are calibrated for contemporary CRT phosphors:  Y = 0.2125 R + 0.7154 G + 0.0721 BAlternatively, you can read image in grayscale by:Three of the suggested methods were tested for speed with 1000 RGBA PNG images (224 x 256 pixels) running with Python 3.5 on Ubuntu 16.04 LTS (Xeon E5 2670 with SSD).Average run timespil  : 1.037 secondsscipy: 1.040 secondssk   : 2.120 secondsPIL and SciPy gave identical numpy arrays (ranging from 0 to 255). SkImage gives arrays from 0 to 1. In addition the colors are converted slightly different, see the example from the CUB-200 dataset.SkImage:  PIL    :  SciPy  :  Original: Diff    : CodeYou can always read the image file as grayscale right from the beginning using imread from OpenCV:Furthermore, in case you want to read the image as RGB, do some processing and then convert to Gray Scale you could use cvtcolor from OpenCV:The fastest and current way is to use Pillow, installed via pip install Pillow.The code is then:The tutorial is cheating because it is starting with a greyscale image encoded in RGB, so they are just slicing a single color channel and treating it as greyscale.  The basic steps you need to do are to transform from the RGB colorspace to a colorspace that encodes with something approximating the luma/chroma model, such as YUV/YIQ or HSL/HSV, then slice off the luma-like channel and use that as your greyscale image.  matplotlib does not appear to provide a mechanism to convert to YUV/YIQ, but it does let you convert to HSV.Try using matplotlib.colors.rgb_to_hsv(img) then slicing the last value (V) from the array for your grayscale.  It's not quite the same as a luma value, but it means you can do it all in matplotlib.Background:Alternatively, you could use PIL or the builtin colorsys.rgb_to_yiq() to convert to a colorspace with a true luma value.  You could also go all in and roll your own luma-only converter, though that's probably overkill.If you're using NumPy/SciPy already you may as well use:scipy.ndimage.imread(file_name, mode='L')Using this formula We can doHowever, the GIMP converting color to grayscale image software has three algorithms to do the task.you could do:  Use img.Convert(), supports「L」,「RGB」and「CMYK.」modeOutput: I came to this question via Google, searching for a way to convert an already loaded image to grayscale.Here is a way to do it with SciPy:You can use greyscale() directly for the transformation.

How do I copy a string to the clipboard on Windows using Python?

tester

[How do I copy a string to the clipboard on Windows using Python?](https://stackoverflow.com/questions/579687/how-do-i-copy-a-string-to-the-clipboard-on-windows-using-python)

I'm trying to make a basic Windows application that builds a string out of user input and then adds it to the clipboard. How do I copy a string to the clipboard using Python?

2009-02-23 22:38:02Z

I'm trying to make a basic Windows application that builds a string out of user input and then adds it to the clipboard. How do I copy a string to the clipboard using Python?Actually, pywin32 and ctypes seem to be an overkill for this simple task. Tkinter is a cross-platform GUI framework, which ships with Python by default and has clipboard accessing methods along with other cool stuff.If all you need is to put some text to system clipboard, this will do it:And that's all, no need to mess around with platform-specific third-party libraries.If you are using Python 3, replace TKinter with tkinter.I didn't have a solution, just a workaround.Windows Vista onwards has an inbuilt command called clip that takes the output of a command from command line and puts it into the clipboard. For example, ipconfig | clip.So I made a function with the os module which takes a string and adds it to the clipboard using the inbuilt Windows solution.As previously noted in the comments however, one downside to this approach is that the echo command automatically adds a newline to the end of your text. To avoid this you can use a modified version of the command:If you are using Windows XP it will work just following the steps in Copy and paste from Windows XP Pro's command prompt straight to the Clipboard.You can also use ctypes to tap into the Windows API and avoid the massive pywin32 package. This is what I use (excuse the poor style, but the idea is there):You can use pyperclip - cross-platform clipboard module. Or Xerox - similar module, except requires the win32 Python module to work on Windows.You can use the excellent pandas, which has a built in clipboard support, but you need to pass through a DataFrame.The simplest way is with pyperclip. Works in python 2 and 3.To install this library, use:Example usage:If you want to get the contents of the clipboard:For some reason I've never been able to get the Tk solution to work for me. kapace's solution is much more workable, but the formatting is contrary to my style and it doesn't work with Unicode. Here's a modified version.The above has changed since this answer was first created, to better cope with extended Unicode characters and Python 3. It has been tested in both Python 2.7 and 3.5, and works even with emoji such as \U0001f601 (😁).Looks like you need to add win32clipboard to your site-packages.  It's part of the pywin32 packageI've tried various solutions, but this is the simplest one that passes my test:Tested OK in Python 3.4 on Windows 8.1 and Python 2.7 on Windows 7. Also when reading Unicode data with Unix linefeeds copied from Windows. Copied data stays on the clipboard after Python exits: "Testing

the「clip—board」: 📋"If you want no external dependencies, use this code (now part of cross-platform pyperclip - C:\Python34\Scripts\pip install --upgrade pyperclip):Widgets also have method named .clipboard_get() that returns the contents of the clipboard (unless some kind of error happens based on the type of data in the clipboard).The clipboard_get() method is mentioned in this bug report:

http://bugs.python.org/issue14777Strangely, this method was not mentioned in the common (but unofficial) online TkInter documentation sources that I usually refer to.I think there is a much simpler solution to this.Then run your program in the command lineThis will pipe the output of your file to the clipboardYou can use module clipboard. Its simple and extremely easy to use. Works with Mac, Windows, & Linux.

Note: Its an alternative of pyperclipAfter installing, import it:Then you can copy like this:You can also paste the copied text:Here's the most easy and reliable way I found if you're okay depending on Pandas. However I don't think this is officially part of the Pandas API so it may break with future updates. It works as of 0.25.3In addition to Mark Ransom's answer using ctypes:

This does not work for (all?) x64 systems since the handles seem to be truncated to int-size.

Explicitly defining args and return values helps to overcomes this problem.The snippet I share here take advantage of the ability to format text files: what if you want to copy a complex output to the clipboard ? (Say a numpy array in column or a list of something)works only for windows, can be adapted for linux or mac I guess. Maybe a bit complicated...example:Ctrl+V in any text editor :This is the improved answer of atomizer.Note 2 calls of update() and 200 ms delay between them. They protect freezing applications due to an unstable state of the clipboard:Use python's clipboard library!Clipboard contains 'abc' now. Happy pasting!Not all of the answers worked for my various python configurations so this solution only uses the subprocess module. However, copy_keyword has to be pbcopy for Mac or clip for Windows:Here's some more extensive code that automatically checks what the current operating system is:You can use winclip32 module!

install:to copy:to get:for more informations: https://pypi.org/project/winclip32/Code snippet to copy the clipboard:Create a wrapper Python code in a module named (clipboard.py):Then import the above module into your code.I must give credit to the blog post Clipboard Access in IronPython.you can try this:

Check if a string contains a number

JamesDonnelly

[Check if a string contains a number](https://stackoverflow.com/questions/19859282/check-if-a-string-contains-a-number)

Most of the questions I've found are biased on the fact they're looking for letters in their numbers, whereas I'm looking for numbers in what I'd like to be a numberless string.

I need to enter a string and check to see if it contains any numbers and if it does reject it.The function isdigit() only returns True if ALL of the characters are numbers. I just want to see if the user has entered a number so a sentence like

"I own 1 dog" or something.Any ideas?

2013-11-08 12:37:06Z

Most of the questions I've found are biased on the fact they're looking for letters in their numbers, whereas I'm looking for numbers in what I'd like to be a numberless string.

I need to enter a string and check to see if it contains any numbers and if it does reject it.The function isdigit() only returns True if ALL of the characters are numbers. I just want to see if the user has entered a number so a sentence like

"I own 1 dog" or something.Any ideas?You can use any function, with the str.isdigit function, like thisAlternatively you can use a Regular Expression, like thisYou can use a combination of any and str.isdigit:The function will return True if a digit exists in the string, otherwise False.Demo:use Ref: https://docs.python.org/2/library/stdtypes.html#str.isalphahttps://docs.python.org/2/library/re.htmlYou should better use regular expression. It's much faster.You could apply the function isdigit() on every character in the String. Or you could use regular expressions.Also I found How do I find one number in a string in Python? with very suitable ways to return numbers. The solution below is from the answer in that question.Alternatively:For further Information take a look at the regex docu: http://docs.python.org/2/library/re.htmlEdit: This Returns the actual numbers, not a boolean value, so the answers above are more correct for your caseThe first method will return the first digit and subsequent consecutive digits. Thus 1.56 will be returned as 1.  10,000 will be returned as 10. 0207-100-1000 will be returned as 0207. The second method does not work.To extract all digits, dots and commas, and not lose non-consecutive digits, use:You can use NLTK method for it.This will find both '1' and 'One' in the text:You can accomplish this as follows:if a_string.isdigit():

     do_this()

 else:

     do_that()https://docs.python.org/2/library/stdtypes.html#str.isdigitUsing .isdigit() also means not having to resort to exception handling (try/except) in cases where you need to use list comprehension (try/except is not possible inside a list comprehension).You can use range with count to check how many times a number appears in the string by checking it against the range:I'm surprised that no-one mentionned this combination of any and map:in python 3 it's probably the fastest there (except maybe for regexes) is because it doesn't contain any loop (and aliasing the function avoids looking it up in str). Don't use that in python 2 as map returns a list, which breaks any short-circuitingWhat about this one?Simpler way to solve is asThis code generates a sequence with size n which at least contain an uppercase, lowercase, and a digit. By using the while loop, we have guaranteed this event.   any and ord can be combined to serve the purpose as shown below.A couple of points about this implementation.This returns all the string that has both alphabets and numbers in it. isalpha() returns the string with all digits or all characters.I'll make the @zyxue answer a bit more explicit:which is the solution with the fastest benchmark from the solutions that @zyxue proposed on the answer.

How to read a 6 GB csv file with pandas

Rajkumar Kumawat

[How to read a 6 GB csv file with pandas](https://stackoverflow.com/questions/25962114/how-to-read-a-6-gb-csv-file-with-pandas)

I am trying to read a large csv file (aprox. 6 GB) in pandas and i am getting the following memory error:Any help on this?? 

2014-09-21 17:46:43Z

I am trying to read a large csv file (aprox. 6 GB) in pandas and i am getting the following memory error:Any help on this?? The error shows that the machine does not have enough memory to read the entire

CSV into a DataFrame at one time. Assuming you do not need the entire dataset in

memory all at one time, one way to avoid the problem would be to process the CSV in

chunks (by specifying the chunksize parameter):The chunksize parameter specifies the number of rows per chunk.

(The last chunk may contain fewer than chunksize rows, of course.)Chunking shouldn't always be the first port of call for this problem.I proceeded like this:For large data l recommend you use the library "dask"  e.g: You can read more from the documentation here.The above answer is already satisfying the topic. Anyway, if you need all the data in memory - have a look at bcolz. Its compressing the data in memory. I have had really good experience with it. But its missing a lot of pandas featuresEdit: I got compression rates at around 1/10 or orig size i think, of course depending of the kind of data. Important features missing were aggregates. You can read in the data as chunks and save each chunk as pickle. In the next step you read in the pickles and append each pickle to your desired dataframe.The function read_csv and read_table is almost the same. But you must assign the delimiter「，」when you use the function read_table in your program.Solution 1: Using pandas with large dataSolution 2:You can try sframe, that have the same syntax as pandas but allows you to manipulate files that are bigger than your RAM.If you use pandas read large file into chunk and then yield row by row, here is what I have doneHere follows an example:In addition to the answers above, for those who want to process CSV and then export to csv, parquet or SQL, d6tstack is another good option. You can load multiple files and it deals with data schema changes (added/removed columns). Chunked out of core support is already built in.In case someone is still looking for something like this, I found that this new library called modin can help. It uses distributed computing that can help with the read. Here's a nice article comparing its functionality with pandas. It essentially uses the same functions as pandas.I want to make a more comprehensive answer based off of the most of the potential solutions that are already provided. I also want to point out one more potential aid that may help reading process.Option 1: dtypes"dtypes" is a pretty powerful parameter that you can use to reduce the memory pressure of read methods. See this and this answer. Pandas, on default, try to infer dtypes of the data.Referring to data structures, every data stored, a memory allocation takes place. At a basic level refer to the values below (The table below illustrates values for C programming language):Refer to this page to see the matching between NumPy and C types.Let's say you have an array of integers of digits. You can both theoretically and practically assign, say array of 16-bit integer type, but you would then allocate more memory than you actually need to store that array. To prevent this, you can set dtype option on read_csv. You do not want to store the array items as long integer where actually you can fit them with 8-bit integer (np.int8 or np.uint8).Observe the following dtype map.

Source: https://pbpython.com/pandas_dtypes.htmlYou can pass dtype parameter as a parameter on pandas methods as dict on read like {column: type}.Option 2: Read by ChunksReading the data in chunks allows you to access a part of the data in-memory, and you can apply preprocessing on your data and preserve the processed data rather than raw data. It'd be much better if you combine this option with the first one, dtypes.I want to point out the pandas cookbook sections for that process, where you can find it here. Note those two sections there;Option 3: DaskDask is a framework that is defined in Dask's website as:It was born to cover the necessary parts where pandas cannot reach. Dask is a powerful framework that allows you much more data access by processing it in a distributed way.You can use dask to preprocess your data as a whole, Dask takes care of the chunking part, so unlike pandas you can just define your processing steps and let Dask do the work. Dask does not apply the computations before it is explicitly pushed by compute and/or persist (see the answer here for the difference).Other Aids (Ideas)

Why does this iterative list-growing code give IndexError: list assignment index out of range?

Vladan

[Why does this iterative list-growing code give IndexError: list assignment index out of range?](https://stackoverflow.com/questions/5653533/why-does-this-iterative-list-growing-code-give-indexerror-list-assignment-index)

Please consider the following code:The output (Python 2.6.6 on Win 7 32-bit) is:I guess it's something simple I don't understand. Can someone clear it up?

2011-04-13 18:01:35Z

Please consider the following code:The output (Python 2.6.6 on Win 7 32-bit) is:I guess it's something simple I don't understand. Can someone clear it up?j is an empty list, but you're attempting to write to element [0] in the first iteration, which doesn't exist yet.Try the following instead, to add a new element to the end of the list:Of course, you'd never do this in practice if all you wanted to do was to copy an existing list. You'd just do:Alternatively, if you wanted to use the Python list like an array in other languages, then you could pre-create a list with its elements set to a null value (None in the example below), and later, overwrite the values in specific positions:The thing to realise is that a list object will not allow you to assign a value to an index that doesn't exist.Your other option is to initialize j:Do j.append(l) instead of j[k] = l and avoid k at all.You could also use a list comprehension:or make a copy of it using the statement:Also avoid using lower-case "L's" because it is easy for them to be confused with 1'sI think the Python method insert is what you're looking for:Inserts element x at position i.

list.insert(i,x)You could use a dictionary (similar to an associative array) for jwill print :One more way: In this case j will be a numpy arrayIn python with array you can only append and add a element into the arrayMaybe you need extend()

How to check if a float value is a whole number

chopper draw lion4

[How to check if a float value is a whole number](https://stackoverflow.com/questions/21583758/how-to-check-if-a-float-value-is-a-whole-number)

I am trying to find the largest cube root that is a whole number, that is less than 12,000. I am not sure how to check if it is a whole number or not though! I could convert it to a string then use indexing to check the end values and see whether they are zero or not, that seems rather cumbersome though. Is there a simpler way?

2014-02-05 17:06:10Z

I am trying to find the largest cube root that is a whole number, that is less than 12,000. I am not sure how to check if it is a whole number or not though! I could convert it to a string then use indexing to check the end values and see whether they are zero or not, that seems rather cumbersome though. Is there a simpler way?To check if a float value is a whole number, use the float.is_integer() method:The method was added to the float type in Python 2.6.Take into account that in Python 2, 1/3 is 0 (floor division for integer operands!), and that floating point arithmetic can be imprecise (a float is an approximation using binary fractions, not a precise real number). But adjusting your loop a little this gives:which means that anything over 3 cubed, (including 10648) was missed out due to the aforementioned imprecision:You'd have to check for numbers close to the whole number instead, or not use float() to find your number. Like rounding down the cube root of 12000:If you are using Python 3.5 or newer, you can use the math.isclose() function to see if a floating point value is within a configurable margin:For older versions, the naive implementation of that function (skipping error checking and ignoring infinity and NaN) as mentioned in PEP485:We can use the modulo (%) operator. This tells us how many remainders we have when we divide x by y - expresses as x % y. Every whole number must divide by 1, so if there is a remainder, it must not be a whole number. This function will return a boolean, True or False, depending on whether n is a whole number.You could use this:You don't need to loop or to check anything. Just take a cube root of 12,000 and round it down: You can use a modulo operation for that.Wouldn't it be easier to test the cube roots? Start with 20 (20**3 = 8000) and go up to 30 (30**3 = 27000). Then you have to test fewer than 10 integers.How about The above answers work for many cases but they miss some. Consider the following:Using this as a benchmark, some of the other suggestions don't get the behavior we might want:Instead try:now we get:isclose comes with Python 3.5+, and for other Python's you can use this mostly equivalent definition (as mentioned in the corresponding PEP)Just a side info, is_integer is doing internally: All the answers are good but a sure fire method would be The function returns True if it's a whole number else False....I know I'm a bit late but here's one of the interesting methods which I made...Edit: as stated by the comment below, a cheaper equivalent test would be:Try using:It will give lot more precision than any other methods.You can use the round function to compute the value. Yes in python as many have pointed when we compute the value of a cube root, it will give you an output with a little bit of error. To check if the value is a whole number you can use the following function:But remember that int(n) is equivalent to math.floor and because of this if you find the int(41063625**(1.0/3.0)) you will get 344 instead of 345.So please be careful when using int withe cube roots.

Iterate over model instance field names and values in template

Wayne Koorts

[Iterate over model instance field names and values in template](https://stackoverflow.com/questions/2170228/iterate-over-model-instance-field-names-and-values-in-template)

I'm trying to create a basic template to display the selected instance's field values, along with their names.  Think of it as just a standard output of the values of that instance in table format, with the field name (verbose_name specifically if specified on the field) in the first column and the value of that field in the second column.For example, let's say we have the following model definition:I would want it to be output in the template like so (assume an instance with the given values):What I'm trying to achieve is being able to pass an instance of the model to a template and be able to iterate over it dynamically in the template, something like this:Is there a neat, "Django-approved" way to do this?  It seems like a very common task, and I will need to do it often for this particular project.

2010-01-31 01:48:11Z

I'm trying to create a basic template to display the selected instance's field values, along with their names.  Think of it as just a standard output of the values of that instance in table format, with the field name (verbose_name specifically if specified on the field) in the first column and the value of that field in the second column.For example, let's say we have the following model definition:I would want it to be output in the template like so (assume an instance with the given values):What I'm trying to achieve is being able to pass an instance of the model to a template and be able to iterate over it dynamically in the template, something like this:Is there a neat, "Django-approved" way to do this?  It seems like a very common task, and I will need to do it often for this particular project.model._meta.get_all_field_names() will give you all the model's field names, then you can use model._meta.get_field() to work your way to the verbose name, and getattr(model_instance, 'field_name') to get the value from the model.NOTE: model._meta.get_all_field_names() is deprecated in django 1.9. Instead use model._meta.get_fields() to get the model's fields and field.name to get each field name.You can use Django's to-python queryset serializer.Just put the following code in your view:And then in the template:Its great advantage is the fact that it handles relation fields.For the subset of fields try:Finally found a good solution to this on the dev mailing list:In the view add:in the template add:In light of Django 1.8's release (and the formalization of the Model _meta API, I figured I would update this with a more recent answer.Assuming the same model:In the below example, we will utilize the formalized method for retrieving all field instances of a model via Client._meta.get_fields():Actually, it has been brought to my attention that the above is slightly overboard for what was needed (I agree!). Simple is better than complex. I am leaving the above for reference. However, to display in the template, the best method would be to use a ModelForm and pass in an instance. You can iterate over the form (equivalent of iterating over each of the form's fields) and use the label attribute to retrieve the verbose_name of the model field, and use the value method to retrieve the value:Now, we render the fields in the template:Here's another approach using a model method. This version resolves picklist/choice fields, skips empty fields, and lets you exclude specific fields. Then in your template:Ok, I know this is a bit late, but since I stumbled upon this before finding the correct answer so might someone else.From the django docs:You can use the values() method of a queryset, which returns a dictionary. Further, this method accepts a list of fields to subset on. The values() method will not work with get(), so you must use filter() (refer to the QuerySet API).In view...In detail.html...For a collection of instances returned by filter:In detail.html...I used https://stackoverflow.com/a/3431104/2022534 but replaced Django's model_to_dict() with this to be able to handle ForeignKey:Please note that I have simplified it quite a bit by removing the parts of the original I didn't need. You might want to put those back.You can have a form do the work for you.Then in the template:There should really be a built-in way to do this.  I wrote this utility build_pretty_data_view that takes a model object and form instance (a form based on your model) and returns a SortedDict.Benefits to this solution include:To use this solution, first add this file/function somewhere, then import it into your views.py.utils.pySo now in your views.py you might do something like thisNow in your my-template.html template you can iterate over the data like so...Good Luck. Hope this helps someone!Below is mine, inspired by shacker's get_all_fields.

It gets a dict of one model instance, if encounter relation field, then asign the field value a dict recursively.This function is mainly used to dump a model instance to json data:Instead of editing every model I would recommend to write one template tag which will return all field of any model given.  

Every object has list of fields ._meta.fields.

Every field object has attribute name that will return it's name and method value_to_string() that supplied with your model object will return its value.

The rest is as simple as it's said in Django documentation.Here is my example how this templatetag might look like:Yeah it's not pretty, you'll have to make your own wrapper. Take a look at builtin databrowse app, which has all the functionality you need really.This may be considered a hack but I've done this before using modelform_factory to turn a model instance into a form.The Form class has a lot more information inside that's super easy to iterate over and it will serve the same purpose at the expense of slightly more overhead. If your set sizes are relatively small I think the performance impact would be negligible.The one advantage besides convenience of course is that you can easily turn the table into an editable datagrid at a later date.I've come up with the following method, which works for me because in every case the model will have a ModelForm associated with it.Here is an extract from the template I am using for this particular view:The nice thing about this method is that I can choose on a template-by-template basis the order in which I would like to display the field labels, using the tuple passed in to GetModelData and specifying the field names.  This also allows me to exclude certain fields (e.g. a User foreign key) as only the field names passed in via the tuple are built into the final dictionary.I'm not going to accept this as the answer because I'm sure someone can come up with something more "Djangonic" :-)Update: I'm choosing this as the final answer because it is the simplest out of those given that does what I need.  Thanks to everyone who contributed answers.Django 1.7 solution for me:There variables are exact to the question, but you should definitely be able to dissect this exampleThe key here is to pretty much use the .__dict__ of the model

views.py:template:in the template I used a filter to access the field in the dict

filters.py:I'm using this, https://github.com/miracle2k/django-tables.This approach shows how to use a class like django's ModelForm and a template tag like {{ form.as_table }}, but have all the table look like data output, not a form.The first step was to subclass django's TextInput widget:Then I subclassed django's ModelForm to swap out the default widgets for readonly versions:Those were the only widgets I needed. But it should not be difficult to extend this idea to other widgets. Just an edit of @wonderLet Django handle all the other fields other than the related fields. I feel that is more stableTake a look at django-etc application. It has model_field_verbose_name template tag to get field verbose name from templates: http://django-etc.rtfd.org/en/latest/models.html#model-field-template-tagsI just tested something like this in shell and seems to do it's job:Note that if you want str() representation for foreign objects you should define it in their str method. From that you have dict of values for object. Then you can render some kind of template or whatever. Django >= 2.0Add get_fields() to your models.py:Then call it as object.get_fields on your template.html:

How can I get dictionary key as variable directly in Python (not by searching from value)?

Rick

[How can I get dictionary key as variable directly in Python (not by searching from value)?](https://stackoverflow.com/questions/3545331/how-can-i-get-dictionary-key-as-variable-directly-in-python-not-by-searching-fr)

Sorry for this basic question but my searches on this are not turning up anything other than how to get a dictionary's key based on its value which I would prefer not to use as I simply want the text/name of the key and am worried that searching by value may end up returning 2 or more keys if the dictionary has a lot of entries... what I am trying to do is this:The reason for this is that I am printing these out to a document and I want to use the key name and the value in doing thisI have seen the method below but this seems to just return the key's value

2010-08-23 07:01:32Z

Sorry for this basic question but my searches on this are not turning up anything other than how to get a dictionary's key based on its value which I would prefer not to use as I simply want the text/name of the key and am worried that searching by value may end up returning 2 or more keys if the dictionary has a lot of entries... what I am trying to do is this:The reason for this is that I am printing these out to a document and I want to use the key name and the value in doing thisI have seen the method below but this seems to just return the key's valueYou should iterate over keys with:If you want to print key and value, use the following:Based on the above requirement this is what I would suggest:If the dictionary contains one pair like this:then you can get as For Python 3.5, do this:keys=[i for i in mydictionary.keys()] or

keys = list(mydictionary.keys())As simple as that:You will modify your dictionary and should make a copy of it firstYou could simply use * which unpacks the dictionary keys. Example:Iterate over dictionary (i) will return the key, then using it (i) to get the valueFor python 3

If you want to get only the keys use this. Replace print(key) with print(values) if you want the values.What I sometimes do is I create another dictionary just to be able whatever I feel I need to access as string. Then I iterate over multiple dictionaries matching keys to build e.g. a table with first column as description.You can easily do for a huge amount of dictionaries to match data (I like the fact that with dictionary you can always refer to something well known as the key name) yes I use dictionaries to store results of functions so I don't need to run these functions everytime I call them just only once and then access the results anytime. EDIT: in my example the key name does not really matter (I personally like using the same key names as it is easier to go pick a single value from any of my matching dictionaries), just make sure the number of keys in each dictionary is the sameYou can do this by casting the dict keys and values to list.  It can also be be done for items.Example:easily change the position of your keys and values,then use values to get key,

in dictionary keys can have same value but they(keys) should be different.

for instance if you have a list and the first value of it is a key for your problem and other values are the specs of the first value:you can  save and use the data easily in Dictionary by this loop:then you can find the key(name) base on any input value.

How to serialize SqlAlchemy result to JSON?

Zelid

[How to serialize SqlAlchemy result to JSON?](https://stackoverflow.com/questions/5022066/how-to-serialize-sqlalchemy-result-to-json)

Django has some good automatic serialization of ORM models returned from DB to JSON format.How to serialize SQLAlchemy query result to JSON format? I tried jsonpickle.encode but it encodes query object itself.

I tried json.dumps(items) but it returnsIs it really so hard to serialize SQLAlchemy ORM objects to JSON /XML? Isn't there any default serializer for it? It's very common task to serialize ORM query results nowadays.What I need is just to return JSON or XML data representation of SQLAlchemy query result.SQLAlchemy objects query result in JSON/XML format is needed to be used in javascript datagird (JQGrid http://www.trirand.com/blog/)

2011-02-16 21:04:05Z

Django has some good automatic serialization of ORM models returned from DB to JSON format.How to serialize SQLAlchemy query result to JSON format? I tried jsonpickle.encode but it encodes query object itself.

I tried json.dumps(items) but it returnsIs it really so hard to serialize SQLAlchemy ORM objects to JSON /XML? Isn't there any default serializer for it? It's very common task to serialize ORM query results nowadays.What I need is just to return JSON or XML data representation of SQLAlchemy query result.SQLAlchemy objects query result in JSON/XML format is needed to be used in javascript datagird (JQGrid http://www.trirand.com/blog/)You could use something like this:and then convert to JSON using:It will ignore fields that are not encodable (set them to 'None').It doesn't auto-expand relations (since this could lead to self-references, and loop forever).If, however, you'd rather loop forever, you could use:And then encode objects using:This would encode all children, and all their children, and all their children... Potentially encode your entire database, basically. When it reaches something its encoded before, it will encode it as 'None'.Another alternative, probably better, is to be able to specify the fields you want to expand:You can now call it with:To only expand SQLAlchemy fields called 'parents', for example.You could just output your object as a dictionary:And then you use User.as_dict() to serialize your object.As explained in Convert sqlalchemy row object to python dictYou can convert a RowProxy to a dict like this:Then serialize that to JSON ( you will have to specify an encoder for things like datetime values )

It's not that hard if you just want one record ( and not a full hierarchy of related records ).I recommend using marshmallow. It allows you to create serializers to represent your model instances with support to relations and nested objects.Here is a truncated example from their docs. Take the ORM model, Author:A marshmallow schema for that class is constructed like this:...and used like this:...would produce an output like this:Have a look at their full Flask-SQLAlchemy Example.A library called marshmallow-sqlalchemy specifically integrates SQLAlchemy and marshmallow. In that library, the schema for the Author model described above looks like this:The integration allows the field types to be inferred from the SQLAlchemy Column types.marshmallow-sqlalchemy here.The /users/ route will now return a list of users.The response from jsonify(account) would be this.For security reasons you should never return all the model's fields. I prefer to selectively choose them.Flask's json encoding now supports UUID, datetime and relationships (and added query and query_class for flask_sqlalchemy db.Model class). I've updated the encoder as follows:With this I can optionally add a __json__ property that returns the list of fields I wish to encode:I add @jsonapi to my view, return the resultlist and then my output is as follows:Flask-JsonTools package has an implementation of JsonSerializableBase Base class for your models.Usage:Now the User model is magically serializable.If your framework is not Flask, you can just grab the codeYou can use introspection of SqlAlchemy as this : Get inspired from an answer here : 

    Convert sqlalchemy row object to python dictA more detailed explanation. 

In your model, add: The str() is for python 3 so if using python 2 use unicode(). It should help deserialize dates. You can remove it if not dealing with those.You can now query the database like thisFirst() is needed to avoid weird errors. as_dict() will now deserialize the result. After deserialization, it is ready to be turned to jsonIt is not so straighforward. I wrote some code to do this. I'm still working on it, and it uses the MochiKit framework. It basically translates compound objects between Python and Javascript using a proxy and registered JSON converters. Browser side for database objects is db.js

It needs the basic Python proxy source in proxy.js.On the Python side there is the base proxy module.

Then finally the SqlAlchemy object encoder in webserver.py.

It also depends on metadata extractors found in the models.py file.While the original question goes back awhile, the number of answers here (and my own experiences) suggest it's a non-trivial question with a lot of different approaches of varying complexity with different trade-offs.That's why I built the SQLAthanor library that extends SQLAlchemy's declarative ORM with configurable serialization/de-serialization support that you might want to take a look at.The library supports:You can check out the (I hope!) comprehensive docs here: https://sqlathanor.readthedocs.io/en/latestHope this helps!Custom serialization and deserialization."from_json" (class method) builds a Model object based on json data."deserialize" could be called only on instance, and merge all data from json into Model instance."serialize" - recursive serialization__write_only__ property is needed to define write only properties ("password_hash" for example).Here is a solution that lets you select the relations you want to include in your output as deep as you would like to go.

NOTE: This is a complete re-write taking a dict/str as an arg rather than a list. fixes some stuff..so for an example using person/family/homes/rooms... turning it into json all you need isI thought I'd play a little code golf with this one. FYI: I am using automap_base since we have a separately designed schema according to business requirements. I just started using SQLAlchemy today but the documentation states that automap_base is an extension to declarative_base which seems to be the typical paradigm in the SQLAlchemy ORM so I believe this should work.It does not get fancy with following foreign keys per Tjorriemorrie's solution, but it simply matches columns to values and handles Python types by str()-ing the column values. Our values consist Python datetime.time and decimal.Decimal class type results so it gets the job done.Hope this helps any passers-by!Use the built-in serializer in SQLAlchemy:If you're transferring the object between sessions, remember to detach the object from the current session using session.expunge(obj).

To attach it again, just do session.add(obj).following code will serialize sqlalchemy result to json.Calling fun,The AlchemyEncoder is wonderful but sometimes fails with Decimal values. Here is an improved encoder that solves the decimal problem - When using sqlalchemy to connect to a db I this is a simple solution which is highly configurable. Use pandas.I know this is quite an older post. I took solution given by @SashaB and modified as per my need. I added following things to it:My code is as follows:Hope it helps someone!Under Flask, this works and handles datatime fields, transforming a field of type

'time': datetime.datetime(2018, 3, 22, 15, 40) into

"time": "2018-03-22 15:40:00":The built in serializer chokes with utf-8 cannot decode invalid start byte for some inputs.  Instead, I went with:Maybe you can use a class like thisWith that all objects have the to_dict method While using some raw sql and undefined objects, using cursor.description appeared to get what I was looking for:My take utilizing (too many?) dictionaries:Running with flask (including jsonify) and flask_sqlalchemy to print outputs as JSON.Call the function with jsonify(serialize()).Works with all SQLAlchemy queries I've tried so far (running SQLite3)

Is it possible to implement a Python for range loop without an iterator variable?

Alex Martelli

[Is it possible to implement a Python for range loop without an iterator variable?](https://stackoverflow.com/questions/818828/is-it-possible-to-implement-a-python-for-range-loop-without-an-iterator-variable)

Is it possible to do following without the i?If you just want to do something N amount of times and don't need the iterator.

2009-05-04 05:39:39Z

Is it possible to do following without the i?If you just want to do something N amount of times and don't need the iterator.Off the top of my head, no.I think the best you could do is something like this:But I think you can just live with the extra i variable.Here is the option to use the _ variable, which in reality, is just another variable.Note that _ is assigned the last result that returned in an interactive python session:For this reason, I would not use it in this manner. I am unaware of any idiom as mentioned by Ryan. It can mess up your interpreter.And according to Python grammar, it is an acceptable variable name:You may be looking forthis is THE fastest way to iterate times times in Python.The general idiom for assigning to a value that isn't used is to name it _.What everyone suggesting you to use _ isn't saying is that _ is frequently used as a shortcut to one of the gettext functions, so if you want your software to be available in more than one language then you're best off avoiding using it for other purposes.Here's a random idea that utilizes (abuses?) the data model (Py3 link).I wonder if there is something like this in the standard libraries?You can use _11 (or any number or another invalid identifier) to prevent name-colision with gettext. Any time you use underscore + invalid identifier you get a dummy name that can be used in for loop.May be answer would depend on what problem you have with using iterator?

may be useor but frankly i see no point in using such approachesOUTPUT:I generally agree with solutions given above. Namely with: If one is to define an object as in #3 I would recommend implementing protocol for with keyword or apply contextlib.Further I propose yet another solution. It is a 3 liner and is not of supreme elegance, but it uses itertools package and thus might be of an interest.In these example 2 is the number of times to iterate the loop. chain is wrapping two repeat iterators, the first being limited but the second is infinite. Remember that these are true iterator objects, hence they do not require infinite memory. Obviously this is much slower then solution #1. Unless written as a part of a function it might require a clean up for times variable.We have had some fun with the following, interesting to share so: Results:  If do_something is a simple function or can be wrapped in one, a simple map() can do_something range(some_number) times:If you want to pass arguments to do_something, you may also find the itertools repeatfunc recipe reads well:To pass the same arguments:To pass different arguments:Instead of an unneeded counter, now you have an unneeded list.

Best solution is to use a variable that starts with "_", that tells syntax checkers that you are aware you are not using the variable.If you really want to avoid putting something with a name (either an iteration variable as in the OP, or unwanted list or unwanted generator returning true the wanted amount of time) you could do it if you really wanted:The trick that's used is to create an anonymous class type('', (), {}) which results in a class with empty name, but NB that it is not inserted in the local or global namespace (even if a nonempty name was supplied). Then you use a member of that class as iteration variable which is unreachable since the class it's a member of is unreachable.Taken from http://docs.python.org/2/library/itertools.htmlWhat about:

Is a Python dictionary an example of a hash table?

Tommy Herbert

[Is a Python dictionary an example of a hash table?](https://stackoverflow.com/questions/114830/is-a-python-dictionary-an-example-of-a-hash-table)

One of the basic data structures in Python is the dictionary, which allows one to record "keys" for looking up "values" of any type.  Is this implemented internally as a hash table?  If not, what is it?

2008-09-22 13:22:28Z

One of the basic data structures in Python is the dictionary, which allows one to record "keys" for looking up "values" of any type.  Is this implemented internally as a hash table?  If not, what is it?Yes, it is a hash mapping or hash table. You can read a description of python's dict implementation, as written by Tim Peters, here.That's why you can't use something 'not hashable' as a dict key, like a list:You can read more about hash tables or check how it has been implemented in python and why it is implemented that way.There must be more to a Python dictionary than a table lookup on hash(). By brute experimentation I found this hash collision:Yet it doesn't break the dictionary:Sanity check:Possibly there's another lookup level beyond hash() that avoids collisions between dictionary keys. Or maybe dict() uses a different hash.(By the way, this in Python 2.7.10. Same story in Python 3.4.3 and 3.5.0 with a collision at hash(1.1) == hash(214748749.8).)Yes.  Internally it is implemented as open hashing based on a primitive polynomial over Z/2 (source).To expand upon nosklo's explanation:

How to get exit code when using Python subprocess communicate method?

CarpeNoctem

[How to get exit code when using Python subprocess communicate method?](https://stackoverflow.com/questions/5631624/how-to-get-exit-code-when-using-python-subprocess-communicate-method)

How do I retrieve the exit code when using Python's subprocess module and the communicate() method?Relevant code:Should I be doing this another way?

2011-04-12 07:11:02Z

How do I retrieve the exit code when using Python's subprocess module and the communicate() method?Relevant code:Should I be doing this another way?Popen.communicate will set the returncode attribute when it's done(*). Here's the relevant documentation section:So you can just do (I didn't test it but it should work):(*) This happens because of the way it's implemented: after setting up threads to read the child's streams, it just calls wait. You should first make sure that the process has completed running and the return code has been read out using the .wait method. This will return the code. If you want access to it later, it's stored as .returncode in the Popen object. exitcode = data.wait(). The child process will be blocked If it writes to standard output/error, and/or reads from standard input, and there are no peers..poll() will update the return code.TryIn addition, after .poll() is called the return code is available in the object as child.returncode.This worked for me. It also prints the output returned by the child process

Is Python interpreted, or compiled, or both?

Pankaj Upadhyay

[Is Python interpreted, or compiled, or both?](https://stackoverflow.com/questions/6889747/is-python-interpreted-or-compiled-or-both)

From my understanding:An interpreted language is a high-level language run and executed by an interpreter (a program which converts the high-level language to machine code and then executing) on the go; it processes the program a little at a time.A compiled language is a high-level language whose code is first converted to machine-code by a compiler (a program which converts the high-level language to machine code) and then executed by an executor (another program for running the code).Correct me if my definitions are wrong.Now coming back to Python, I am bit confused about this. Everywhere you learn that Python is an interpreted language, but it's interpreted to some intermediate code (like byte-code or IL) and not to the machine code. So which program then executes the IM code? Please help me understand how a Python script is handled and run.  

2011-07-31 13:31:51Z

From my understanding:An interpreted language is a high-level language run and executed by an interpreter (a program which converts the high-level language to machine code and then executing) on the go; it processes the program a little at a time.A compiled language is a high-level language whose code is first converted to machine-code by a compiler (a program which converts the high-level language to machine code) and then executed by an executor (another program for running the code).Correct me if my definitions are wrong.Now coming back to Python, I am bit confused about this. Everywhere you learn that Python is an interpreted language, but it's interpreted to some intermediate code (like byte-code or IL) and not to the machine code. So which program then executes the IM code? Please help me understand how a Python script is handled and run.  First off, interpreted/compiled is not a property of the language but a property of the implementation. For most languages, most if not all implementations fall in one category, so one might save a few words saying the language is interpreted/compiled too, but it's still an important distinction, both because it aids understanding and because there are quite a few languages with usable implementations of both kinds (mostly in the realm of functional languages, see Haskell and ML). In addition, there are C interpreters and projects that attempt to compile a subset of Python to C or C++ code (and subsequently to machine code).Second, compilation is not restricted to ahead-of-time compilation to native machine code. A compiler is, more generally, a program that converts a program in one programming language into a program in another programming language (arguably, you can even have a compiler with the same input and output language if significant transformations are applied). And JIT compilers compile to native machine code at runtime, which can give speed very close to or even better than ahead of time compilation (depending on the benchmark and the quality of the implementations compared).But to stop nitpicking and answer the question you meant to ask: Practically (read: using a somewhat popular and mature implementation), Python is compiled. Not compiled to machine code ahead of time (i.e. "compiled" by the restricted and wrong, but alas common definition), "only" compiled to bytecode, but it's still compilation with at least some of the benefits. For example, the statement a = b.c() is compiled to a byte stream which, when "disassembled", looks somewhat like load 0 (b); load_str 'c'; get_attr; call_function 0; store 1 (a). This is a simplification, it's actually less readable and a bit more low-level -  you can experiment with the standard library dis module and see what the real deal looks like. Interpreting this is faster than interpreting from a higher-level representation.That bytecode is either interpreted (note that there's a difference, both in theory and in practical performance, between interpreting directly and first compiling to some intermediate representation and interpret that), as with the reference implementation (CPython), or both interpreted and compiled to optimized machine code at runtime, as with PyPy.The CPU can only understand machine code indeed. For interpreted programs, the ultimate goal of an interpreter is to "interpret" the program code into machine code. However, usually a modern interpreted language does not interpret human code directly because it is too inefficient.The Python interpreter first reads the human code and optimizes it to some intermediate code before interpreting it into machine code. That's why you always need another program to run a Python script, unlike in C++ where you can run the compiled executable of your code directly. For example, c:\Python27\python.exe or /usr/bin/python.The answer depends on what implementation of python is being used. If you are using lets say CPython (The Standard implementation of python) or Jython (Targeted for integration with java programming language)it is first translated into bytecode, and depending on the implementation of python you are using,  this bycode is directed to the corresponding virtual machine for interpretation. PVM (Python Virtual Machine) for CPython and JVM (Java Virtual Machine) for Jython.But lets say you are using PyPy which is another standard CPython implementation. It would use a Just-In-Time Compiler. According to python.org it is an interpreter.https://www.python.org/doc/essays/blurb/...... ...Yes, it is both compiled and interpreted language. Then why we generally call it as interpreted languate?see how it is both- compiled and interpreted?First of all I want to tell that you will like my answer more if you are from the Java world.In the Java the source code first gets converted to the byte code through javac compiler then directed to the JVM(responsible for generating the native code for execution purpose). Now I want to show you that we call the Java as compiled language because we can see that it really compiles the source code and gives the .class file(nothing but bytecode) through:javac Hello.java  -------> produces Hello.class filejava Hello -------->Directing bytecode to JVM for execution purposeThe same thing happens with python i.e. first the source code gets converted to the bytecode through the compiler then directed to the PVM(responsible for generating the native code for execution purpose). Now I want to show you that we usually call the Python as an interpreted language because the compilation happens behind the scene

and when we run the python code through: python Hello.py -------> directly excutes the code and we can see the output provied that code is syntactically correct@ python Hello.py it looks like it directly executes but really it first generates the bytecode that is interpreted by the interpreter to produce the native code for the execution purpose.CPython- Takes the responsibility of both compilation and interpretation.Look into the below lines if you need more detail:As I mentioned that CPython compiles the source code but actual compilation happens with the help of cython then interpretation happens with the help of CPythonNow let's talk a little bit about the role of Just-In-Time compiler in Java and PythonIn JVM the Java Interpreter exists which interprets the bytecode line by line to get the native machine code for execution purpose but when Java bytecode is executed by an interpreter, the execution will always be slower. So what is the solution? the solution is Just-In-Time compiler which produces the native code which can be executed much more quickly than that could be interpreted. Some JVM vendors use Java Interpreter and some use Just-In-Time compiler. Reference: click hereIn python to get around the interpreter to achieve the fast execution use another python implementation(PyPy) instead of CPython.

click here for other implementation of python including PyPy.If ( You know Java ) { 

Python code is converted into bytecode like java does.

That bytecode is executed again everytime you try to access it.

} else { 

Python code is initially traslated into something called bytecode that is quite 

close to machine language but not actual machine code

so each time we access or run it that bytecode is executed again

}Almost, we can say Python is interpreted language. But we are using some part of one time compilation process in python to convert complete source code into byte-code like java language.For newbiesPython automatically compiles your script to compiled code, so called byte code, before running it.Running a script is not considered an import and no .pyc will be created.For example, if you have a script file abc.py that imports another module xyz.py, when you run abc.py, xyz.pyc will be created since xyz is imported, but no abc.pyc file will be created since abc.py isn’t being imported.Python(the interpreter) is compiled.       Proof:  It won't even compile your code if it contains syntax error. Example 1:         Output:Example 2:       Output:   The python code you write is compiled into python bytecode, which creates file with extension .pyc. If compiles, again question is, why not compiled language. Note that this isn't compilation in the traditional sense of the word. Typically, we’d say that compilation is taking a high-level language and converting it to machine code. But it is a compilation of sorts. Compiled in to intermediate code not into machine code (Hope you got it Now).Back to the execution process, your bytecode, present in pyc file, created in compilation step, is then executed by appropriate virtual machines, in our case, the CPython VM 

The time-stamp (called as magic number) is used to validate whether .py file is changed or not, depending on that new pyc file is created. If pyc is of current code then it simply skips compilation step.

How to get http headers in flask?

emil

[How to get http headers in flask?](https://stackoverflow.com/questions/29386995/how-to-get-http-headers-in-flask)

I am newbie to python and using Python Flask and generating REST API service.I want to check authorization header which is sent the client.But I can't find way to get HTTP header in flask.Any help for getting HTTP header authorization is appreciated.

2015-04-01 09:16:56Z

I am newbie to python and using Python Flask and generating REST API service.I want to check authorization header which is sent the client.But I can't find way to get HTTP header in flask.Any help for getting HTTP header authorization is appreciated.request.headers behaves like a dictionary, so you can also get your header like you would with any dictionary:just note, The different between the methods are, if the header is not existwill return None or no exception, so you can use it likebut the following will throw an errorYou can handle it byIf any one's trying to fetch all headers that were passed then just simply use:it gives you all the headers in a dict from which you can actually do whatever ops you want to. In my use case I had to forward all headers to another API since the python API was a proxy

How to implement an ordered, default dict? [duplicate]

jlconlin

[How to implement an ordered, default dict? [duplicate]](https://stackoverflow.com/questions/6190331/how-to-implement-an-ordered-default-dict)

I would like to combine OrderedDict() and defaultdict() from collections in one object, which shall be an ordered, default dict.

Is this possible?

2011-05-31 16:02:12Z

I would like to combine OrderedDict() and defaultdict() from collections in one object, which shall be an ordered, default dict.

Is this possible?The following (using a modified version of this recipe) works for me:Here is another possibility, inspired by Raymond Hettinger's super() Considered Super, tested on Python 2.7.X and 3.4.X:If you check out the class's MRO (aka, help(OrderedDefaultDict)), you'll see this:meaning that when an instance of OrderedDefaultDict is initialized, it defers to the OrderedDict's init, but this one in turn will call the defaultdict's methods before calling __builtin__.dict, which is precisely what we want.Here's another solution to think about if your use case is simple like mine and you don't necessarily want to add the complexity of a DefaultOrderedDict class implementation to your code.(None is my desired default value.)Note that this solution won't work if one of your requirements is to dynamically insert new keys with the default value.  A tradeoff of simplicity. Update 3/13/17 - I learned of a convenience function for this use case.  Same as above but you can omit the line items = ... and just:Output:And if your keys are single characters, you can just pass one string:This has the same output as the two examples above.You can also pass a default value as the second arg to OrderedDict.fromkeys(...).If you want a simple solution that doesn't require a class, you can just use OrderedDict.setdefault(key, default=None) or OrderedDict.get(key, default=None). If you only get / set from a few places, say in a loop, you can easily just setdefault.It is even easier for lists with setdefault:But if you use it more than a few times, it is probably better to set up a class, like in the other answers.A simpler version of @zeekay 's answer is:A simple and elegant solution building on @NickBread. 

Has a slightly different API to set the factory, but good defaults are always nice to have.Another simple approach would be to use dictionary get methodInspired by other answers on this thread, you can use something like,I would like to know if there're any downsides of initializing another object of the same class in the missing method. i tested the default dict and discovered it's also sorted!

maybe it was just a coincidence but anyway you can use the sorted function:i think it's simpler

Python if not == vs if !=

lafferc

[Python if not == vs if !=](https://stackoverflow.com/questions/31026754/python-if-not-vs-if)

What is the difference between these two lines of code:and Is one more efficient than the other?Would it be better to use

2015-06-24 12:35:39Z

What is the difference between these two lines of code:and Is one more efficient than the other?Would it be better to useUsing dis to look at the bytecode generated for the two versions:not ==!=The latter has fewer operations, and is therefore likely to be slightly more efficient. It was pointed out in the commments (thanks, @Quincunx) that where you have if foo != bar vs. if not foo == bar the number of operations is exactly the same, it's just that the COMPARE_OP changes and POP_JUMP_IF_TRUE switches to POP_JUMP_IF_FALSE:not ==:!=In this case, unless there was a difference in the amount of work required for each comparison, it's unlikely you'd see any performance difference at all.However, note that the two versions won't always be logically identical, as it will depend on the implementations of __eq__ and __ne__ for the objects in question. Per the data model documentation:For example:Finally, and perhaps most importantly: in general, where the two are logically identical, x != y is much more readable than not x == y.@jonrsharpe has an excellent explanation of what's going on. I thought I'd just show the difference in time when running each of the 3 options 10,000,000 times (enough for a slight difference to show).Code used:And the cProfile profiler results:So we can see that there is a very minute difference of ~0.7% between if not x == 'val': and if x != 'val':. Of these, if x != 'val': is the fastest.

However, most surprisingly, we can see that is in fact the fastest, and beats if x != 'val': by ~0.3%. This isn't very readable, but I guess if you wanted a negligible performance improvement, one could go down this route.In the first one Python has to execute one more operations than necessary(instead of just checking not equal to it has to check if it is not true that it is equal, thus one more operation). It would be impossible to tell the difference from one execution, but if run many times, the second would be more efficient. Overall I would use the second one, but mathematically they are the sameHere you can see that not x == y has one more instruction than x != y. So the performance difference will be very small in most cases unless you are doing millions of comparisons and even then this will likely not be the cause of a bottleneck. An additional note, since the other answers answered your question mostly correctly, is that if a class only defines __eq__() and not __ne__(), then your COMPARE_OP (!=) will run __eq__() and negate it. At that time, your third option is likely to be a tiny bit more efficient, but should only be considered if you NEED the speed, since it's difficult to understand quickly.It's about your way of reading it. not operator is dynamic, that's why you are able to apply it in But != could be read in a better context as an operator which does the opposite of what == does.I want to expand on my readability comment above.Again, I completely agree with readability overriding other (performance-insignificant) concerns.What I would like to point out is the brain interprets "positive" faster than it does "negative".  E.g., "stop" vs. "do not go" (a rather lousy example due to the difference in number of words).So given a choice:is preferable to the functionally-equivalent:Less readability/understandability leads to more bugs.  Perhaps not in initial coding, but the (not as smart as you!) maintenance changes...

What does「hashable」mean in Python?

user1755071

[What does「hashable」mean in Python?](https://stackoverflow.com/questions/14535730/what-does-hashable-mean-in-python)

I tried searching internet but could not find the meaning of hashable.When they say objects are hashable or hashable objects what does it mean?

2013-01-26 09:48:07Z

I tried searching internet but could not find the meaning of hashable.When they say objects are hashable or hashable objects what does it mean?From the Python glossary:All the answers here have good working explanation of hashable objects in python, but I believe one needs to understand the term Hashing first.Hashing is a concept in computer science which is used to create high performance, pseudo random access data structures where large amount of data is to be stored and accessed quickly.For example, if you have 10,000 phone numbers, and you want to store them in an array (which is a sequential data structure that stores data in contiguous memory locations, and provides random access), but you might not have the required amount of contiguous memory locations.So, you can instead use an array of size 100, and use a hash function to map a set of values to same indices, and these values can be stored in a linked list. This provides a performance similar to an array. Now, a hash function can be as simple as dividing the number with the size  of the array and taking the remainder as the index.For more detail refer to https://en.wikipedia.org/wiki/Hash_functionHere is another good reference: http://interactivepython.org/runestone/static/pythonds/SortSearch/Hashing.htmlAnything that is not mutable (mutable means, likely to change) can be hashed. Besides the hash function to look for, if a class has it, by eg. dir(tuple) and looking for the __hash__ method, here are some examplesList of immutable types:List of mutable types:In my understanding according to Python glossary, when you create a instance of objects that are hashable, an unchangeable value is also calculated according to the members or values of the instance. 

For example, that value could then be used as a key in a dict as below:we can find that the hash value of tuple_a and tuple_c are the same since they have the same members. 

When we use tuple_a as the key in dict_a, we can find that the value for dict_a[tuple_c] is the same, which means that, when they are used as the key in a dict, they return the same value because the hash values are the same.

For those objects that are not hashable, the method hash is defined as None:I guess this hash value is calculated upon the initialization of the instance, not in a dynamic way, that's why only immutable objects are hashable. Hope this helps.Let me give you a working example to understand the hashable objects in python. I am taking 2 Tuples for this example.Each value in a tuple has a unique Hash Value which never changes during its lifetime. So based on this has value, the comparison between two tuples is done. We can get the hash value of a tuple element using the Id().In python it means that the object can be members of sets in order to return a index. That is, they have unique identity/ id.for example, in python 3.3:the data structure Lists are not hashable but the data structure Tuples are hashable. For creating a hashing table from scratch, all the values has to set to "None" and modified once a requirement arises.

Hashable objects refers to the modifiable datatypes(Dictionary,lists etc). Sets on the other hand cannot be reinitialized once assigned, so sets are non hashable. Whereas, The variant of set() -- frozenset() -- is hashable.

Django CSRF check failing with an Ajax POST request

firebush

[Django CSRF check failing with an Ajax POST request](https://stackoverflow.com/questions/5100539/django-csrf-check-failing-with-an-ajax-post-request)

I could use some help complying with Django's CSRF protection mechanism via my AJAX post. I've followed the directions here:http://docs.djangoproject.com/en/dev/ref/contrib/csrf/I've copied the AJAX sample code they have on that page exactly:http://docs.djangoproject.com/en/dev/ref/contrib/csrf/#ajaxI put an alert printing the contents of getCookie('csrftoken') before the xhr.setRequestHeader call and it is indeed populated with some data. I'm not sure how to verify that the token is correct, but I'm encouraged that it's finding and sending something.But Django is still rejecting my AJAX post.Here's my JavaScript:Here's the error I'm seeing from Django:I'm sure I'm missing something, and maybe it's simple, but I don't know what it is.  I've searched around SO and saw some information about turning off the CSRF check for my view via the csrf_exempt decorator, but I find that unappealing.  I've tried that out and it works, but I'd rather get my POST to work the way Django was designed to expect it, if possible.Just in case it's helpful, here's the gist of what my view is doing:Thanks for your replies!

2011-02-24 04:58:11Z

I could use some help complying with Django's CSRF protection mechanism via my AJAX post. I've followed the directions here:http://docs.djangoproject.com/en/dev/ref/contrib/csrf/I've copied the AJAX sample code they have on that page exactly:http://docs.djangoproject.com/en/dev/ref/contrib/csrf/#ajaxI put an alert printing the contents of getCookie('csrftoken') before the xhr.setRequestHeader call and it is indeed populated with some data. I'm not sure how to verify that the token is correct, but I'm encouraged that it's finding and sending something.But Django is still rejecting my AJAX post.Here's my JavaScript:Here's the error I'm seeing from Django:I'm sure I'm missing something, and maybe it's simple, but I don't know what it is.  I've searched around SO and saw some information about turning off the CSRF check for my view via the csrf_exempt decorator, but I find that unappealing.  I've tried that out and it works, but I'd rather get my POST to work the way Django was designed to expect it, if possible.Just in case it's helpful, here's the gist of what my view is doing:Thanks for your replies!Real solutionOk, I managed to trace the problem down. It lies in the Javascript (as I suggested below) code.What you need is this:instead of the code posted in the official docs:

https://docs.djangoproject.com/en/2.2/ref/csrf/The working code, comes from this Django entry: http://www.djangoproject.com/weblog/2011/feb/08/security/So the general solution is: "use ajaxSetup handler instead of ajaxSend handler". I don't know why it works. But it works for me :)Previous post (without answer)I'm experiencing the same problem actually.It occurs after updating to Django 1.2.5 - there were no errors with AJAX POST requests in Django 1.2.4 (AJAX wasn't protected in any way, but it worked just fine).Just like OP, I have tried the JavaScript snippet posted in Django documentation. I'm using jQuery 1.5. I'm also using the "django.middleware.csrf.CsrfViewMiddleware" middleware.I tried to follow the the middleware code and I know that it fails on this:and thenthis "if" is true, because "request_csrf_token" is empty.Basically it means that the header is NOT set. So is there anything wrong with this JS line:?I hope that provided details will help us in resolving the issue :)If you use the $.ajax function, you can simply add the csrf token in the data body:Add this line to your jQuery code:and done.The issue is because django is expecting the value from the cookie to be passed back as part of the form data. The code from the previous answer is getting javascript to hunt out the cookie value and put it into the form data. Thats a lovely way of doing it from a technical point of view, but it does look a bit verbose.In the past, I have done it more simply by getting the javascript to put the token value into the post data.If you use {% csrf_token %} in your template, you will get a hidden form field emitted that carries the value. But, if you use {{ csrf_token }} you will just get the bare value of the token, so you can use this in javascript like this....Then you can include that, with the required key name in the hash you then submit as the data to the ajax call.The {% csrf_token %} put in html templates inside <form></form> translates to something like:so why not just grep it in your JS like this:and then pass it e.g doing some POST, like:Non-jquery answer:usage:If your form posts correctly in Django without JS, you should be able to progressively enhance it with ajax without any hacking or messy passing of the csrf token. Just serialize the whole form and that will automatically pick up all your form fields including the hidden csrf field:I've tested this with Django 1.3+ and jQuery 1.5+. Obviously this will work for any HTML form, not just Django apps.Use Firefox with Firebug. Open the 'Console' tab while firing the ajax request. With DEBUG=True you get the nice django error page as response and you can even see the rendered html of the ajax response in the console tab.Then you will know what the error is.The accepted answer is most likely a red herring. The difference between Django 1.2.4 and 1.2.5 was the requirement for a CSRF token for AJAX requests.  I came across this problem on Django 1.3 and it was caused by the CSRF cookie not being set in the first place. Django will not set the cookie unless it has to. So an exclusively or heavily ajax site running on Django 1.2.4 would potentially never have sent a token to the client and then the upgrade requiring the token would cause the 403 errors.   The ideal fix is here:

http://docs.djangoproject.com/en/dev/ref/contrib/csrf/#page-uses-ajax-without-any-html-form

but you'd have to wait for 1.4 unless this is just documentation catching up with the codeEditNote also that the later Django docs note a bug in jQuery 1.5 so ensure you are using 1.5.1 or later with the Django suggested code:

http://docs.djangoproject.com/en/1.3/ref/contrib/csrf/#ajaxIt seems nobody has mentioned how to do this in pure JS using the X-CSRFToken header and {{ csrf_token }}, so here's a simple solution where you don't need to search through the cookies or the DOM:As it is not stated anywhere in the current answers, the fastest solution if you are not embedding js into your template is: Put <script type="text/javascript"> window.CSRF_TOKEN = "{{ csrf_token }}"; </script> before your reference to script.js file in your template, then add csrfmiddlewaretoken into your data dictionary in your js file:I've just encountered a bit different but similar situation. Not 100% sure if it'd be a resolution to your case, but I resolved the issue for Django 1.3 by setting a POST parameter 'csrfmiddlewaretoken' with the proper cookie value string which is usually returned within the form of your home HTML by Django's template system with '{% csrf_token %}' tag. I did not try on the older Django, just happened and resolved on Django1.3.

My problem was that the first request submitted via Ajax from a form was successfully done but the second attempt from the exact same from failed, resulted in 403 state even though the header 'X-CSRFToken' is correctly placed with the CSRF token value as well as in the case of the first attempt.

Hope this helps.Regards,Hiroyou can paste this js into your html file, remember put it before other js functionOne CSRF token is assigned to every session ( i.e. every time you log in).

So before you wish to get some data entered by user and send that as ajax call to some function which is protected by csrf_protect decorator, try to find the functions that are being called before you are getting this data from user. E.g. some template must be being rendered on which your user is entering data.

That template is being rendered by some function.

In this function you can get csrf  token as follows:

csrf = request.COOKIES['csrftoken']

Now pass this csrf value in context dictionary against which template in question is being rendered.

Now in that template write this line:  

Now in your javascript function, before making ajax request, write this:

 var csrf = $('#csrf').val()   this will pick value of token passed to template and store it in variable csrf.

Now while making ajax call, in your post data, pass this value as well :

"csrfmiddlewaretoken": csrf This will work even if you are not implementing django forms.In fact, logic over here is : You need token which you can get from request.

So you just need to figure out the function being called immediately after log in. Once you have this token, either make another ajax call to get it or pass it to some template which is accessible by your ajax.1) the django csrf check (assuming you're sending one) is here2) In my case, settings.CSRF_HEADER_NAME was set to 'HTTP_X_CSRFTOKEN' and my AJAX call was sending a header named 'HTTP_X_CSRF_TOKEN' so stuff wasn't working. I could either change it in the AJAX call, or django setting.3) If you opt to change it server-side, find your install location of django and throw a breakpoint in the csrf middleware.f you're using virtualenv, it'll be something like: ~/.envs/my-project/lib/python2.7/site-packages/django/middleware/csrf.pyThen, make sure the csrf token is correctly sourced from request.META4) If you need to change your header, etc - change that variable in your settings fileIf someone is strugling with axios to make this work this helped me:Source: https://cbuelter.wordpress.com/2017/04/10/django-csrf-with-axios/In my case the problem was with the nginx config that I've copied from main server to a temporary one with disabling https that is not needed on the second one in the process.I had to comment out these two lines in the config to make it work again:Here's a less verbose solution provided by Django:Source: https://docs.djangoproject.com/en/1.11/ref/csrf/

get list of pandas dataframe columns based on data type

yoshiserry

[get list of pandas dataframe columns based on data type](https://stackoverflow.com/questions/22470690/get-list-of-pandas-dataframe-columns-based-on-data-type)

If I have a dataframe with the following columns: I would like to be able to say: here is a dataframe, give me a list of the columns which are of type Object or of type DateTime?I have a function which converts numbers (Float64) to two decimal places, and I would like to use this list of dataframe columns, of a particular type, and run it through this function to convert them all to 2dp.Maybe:

2014-03-18 04:54:22Z

If I have a dataframe with the following columns: I would like to be able to say: here is a dataframe, give me a list of the columns which are of type Object or of type DateTime?I have a function which converts numbers (Float64) to two decimal places, and I would like to use this list of dataframe columns, of a particular type, and run it through this function to convert them all to 2dp.Maybe:If you want a list of columns of a certain type, you can use groupby:As of pandas v0.14.1, you can utilize select_dtypes() to select columns by dtypeUsing dtype will give you desired column's data type:if you want to know data types of all the column at once, you can use plural of dtype as dtypes:You can use boolean mask on the dtypes attribute:You can look at just those columns with the desired dtype:Now you can use round (or whatever) and assign it back:This should do the trickuse df.info(verbose=True) where df is a pandas datafarme, by default verbose=FalseThe most direct way to get a list of columns of certain dtype e.g. 'object':For example:To get all 'object' dtype columns:For just the list:If you want a list of only the object columns you could do:and then if you want to get another list of only the numerics:I came up with this three liner. Essentially, here's what it does: This made my life much easier in trying to generate schemas on the fly. Hope this helpsfor yoshiserry;I use infer_objects()df.infer_objects().dtypes

What is the source code of the「this」module doing?

byterussian

[What is the source code of the「this」module doing?](https://stackoverflow.com/questions/5855758/what-is-the-source-code-of-the-this-module-doing)

If you open a Python interpreter, and type "import this", as you know, it prints:In the python source(Lib/this.py) this text is generated by a curious piece of code:

2011-05-02 09:56:38Z

If you open a Python interpreter, and type "import this", as you know, it prints:In the python source(Lib/this.py) this text is generated by a curious piece of code:This is called rot13 encoding:Builds the translation table, for both uppercase (this is what 65 is for) and lowercase (this is what 97 is for) chars.Prints the translated string.If you want to make the ROT13 substitution by hand - or in your head - you can check that because 13*2 = 26 (the number of the letters of the English alphabet), it's essentially an interchange:Vs lbh cenpgvfr ybat rabhtu, lbh'yy riraghnyyl znfgre gur Mra bs EBG-13 nytbevguz naq ernq guvf Xyvatba ybbxvat grkgf jvgubhg pbzchgre uryc.It's a substitution cipher, rot13.It's a substitution cipher (as mentioned in previous answers). Historically speaking, it's the Caesar cipher.https://www.google.de/search?q=caesar+cipher&cad=hIt uses ROT13 encoding. This is used because it's a joke.You can also use Python functions to decode string.Python 2 only:Python 2 & 3:

How to find list intersection?

csguy11

[How to find list intersection?](https://stackoverflow.com/questions/3697432/how-to-find-list-intersection)

actual output: [1,3,5,6]

expected output: [1,3,5]How can we achieve a boolean AND operation (list intersection) on two lists?

2010-09-13 01:30:14Z

actual output: [1,3,5,6]

expected output: [1,3,5]How can we achieve a boolean AND operation (list intersection) on two lists?If order is not important and you don't need to worry about duplicates then you can use set intersection:Using list comprehensions is a pretty obvious one for me. Not sure about performance, but at least things stay lists.[x for x in a if x in b]Or "all the x values that are in A, if the X value is in B".If you convert the larger of the two lists into a set, you can get the intersection of that set with any iterable using intersection():Make a set out of the larger one:Then,will do what you want (preserving b's ordering, not a's -- can't necessarily preserve both) and do it fast.  (Using if x in a as the condition in the list comprehension would also work, and avoid the need to build _auxset, but unfortunately for lists of substantial length it would be a lot slower).If you want the result to be sorted, rather than preserve either list's ordering, an even neater way might be:Here's some Python 2 / Python 3 code that generates timing information for both list-based and set-based methods of finding the intersection of two lists. The pure list comprehension algorithms are O(n^2), since in on a list is a linear search. The set-based algorithms are O(n), since set search is O(1), and set creation is O(n) (and converting a set to a list is also O(n)). So for sufficiently large n the set-based algorithms are faster, but for small n the overheads of creating the set(s) make them slower than the pure list comp algorithms. output    Generated using a 2GHz single core machine with 2GB of RAM running Python 2.6.6 on a Debian flavour of Linux (with Firefox running in the background).  These figures are only a rough guide, since the actual speeds of the various algorithms are affected differently by the proportion of elements that are in both source lists.  Should work like a dream.  And, if you can, use sets instead of lists to avoid all this type changing!A functional way can be achieved using filter and lambda operator.Edit: It filters out x that exists in both list1 and list, set difference can also be achieved using:Edit2: python3 filter returns a filter object, encapsulating it with list returns the output list.This is an example when you need Each element in the result should appear as many times as it shows in both arrays.It might be late but I just thought I should share for the case where you are required to do it manually (show working - haha) OR when you need all elements to appear as many times as possible or when you also need it to be unique.Kindly note that tests have also been written for it.If, by Boolean AND, you mean items that appear in both lists, e.g. intersection, then you should look at Python's set and frozenset types.

How do I check if there are duplicates in a flat list?

teggy

[How do I check if there are duplicates in a flat list?](https://stackoverflow.com/questions/1541797/how-do-i-check-if-there-are-duplicates-in-a-flat-list)

For example, given the list ['one', 'two', 'one'], the algorithm should return True, whereas given ['one', 'two', 'three'] it should return False.

2009-10-09 04:30:32Z

For example, given the list ['one', 'two', 'one'], the algorithm should return True, whereas given ['one', 'two', 'three'] it should return False.Use set() to remove duplicates if all values are hashable:Recommended for short lists only:Do not use on a long list -- it can take time proportional to the square of the number of items in the list!For longer lists with hashable items (strings, numbers, &c):If your items are not hashable (sublists, dicts, etc) it gets hairier, though it may still be possible to get O(N logN) if they're at least comparable.  But you need to know or test the characteristics of the items (hashable or not, comparable or not) to get the best performance you can -- O(N) for hashables, O(N log N) for non-hashable comparables, otherwise it's down to O(N squared) and there's nothing one can do about it:-(.This is old, but the answers here led me to a slightly different solution.  If you are up for abusing comprehensions, you can get short-circuiting this way.If you are fond of functional programming style, here is a useful function, self-documented and tested code using doctest.From there you can test unicity by checking whether the second element of the returned pair is empty:Note that this is not efficient since you are explicitly constructing the decomposition. But along the line of using reduce, you can come up to something equivalent (but slightly less efficient) to answer 5:I recently answered a related question to establish all the duplicates in a list, using a generator. It has the advantage that if used just to establish 'if there is a duplicate' then you just need to get the first item and the rest can be ignored, which is the ultimate shortcut.This is an interesting set based approach I adapted straight from moooeeeep:Accordingly, a full list of dupes would be list(getDupes(etc)).  To simply test "if" there is a dupe, it should be wrapped as follows:This scales well and provides consistent operating times wherever the dupe is in the list -- I tested with lists of up to 1m entries.  If you know something about the data, specifically, that dupes are likely to show up in the first half, or other things that let you skew your requirements, like needing to get the actual dupes, then there are a couple of really alternative dupe locators that might outperform. The two I recommend are...Simple dict based approach, very readable:Leverage itertools (essentially an ifilter/izip/tee) on the sorted list, very efficient if you are getting all the dupes though not as quick to get just the first:These were the top performers from the approaches I tried for the full dupe list, with the first dupe occurring anywhere in a 1m element list from the start to the middle.  It was surprising how little overhead the sort step added.  Your mileage may vary, but here are my specific timed results:Another way of doing this succinctly is with Counter.To just determine if there are any duplicates in the original list:Or to get a list of items that have duplicates:I thought it would be useful to compare the timings of the different solutions presented here. For this I used my own library simple_benchmark:So indeed for this case the solution from Denis Otkidach is fastest.Some of the approaches also exhibit a much steeper curve, these are the approaches that scale quadratic with the number of elements (Alex Martellis first solution, wjandrea and both of Xavier Decorets solutions). Also important to mention is that the pandas solution from Keiku has a very big constant factor. But for larger lists it almost catches up with the other solutions.And in case the duplicate is at the first position. This is useful to see which solutions are short-circuiting:Here several approaches don't short-circuit: Kaiku, Frank, Xavier_Decoret (first solution), Turn, Alex Martelli (first solution) and the approach presented by Denis Otkidach (which was fastest in the no-duplicate case).I included a function from my own library here: iteration_utilities.all_distinct which can compete with the fastest solution in the no-duplicates case and performs in constant-time for the duplicate-at-begin case (although not as fastest).The code for the benchmark:And for the arguments:I found this to do the best performance because it short-circuit the operation when the first duplicated it found, then this algorithm has time and space complexity O(n) where n is the list's length:I dont really know what set does behind the scenes, so I just like to keep it simple.A more simple solution is as follows. Just check True/False with pandas .duplicated() method and then take sum. Please also see  pandas.Series.duplicated — pandas 0.24.1 documentation If the list contains unhashable items, you can use Alex Martelli's solution but with a list instead of a set, though it's slower for larger inputs: O(N^2).I used pyrospade's approach, for its simplicity, and modified that slightly on a short list made from the case-insensitive Windows registry. If the raw PATH value string is split into individual paths all 'null' paths (empty or whitespace-only strings) can be removed by using:The original PATH has both 'null' entries and duplicates for testing purposes:Null paths have been removed, but still has duplicates, e.g., (1, 3) and (13, 20):And finally, the dupes have been removed:

Call int() function on every list element?

Silver Light

[Call int() function on every list element?](https://stackoverflow.com/questions/3371269/call-int-function-on-every-list-element)

I have a list with numeric strings, like so:I would like to convert every list element to integer, so it would look like this:I could do it using a loop, like so:Does it have to be so ugly? I'm sure there is a more pythonic way to do this in a one line of code. Please help me out.

2010-07-30 12:12:58Z

I have a list with numeric strings, like so:I would like to convert every list element to integer, so it would look like this:I could do it using a loop, like so:Does it have to be so ugly? I'm sure there is a more pythonic way to do this in a one line of code. Please help me out.This is what list comprehensions are for:In Python 2.x another approach is to use map:Note: in Python 3.x map returns a map object which you can convert to a list if you want:just a point,the list comprehension is more natural, whileis faster. Probably this will not matter in most casesUseful read: LP vs mapIf you are intending on passing those integers to a function or method, consider this example:This construction is intentionally remarkably similar to list comprehensions mentioned by adamk. Without the square brackets, it's called a generator expression, and is a very memory-efficient way of passing a list of arguments to a method. A good discussion is available here: Generator Expressions vs. List ComprehensionAnother way to make it in Python 3:numbers = [*map(int, numbers)]Another way,Thought I'd consolidate the answers and show some timeit results.Python 2 sucks pretty bad at this, but map is a bit faster than comprehension.Python 3 is over 4x faster by itself, but converting the map generator object to a list is still faster than comprehension, and creating the list by unpacking the map generator (thanks Artem!) is slightly faster still.Note: In Python 3, 4 elements seems to be the crossover point (3 in Python 2) where comprehension is slightly faster, though unpacking the generator is still faster than either for lists with more than 1 element.

Python Infinity - Any caveats?

Casebash

[Python Infinity - Any caveats?](https://stackoverflow.com/questions/1628026/python-infinity-any-caveats)

So Python has positive and negative infinity:This just seems like the type of feature that has to have some caveat. Is there anything I should be aware of? 

2009-10-27 00:12:06Z

So Python has positive and negative infinity:This just seems like the type of feature that has to have some caveat. Is there anything I should be aware of? You can still get not-a-number (NaN) values from simple arithmetic involving inf:Note that you will normally not get an inf value through usual arithmetic calculations:The inf value is considered a very special value with unusual semantics, so it's better to know about an OverflowError straight away through an exception, rather than having an inf value silently injected into your calculations.Python's implementation follows the IEEE-754 standard pretty well, which you can use as a guidance, but it relies on the underlying system it was compiled on, so platform differences may occur. Recently¹, a fix has been applied that allows "infinity" as well as "inf", but that's of minor importance here.The following sections equally well apply to any language that implements IEEE floating point arithmetic correctly, it is not specific to just Python.When dealing with infinity and greater-than > or less-than < operators, the following counts:When compared for equality, +inf and +inf are equal, as are -inf and -inf. This is a much debated issue and may sound controversial to you, but it's in the IEEE standard and Python behaves just like that.Of course, +inf is unequal to -inf and everything, including NaN itself, is unequal to NaN.Most calculations with infinity will yield infinity, unless both operands are infinity, when the operation division or modulo, or with multiplication with zero, there are some special rules to keep in mind:Note 1: as an additional caveat, that as defined by the IEEE standard, if your calculation result under-or overflows, the result will not be an under- or overflow error, but positive or negative infinity: 1e308 * 10.0 yields inf.Note 2: because any calculation with NaN returns NaN and any comparison to NaN, including NaN itself is false, you should use the math.isnan function to determine if a number is indeed NaN.Note 3: though Python supports writing float('-NaN'), the sign is ignored, because there exists no sign on NaN internally. If you divide -inf / +inf, the result is NaN, not -NaN (there is no such thing).Note 4: be careful to rely on any of the above, as Python relies on the C or Java library it was compiled for and not all underlying systems implement all this behavior correctly. If you want to be sure, test for infinity prior to doing your calculations.¹) Recently means since version 3.2.

²) Floating points support positive and negative zero, so: x / float('inf') keeps its sign and -1 / float('inf') yields -0.0, 1 / float(-inf) yields -0.0, 1 / float('inf') yields 0.0 and -1/ float(-inf) yields 0.0. In addition, 0.0 == -0.0 is true, you have to manually check the sign if you don't want it to be true.So does C99.The IEEE 754 floating point representation used by all modern processors has several special bit patterns reserved for positive infinity (sign=0, exp=~0, frac=0), negative infinity (sign=1, exp=~0, frac=0), and many NaN (Not a Number: exp=~0, frac≠0).All you need to worry about: some arithmetic may cause floating point exceptions/traps, but those aren't limited to only these "interesting" constants.I found a caveat that no one so far has mentioned. I don't know if it will come up often in practical situations, but here it is for the sake of completeness.Usually, calculating a number modulo infinity returns itself as a float, but a fraction modulo infinity returns nan (not a number). Here is an example:I filed an issue on the Python bug tracker. It can be seen at https://bugs.python.org/issue32968.Update: this will be fixed in Python 3.8.in a 1/x fraction, up to x = 1e-323 it is inf but when x = 1e-324 or little it throws ZeroDivisionErrorso be cautious!

What is the difference between encode/decode?

ʞɔıu

[What is the difference between encode/decode?](https://stackoverflow.com/questions/447107/what-is-the-difference-between-encode-decode)

I've never been sure that I understand the difference between str/unicode decode and encode.I know that str().decode() is for when you have a string of bytes that you know has a certain character encoding, given that encoding name it will return a unicode string.I know that unicode().encode() converts unicode chars into a string of bytes according to a given encoding name.But I don't understand what str().encode() and unicode().decode() are for. Can anyone explain, and possibly also correct anything else I've gotten wrong above?EDIT:Several answers give info on what .encode does on a string, but no-one seems to know what .decode does for unicode.

2009-01-15 15:13:59Z

I've never been sure that I understand the difference between str/unicode decode and encode.I know that str().decode() is for when you have a string of bytes that you know has a certain character encoding, given that encoding name it will return a unicode string.I know that unicode().encode() converts unicode chars into a string of bytes according to a given encoding name.But I don't understand what str().encode() and unicode().decode() are for. Can anyone explain, and possibly also correct anything else I've gotten wrong above?EDIT:Several answers give info on what .encode does on a string, but no-one seems to know what .decode does for unicode.The decode method of unicode strings really doesn't have any applications at all (unless you have some non-text data in a unicode string for some reason -- see below). It is mainly there for historical reasons, i think. In Python 3 it is completely gone.unicode().decode() will perform an implicit encoding of s using the default (ascii) codec. Verify this like so:The error messages are exactly the same.For str().encode() it's the other way around -- it attempts an implicit decoding of s with the default encoding:Used like this, str().encode() is also superfluous.But there is another application of the latter method that is useful: there are encodings that have nothing to do with character sets, and thus can be applied to 8-bit strings in a meaningful way:You are right, though: the ambiguous usage of "encoding" for both these applications is... awkard. Again, with separate byte and string types in Python 3, this is no longer an issue.To represent a unicode string as a string of bytes is known as encoding. Use u'...'.encode(encoding).Example:You typically encode a unicode string whenever you need to use it for IO, for instance transfer it over the network, or save it to a disk file.To convert a string of bytes to a unicode string is known as decoding. Use unicode('...', encoding) or '...'.decode(encoding).Example:You typically decode a string of bytes whenever you receive string data from the network or from a disk file.I believe there are some changes in unicode handling in python 3, so the above is probably not correct for python 3.Some good links:anUnicode.encode('encoding') results in a string object and can be called on a unicode object  aString.decode('encoding') results in an unicode object and can be called on a string, encoded in given encoding.Some more explanations:You can create some unicode object, which doesn't have any encoding set. The way it is stored by Python in memory is none of your concern. You can search it, split it and call any string manipulating function you like.But there comes a time, when you'd like to print your unicode object to console or into some text file. So you have to encode it (for example - in UTF-8), you call encode('utf-8') and you get a string with '\u<someNumber>' inside, which is perfectly printable.Then, again - you'd like to do the opposite - read string encoded in UTF-8 and treat it as an Unicode, so the \u360 would be one character, not 5. Then you decode a string (with selected encoding) and get brand new object of the unicode type.Just as a side note - you can select some pervert encoding, like 'zip', 'base64', 'rot' and some of them will convert from string to string, but I believe the most common case is one that involves UTF-8/UTF-16 and string.mybytestring.encode(somecodec) is meaningful for these values of somecodec:I am not sure what decoding an already decoded unicode text is good for. Trying that with any encoding seems to always try to encode with the system's default encoding first.There are a few encodings that can be used to de-/encode from str to str or from unicode to unicode. For example base64, hex or even rot13. They are listed in the codecs module.Edit:The decode message on a unicode string can undo the corresponding encode operation:The returned type is str instead of unicode which is unfortunate in my opinion. But when you are not doing a proper en-/decode between str and unicode this looks like a mess anyway.The simple answer is that they are the exact opposite of each other.The computer uses the very basic unit of byte to store and process information; it is meaningless for human eyes.For example,'\xe4\xb8\xad\xe6\x96\x87' is the representation of two Chinese characters, but the computer only knows (meaning print or store) it is Chinese Characters when they are given a dictionary to look for that Chinese word, in this case, it is a "utf-8" dictionary, and it would fail to correctly show the intended Chinese word if you look into a different or wrong dictionary (using a different decoding method). In the above case, the process for a computer to look for Chinese word is decode(). And the process of computer writing the Chinese into computer memory is encode().So the encoded information is the raw bytes, and the decoded information is the raw bytes and the name of the dictionary to reference (but not the dictionary itself).

Convert string to Python class object?

S.Lott

[Convert string to Python class object?](https://stackoverflow.com/questions/1176136/convert-string-to-python-class-object)

Given a string as user input to a Python function, I'd like to get a class object out of it if there's a class with that name in the currently defined namespace. Essentially, I want the implementation for a function which will produce this kind of result:Is this, at all, possible?

2009-07-24 07:01:23Z

Given a string as user input to a Python function, I'd like to get a class object out of it if there's a class with that name in the currently defined namespace. Essentially, I want the implementation for a function which will produce this kind of result:Is this, at all, possible?This seems simplest.This could work:You could do something like:You want the class Baz, which lives in module foo.bar. With Python 2.7,

you want to use importlib.import_module(), as this will make transitioning to Python 3 easier: With Python < 2.7:Use:This accurately handles both old-style and new-style classes.I've looked at how django handles thisdjango.utils.module_loading has thisYou can use it like import_string("module_path.to.all.the.way.to.your_class")Yes, you can do this. Assuming your classes exist in the global namespace, something like this will do it:In terms of arbitrary code execution, or undesired user passed names, you could have a list of acceptable function/class names, and if the input matches one in the list, it is eval'd.PS: I know....kinda late....but it's for anyone else who stumbles across this in the future.Using importlib worked the best for me.This uses string dot notation for the python module that you want to import.If you really want to retrieve classes you make with a string, you should store (or properly worded, reference) them in a dictionary. After all, that'll also allow to name your classes in a higher level and avoid exposing unwanted classes.Example, from a game where actor classes are defined in Python and you want to avoid other general classes to be reached by user input.Another approach (like in the example below) would to make an entire new class, that holds the dict above. This would:Example:This returns me:Another fun experiment to do with those is to add a method that pickles the ClassHolder so you never lose all the classes you did :^)

Why is there no GIL in the Java Virtual Machine? Why does Python need one so bad?

AgentLiquid

[Why is there no GIL in the Java Virtual Machine? Why does Python need one so bad?](https://stackoverflow.com/questions/991904/why-is-there-no-gil-in-the-java-virtual-machine-why-does-python-need-one-so-bad)

I'm hoping someone can provide some insight as to what's fundamentally different about the Java Virtual Machine that allows it to implement threads nicely without the need for a Global Interpreter Lock (GIL), while Python necessitates such an evil.

2009-06-14 01:12:55Z

I'm hoping someone can provide some insight as to what's fundamentally different about the Java Virtual Machine that allows it to implement threads nicely without the need for a Global Interpreter Lock (GIL), while Python necessitates such an evil.Python (the language) doesn't need a GIL (which is why it can perfectly be implemented on JVM [Jython] and .NET [IronPython], and those implementations multithread freely). CPython (the popular implementation) has always used a GIL for ease of coding (esp. the coding of the garbage collection mechanisms) and of integration of non-thread-safe C-coded libraries (there used to be a ton of those around;-).The Unladen Swallow project, among other ambitious goals, does plan a GIL-free virtual machine for Python -- to quote that site, "In addition, we intend to remove the GIL and fix the state of multithreading in Python. We believe this is possible through the implementation of a more sophisticated GC system, something like IBM's Recycler (Bacon et al, 2001)."The JVM (at least hotspot) does have a similar concept to the "GIL", it's just much finer in its lock granularity, most of this comes from the GC's in hotspot which are more advanced.In CPython it's one big lock (probably not that true, but good enough for arguments sake), in the JVM it's more spread about with different concepts depending on where it is used.Take a look at, for example, vm/runtime/safepoint.hpp in the hotspot code, which is effectively a barrier. Once at a safepoint the entire VM has stopped with regard to java code, much like the python VM stops at the GIL.In the Java world such VM pausing events are known as "stop-the-world", at these points only native code that is bound to certain criteria is free running, the rest of the VM has been stopped.Also the lack of a coarse lock in java makes JNI much more difficult to write, as the JVM makes less guarantees about its environment for FFI calls, one of the things that cpython makes fairly easy (although not as easy as using ctypes).There is a comment down below in this blog post http://www.grouplens.org/node/244 that hints at the reason why it was so easy dispense with a GIL for IronPython or Jython, it is that CPython uses reference counting whereas the other 2 VMs have garbage collectors.The exact mechanics of why this is so I don't get, but it does sounds like a plausible reason.In this link they have the following explanation:... "Parts of the Interpreter aren't threadsafe, though mostly because making them all threadsafe by massive lock usage would slow single-threaded extremely (source). This seems to be related to the CPython garbage collector using reference counting (the JVM and CLR don't, and therefore don't need to lock/release a reference count every time). But even if someone thought of an acceptable solution and implemented it, third party libraries would still have the same problems."Python lacks jit/aot and the time frame it was written at multithreaded processors didn't exist. Alternatively you could recompile everything in Julia lang which lacks GIL and gain some speed boost on your Python code. Also Jython kind of sucks it's slower than Cpython and Java. If you want to stick to Python consider using parallel plugins, you won't gain an instant speed boost but you can do parallel programming with the right plugin.

Let JSON object accept bytes or let urlopen output strings

Peter Smit

[Let JSON object accept bytes or let urlopen output strings](https://stackoverflow.com/questions/6862770/let-json-object-accept-bytes-or-let-urlopen-output-strings)

With Python 3 I am requesting a json document from a URL.The response object is a file-like object with read and readline methods. Normally a JSON object can be created with a file opened in text mode.What I would like to do is:This however does not work as urlopen returns a file object in binary mode.A work around is of course:but this feels bad...Is there a better way that I can transform a bytes file object to a string file object? Or am I missing any parameters for either urlopen or json.load to give an encoding?

2011-07-28 17:00:24Z

With Python 3 I am requesting a json document from a URL.The response object is a file-like object with read and readline methods. Normally a JSON object can be created with a file opened in text mode.What I would like to do is:This however does not work as urlopen returns a file object in binary mode.A work around is of course:but this feels bad...Is there a better way that I can transform a bytes file object to a string file object? Or am I missing any parameters for either urlopen or json.load to give an encoding?HTTP sends bytes. If the resource in question is text, the character encoding is normally specified, either by the Content-Type HTTP header or by another mechanism (an RFC, HTML meta http-equiv,...).urllib should know how to encode the bytes to a string, but it's too naïve—it's a horribly underpowered and un-Pythonic library.Dive Into Python 3 provides an overview about the situation.Your "work-around" is fine—although it feels wrong, it's the correct way to do it.Python’s wonderful standard library to the rescue…Works with both py2 and py3.Docs: Python 2, Python3I have come to opinion that the question is the best answer :)For anyone else trying to solve this using the requests library:This one works for me, I used 'request' library with json() check out the doc in requests for humansI ran into similar problems using Python 3.4.3 & 3.5.2 and Django 1.11.3. However, when I upgraded to Python 3.6.1 the problems went away.You can read more about it here:

https://docs.python.org/3/whatsnew/3.6.html#jsonIf you're not tied to a specific version of Python, just consider upgrading to 3.6 or later.If you're experiencing this issue whilst using the flask microframework, then you can just do:data = json.loads(response.get_data(as_text=True))From the docs: "If as_text is set to True the return value will be a decoded unicode string"Your workaround actually just saved me. I was having a lot of problems processing the request using the Falcon framework. This worked for me. req being the request form curl pr httpieThis will stream the byte data into json.io.TextIOWrapper is preferred to the codec's module reader. https://www.python.org/dev/peps/pep-0400/Just found this simple method to make HttpResponse content as a jsonHope that helps youAs of Python 3.6, you can use json.loads() to deserialize a bytesobject directly (the encoding must be UTF-8, UTF-16 or UTF-32). So, using only modules from the standard library, you can do:I used below program to use of json.loads()

Python import csv to list

MorganTN

[Python import csv to list](https://stackoverflow.com/questions/24662571/python-import-csv-to-list)

I have a CSV file with about 2000 records. Each record has a string, and a category to it:I need to read this file into a list that looks like this:How can import this CSV to the list I need using Python?

2014-07-09 19:48:13Z

I have a CSV file with about 2000 records. Each record has a string, and a category to it:I need to read this file into a list that looks like this:How can import this CSV to the list I need using Python?Using the csv module:Output:If you need tuples:Output:Old Python 2 answer, also using the csv module:Updated for Python 3:Output:Pandas is pretty good at dealing with data. Here is one example how to use it:One big advantage is that pandas deals automatically with header rows.If you haven't heard of Seaborn, I recommend having a look at it.See also: How do I read and write CSV files with Python?

The content of df is:The content of dicts isThe content of lists is:Output:If csvfile is a file object, it should be opened with newline=''.

csv module If you are sure there are no commas in your input, other than to separate the category, you can read the file line by line and split on ,, then push the result to ListThat said, it looks like you are looking at a CSV file, so you might consider using the modules for it As said already in the comments you can use the csv library in python. csv means comma separated values which seems exactly your case: a label and a value separated by a comma.Being a category and value type I would rather use a dictionary type instead of a list of tuples.Anyway in the code below I show both ways: d is the dictionary and l is the list of tuples.A simple loop would suffice:Unfortunately I find none of the existing answers particularly satisfying. Here is a straightforward and complete Python 3 solution, using the csv module.Notice the skipinitialspace=True argument. This is necessary since, unfortunately, OP's CSV contains whitespace after each comma.Output:Extending your requirements a bit and assuming you do not care about the order of lines and want to get them grouped under categories, the following solution may work for you:This way you get all relevant lines available in the dictionary under key being the category.Here is the easiest way in Python 3.x to import a CSV to a multidimensional array, and its only 4 lines of code without importing anything!Next is a piece of code which uses csv module but extracts file.csv contents to a list of dicts using the first line which is a header of csv table

How to strip all whitespace from string

wrongusername

[How to strip all whitespace from string](https://stackoverflow.com/questions/3739909/how-to-strip-all-whitespace-from-string)

How do I strip all the spaces in a python string? For example, I want a string like strip my spaces to be turned into stripmyspaces, but I cannot seem to accomplish that with strip():

2010-09-18 00:42:10Z

How do I strip all the spaces in a python string? For example, I want a string like strip my spaces to be turned into stripmyspaces, but I cannot seem to accomplish that with strip():Taking advantage of str.split's behavior with no sep parameter:If you just want to remove spaces instead of all whitespace:Even though efficiency isn't the primary goal—writing clear code is—here are some initial timings:Note the regex is cached, so it's not as slow as you'd imagine.  Compiling it beforehand helps some, but would only matter in practice if you call this many times:Even though re.sub is 11.3x slower, remember your bottlenecks are assuredly elsewhere.  Most programs would not notice the difference between any of these 3 choices.Also handles any whitespace characters that you're not thinking of (believe me, there are plenty).Alternatively,And here is Python3 version:The simplest is to use replace:Alternatively, use a regular expression:Try a regex with re.sub. You can search for all whitespace and replace with an empty string.\s in your pattern will match whitespace characters - and not just a space (tabs, newlines, etc). You can read more about it in the manual.As mentioned by Roger Pate following code worked for me:I am using Jupyter Notebook to run following code:The standard techniques to filter a list apply, although they are not as efficient as the split/join or translate methods.We need a set of whitespaces:The filter builtin:A list comprehension (yes, use the brackets: see benchmark below):A fold:Benchmark:TL/DRThis solution was tested using Python 3.6To strip all spaces from a string in Python3 you can use the following function:To remove any whitespace characters (' \t\n\r\x0b\x0c') you can use the following function:ExplanationPython's str.translate method is a built-in class method of str, it takes a table and returns a copy of the string with each character mapped through the passed translation table. Full documentation for str.translateTo create the translation table str.maketrans is used. This method is another built-in class method of str. Here we use it with only one parameter, in this case a dictionary, where the keys are the characters to be replaced mapped to values with the characters replacement value. It returns a translation table for use with str.translate. Full documentation for str.maketransThe string module in python contains some common string operations and constants. string.whitespace is a constant which returns a string containing all ASCII characters that are considered whitespace. This includes the characters space, tab, linefeed, return, formfeed, and vertical tab.Full documentation for stringIn the second function dict.fromkeys is used to create a dictionary where the keys are the characters in the string returned by string.whitespace each with value None. Full documentation for dict.fromkeysIf optimal performance is not a requirement and you just want something dead simple, you can define a basic function to test each character using the string class's built in "isspace" method:Building the no_white_space string this way will not have ideal performance, but the solution is easy to understand.If you don't want to define a function, you can convert this into something vaguely similar with list comprehension. Borrowing from the top answer's join solution:

UnicodeEncodeError: 'charmap' codec can't encode characters

SstrykerR

[UnicodeEncodeError: 'charmap' codec can't encode characters](https://stackoverflow.com/questions/27092833/unicodeencodeerror-charmap-codec-cant-encode-characters)

I'm trying to scrape a website, but it gives me an error.I'm using the following code:And I'm getting the following error:What can I do to fix this?

2014-11-23 18:47:01Z

I'm trying to scrape a website, but it gives me an error.I'm using the following code:And I'm getting the following error:What can I do to fix this?I was getting the same UnicodeEncodeError when saving scraped web content to a file. To fix it I replaced this code:with this:Using io gives you backward compatibility with Python 2.If you only need to support Python 3 you can use the builtin open function instead:I fixed it by adding .encode("utf-8") to soup.That means that print(soup) becomes print(soup.encode("utf-8")).In Python 3.7, and running Windows 10 this worked (I am not sure whether it will work on other platforms and/or other versions of Python)Replacing this line:with open('filename', 'w') as f:With this:with open('filename', 'w', encoding='utf-8') as f:The reason why it is working is because the encoding is changed to UTF-8 when using the file, so characters in UTF-8 are able to be converted to text, instead of returning an error when it encounters a UTF-8 character that is not suppord by the current encoding.While saving the response of get request, same error was thrown on Python 3.7 on window 10. The response received from the URL, encoding was UTF-8 so it is always recommended to check the encoding so same can be passed to avoid such trivial issue as it really kills lots of time in productionWhen I added encoding="utf-8" with the open command it saved the file with the correct response Even I faced the same issue with the encoding that occurs when you try to print it, read/write it or open it. As others mentioned above adding .encoding="utf-8" will help if you are trying to print it. If you are trying to open scraped data and maybe write it into a file, then open the file with (......,encoding="utf-8")For those still getting this error, adding encode("utf-8") to soup will also fix this.

How to iterate over columns of pandas dataframe to run regression

itzy

[How to iterate over columns of pandas dataframe to run regression](https://stackoverflow.com/questions/28218698/how-to-iterate-over-columns-of-pandas-dataframe-to-run-regression)

I'm sure this is simple, but as a complete newbie to python, I'm having trouble figuring out how to iterate over variables in a pandas dataframe and run a regression with each.Here's what I'm doing:I know I can run a regression like this:but suppose I want to do this for each column in the dataframe. In particular, I want to regress FIUIX on FSTMX, and then FSAIX on FSTMX, and then FSAVX on FSTMX. After each regression I want to store the residuals.I've tried various versions of the following, but I must be getting the syntax wrong:I think the problem is I don't know how to refer to the returns column by key, so returns[k] is probably wrong.Any guidance on the best way to do this would be much appreciated. Perhaps there's a common pandas approach I'm missing.

2015-01-29 15:42:25Z

I'm sure this is simple, but as a complete newbie to python, I'm having trouble figuring out how to iterate over variables in a pandas dataframe and run a regression with each.Here's what I'm doing:I know I can run a regression like this:but suppose I want to do this for each column in the dataframe. In particular, I want to regress FIUIX on FSTMX, and then FSAIX on FSTMX, and then FSAVX on FSTMX. After each regression I want to store the residuals.I've tried various versions of the following, but I must be getting the syntax wrong:I think the problem is I don't know how to refer to the returns column by key, so returns[k] is probably wrong.Any guidance on the best way to do this would be much appreciated. Perhaps there's a common pandas approach I'm missing.You can use iteritems():This answer is to iterate over selected columns as well as all columns in a DF.df.columns gives a list containing all the columns' names in the DF. Now that isn't very helpful if you want to iterate over all the columns. But it comes in handy when you want to iterate over columns of your choosing only. We can use Python's list slicing easily to slice df.columns according to our needs. For eg, to iterate over all columns but the first one, we can do:Similarly to iterate over all the columns in reversed order, we can do:We can iterate over all the columns in a lot of cool ways using this technique. Also remember that you can get the indices of all columns easily using:You can index dataframe columns by the position using ix.This returns the first column for example. (0 would be the index)This returns the first row.This would be the value at the intersection of row 0 and column 1:and so on. So you can enumerate() returns.keys(): and use the number to index the dataframe.A workaround is to transpose the DataFrame and iterate over the rows.Using list comprehension, you can get all the columns names (header):[column for column in df]  I'm a bit late but here's how I did this. The steps:This is the code I used on DataFrame called aft_tmt. Feel free to extrapolate to your use case..Based on the accepted answer, if an index corresponding to each column is also desired:The above df[column] type is Series, which can simply be converted into numpy ndarrays:To iterate over the rows of a data frame (rather than its column names as mostly shown in the other answers), you can usewhich outputsTo iterate by column rather than by row, simply transpose df.values:

SFTP in Python? (platform independent)

Mark Wilbur

[SFTP in Python? (platform independent)](https://stackoverflow.com/questions/432385/sftp-in-python-platform-independent)

I'm working on a simple tool that transfers files to a hard-coded location with the password also hard-coded.  I'm a python novice, but thanks to ftplib, it was easy:The problem is that I can't find any library that supports sFTP.  What's the normal way to do something like this securely?  Edit:  Thanks to the answers here, I've gotten it working with Paramiko and this was the syntax.Thanks again!

2009-01-11 04:48:19Z

I'm working on a simple tool that transfers files to a hard-coded location with the password also hard-coded.  I'm a python novice, but thanks to ftplib, it was easy:The problem is that I can't find any library that supports sFTP.  What's the normal way to do something like this securely?  Edit:  Thanks to the answers here, I've gotten it working with Paramiko and this was the syntax.Thanks again!Paramiko supports SFTP. I've used it, and I've used Twisted. Both have their place, but you might find it easier to start with Paramiko.You should check out pysftp https://pypi.python.org/pypi/pysftp  it depends on paramiko, but wraps most common use cases to just a few lines of code.If you want easy and simple, you might also want to look at Fabric. It's an automated deployment tool like Ruby's Capistrano, but simpler and of course for Python. It's build on top of Paramiko.You might not want to do 'automated deployment' but Fabric would suit your use case perfectly none the less. To show you how simple Fabric is: the fab file and command for your script would look like this (not tested, but 99% sure it will work):fab_putfile.py:Then run the file with the fab command:And you're done! :)Here is a sample using pysftp and a private key.pysftp is an easy to use sftp module that utilizes paramiko and pycrypto. It provides a simple interface to sftp.. Other things that you can do with pysftp which are quite useful:More commands and about PySFTP here.Twisted can help you with what you are doing, check out their documentation, there are plenty of examples. Also it is a mature product with a big developer/user community behind it. With RSA Key then refer here Snippet:There are a bunch of answers that mention pysftp, so in the event that you want a context manager wrapper around pysftp, here is a solution that is even less code that ends up looking like the following when usedThe (fuller) example: http://www.prschmid.com/2016/09/simple-opensftp-context-manager-for.htmlThis context manager happens to have auto-retry logic baked in in the event you can't connect the first time around (which surprisingly happens more often than you'd expect in a production environment...)The context manager gist for open_sftp: https://gist.github.com/prschmid/80a19c22012e42d4d6e791c1e4eb8515Paramiko is so slow. Use subprocess and shell, here is an example:You can use the pexpect module Here is a good intro postI haven't tested this but it should work

live output from subprocess command

DilithiumMatrix

[live output from subprocess command](https://stackoverflow.com/questions/18421757/live-output-from-subprocess-command)

I'm using a python script as a driver for a hydrodynamics code.  When it comes time to run the simulation, I use subprocess.Popen to run the code, collect the output from stdout and stderr into a subprocess.PIPE --- then I can print (and save to a log-file) the output information, and check for any errors.  The problem is, I have no idea how the code is progressing.  If I run it directly from the command line, it gives me output about what iteration its at, what time, what the next time-step is, etc.Is there a way to both store the output (for logging and error checking), and also produce a live-streaming output?The relevant section of my code:Originally I was piping the run_command through tee so that a copy went directly to the log-file, and the stream still output directly to the terminal -- but that way I can't store any errors (to my knowlege).Edit:Temporary solution:then, in another terminal, run tail -f log.txt (s.t. log_file = 'log.txt').

2013-08-24 18:27:18Z

I'm using a python script as a driver for a hydrodynamics code.  When it comes time to run the simulation, I use subprocess.Popen to run the code, collect the output from stdout and stderr into a subprocess.PIPE --- then I can print (and save to a log-file) the output information, and check for any errors.  The problem is, I have no idea how the code is progressing.  If I run it directly from the command line, it gives me output about what iteration its at, what time, what the next time-step is, etc.Is there a way to both store the output (for logging and error checking), and also produce a live-streaming output?The relevant section of my code:Originally I was piping the run_command through tee so that a copy went directly to the log-file, and the stream still output directly to the terminal -- but that way I can't store any errors (to my knowlege).Edit:Temporary solution:then, in another terminal, run tail -f log.txt (s.t. log_file = 'log.txt').You have two ways of doing this, either by creating an iterator from the read or readline functions and do:orOr you can create a reader and a writer file. Pass the writer to the Popen and read from the readerThis way you will have the data written in the test.log as well as on the standard output. The only advantage of the file approach is that your code doesn't block. So you can do whatever you want in the meantime and read whenever you want from the reader in a non-blocking way. When you use PIPE, read and readline functions will block until either one character is written to the pipe or a line is written to the pipe respectively.It may be time to explain a bit about how subprocess.Popen does its thing.(Caveat: this is for Python 2.x, although 3.x is similar; and I'm quite fuzzy on the Windows variant.  I understand the POSIX stuff much better.)The Popen function needs to deal with zero-to-three I/O streams, somewhat simultaneously.  These are denoted stdin, stdout, and stderr as usual.You can provide:If you redirect nothing (leave all three as the default None value or supply explicit None), Pipe has it quite easy.  It just needs to spin off the subprocess and let it run.  Or, if you redirect to a non-PIPE—an int or a stream's fileno()—it's still easy, as the OS does all the work.  Python just needs to spin off the subprocess, connecting its stdin, stdout, and/or stderr to the provided file descriptors.If you redirect only one stream, Pipe still has things pretty easy.  Let's pick one stream at a time and watch.Suppose you want to supply some stdin, but let stdout and stderr go un-redirected, or go to a file descriptor.  As the parent process, your Python program simply needs to use write() to send data down the pipe.  You can do this yourself, e.g.:or you can pass the stdin data to proc.communicate(), which then does the stdin.write shown above.  There is no output coming back so communicate() has only one other real job: it also closes the pipe for you.  (If you don't call proc.communicate() you must call proc.stdin.close() to close the pipe, so that the subprocess knows there is no more data coming through.)Suppose you want to capture stdout but leave stdin and stderr alone.  Again, it's easy: just call proc.stdout.read() (or equivalent) until there is no more output.  Since proc.stdout() is a normal Python I/O stream you can use all the normal constructs on it, like:or, again, you can use proc.communicate(), which simply does the read() for you.If you want to capture only stderr, it works the same as with stdout.There's one more trick before things get hard.  Suppose you want to capture stdout, and also capture stderr but on the same pipe as stdout:In this case, subprocess "cheats"!  Well, it has to do this, so it's not really cheating: it starts the subprocess with both its stdout and its stderr directed into the (single) pipe-descriptor that feeds back to its parent (Python) process.  On the parent side, there's again only a single pipe-descriptor for reading the output.  All the "stderr" output shows up in proc.stdout, and if you call proc.communicate(), the stderr result (second value in the tuple) will be None, not a string.The problems all come about when you want to use at least two pipes.  In fact, the subprocess code itself has this bit:But, alas, here we've made at least two, and maybe three, different pipes, so the count(None) returns either 1 or 0.  We must do things the hard way.On Windows, this uses threading.Thread to accumulate results for self.stdout and self.stderr, and has the parent thread deliver self.stdin input data (and then close the pipe).On POSIX, this uses poll if available, otherwise select, to accumulate output and deliver stdin input.  All this runs in the (single) parent process/thread.Threads or poll/select are needed here to avoid deadlock.  Suppose, for instance, that we've redirected all three streams to three separate pipes.  Suppose further that there's a small limit on how much data can be stuffed into to a pipe before the writing process is suspended, waiting for the reading process to "clean out" the pipe from the other end.  Let's set that small limit to a single byte, just for illustration.  (This is in fact how things work, except that the limit is much bigger than one byte.)If the parent (Python) process tries to write several bytes—say, 'go\n'to proc.stdin, the first byte goes in and then the second causes the Python process to suspend, waiting for the subprocess to read the first byte, emptying the pipe.Meanwhile, suppose the subprocess decides to print a friendly "Hello! Don't Panic!" greeting.  The H goes into its stdout pipe, but the e causes it to suspend, waiting for its parent to read that H, emptying the stdout pipe.Now we're stuck: the Python process is asleep, waiting to finish saying "go", and the subprocess is also asleep, waiting to finish saying "Hello! Don't Panic!".The subprocess.Popen code avoids this problem with threading-or-select/poll.  When bytes can go over the pipes, they go.  When they can't, only a thread (not the whole process) has to sleep—or, in the case of select/poll, the Python process waits simultaneously for "can write" or "data available", writes to the process's stdin only when there is room, and reads its stdout and/or stderr only when data are ready.  The proc.communicate() code (actually _communicate where the hairy cases are handled) returns once all stdin data (if any) have been sent and all stdout and/or stderr data have been accumulated.If you want to read both stdout and stderr on two different pipes (regardless of any stdin redirection), you will need to avoid deadlock too.  The deadlock scenario here is different—it occurs when the subprocess writes something long to stderr while you're pulling data from stdout, or vice versa—but it's still there.I promised to demonstrate that, un-redirected, Python subprocesses write to the underlying stdout, not sys.stdout.  So, here is some code:When run:Note that the first routine will fail if you add stdout=sys.stdout, as a StringIO object has no fileno.  The second will omit the hello if you add stdout=sys.stdout since sys.stdout has been redirected to os.devnull.(If you redirect Python's file-descriptor-1, the subprocess will follow that redirection.  The open(os.devnull, 'w') call produces a stream whose fileno() is greater than 2.)We can also use the default file iterator for reading stdout instead of using iter construct with readline(). If you're able to use third-party libraries, You might be able to use something like sarge (disclosure: I'm its maintainer). This library allows non-blocking access to output streams from subprocesses - it's layered over the subprocess module.A good but "heavyweight" solution is to use Twisted - see the bottom.If you're willing to live with only stdout something along those lines should work:(If you use read() it tries to read the entire "file" which isn't useful, what we really could use here is something that reads all the data that's in the pipe right now)One might also try to approach this with threading, e.g.:Now we could potentially add stderr as well by having two threads.Note however the subprocess docs discourage using these files directly and recommends to use communicate() (mostly concerned with deadlocks which I think isn't an issue above) and the solutions are a little klunky so it really seems like the subprocess module isn't quite up to the job (also see: http://www.python.org/dev/peps/pep-3145/ ) and we need to look at something else.A more involved solution is to use Twisted as shown here: https://twistedmatrix.com/documents/11.1.0/core/howto/process.htmlThe way you do this with Twisted is to create your process using reactor.spawnprocess() and providing a ProcessProtocol that then processes output asynchronously.  The Twisted sample Python code is here: https://twistedmatrix.com/documents/11.1.0/core/howto/listings/process/process.pyIt looks like line-buffered output will work for you, in which case something like the following might suit. (Caveat: it's untested.)  This will only give the subprocess's stdout in real time.  If you want to have both stderr and stdout in real time, you'll have to do something more complex with select.Why not set stdout directly to sys.stdout? And if you need to output to a log as well, then you can simply override the write method of f.All of the above solutions I tried failed either to separate stderr and stdout output, (multiple pipes) or blocked forever when the OS pipe buffer was full which happens when the command you are running outputs too fast (there is a warning for this on python poll() manual of subprocess). The only reliable way I found was through select, but this is a posix-only solution:In addition to all these answer, one simple approach could also be as follows:Loop through the readable stream as long as it's readable and if it gets an empty result, stop it.The key here is that readline() returns a line (with \n at the end) as long as there's an output and empty if it's really at the end.Hope this helps someone.Similar to previous answers but the following solution worked for for me on windows using Python3 to provide a common method to print and log in realtime (getting-realtime-output-using-python):I think that the subprocess.communicate method is a bit misleading: it actually fills the stdout and stderr that you specify in the subprocess.Popen.Yet, reading from the subprocess.PIPE that you can provide to the subprocess.Popen's stdout and stderr parameters will eventually fill up OS pipe buffers and deadlock your app (especially if you've multiple processes/threads that must use subprocess).My proposed solution is to provide the stdout and stderr with files - and read the files' content instead of reading from the deadlocking PIPE. These files can be tempfile.NamedTemporaryFile() - which can also be accessed for reading while they're being written into by subprocess.communicate. Below is a sample usage:And this is the source code which is ready to be used with as many comments as I could provide to explain what it does:If you're using python 2, please make sure to first install the latest version of the subprocess32 package from pypi.Solution 1: Log stdout AND stderr concurrently in realtimeA simple solution which logs both stdout AND stderr concurrently in realtime.Solution 2: Create an iterator that returns stdout and stderr line by line, concurrently in realtimeA function read_popen_pipes() that allows you to iterate over both pipes (stdout/stderr), concurrently in realtime:Here is a class which I'm using in one of my projects. It redirects output of a subprocess to the log. At first I tried simply overwriting the write-method but that doesn't work as the subprocess will never call it (redirection happens on filedescriptor level). So I'm using my own pipe, similar to how it's done in the subprocess-module. This has the advantage of encapsulating all logging/printing logic in the adapter and you can simply pass instances of the logger to Popen: subprocess.Popen("/path/to/binary", stderr = LogAdapter("foo"))If you don't need logging but simply want to use print() you can obviously remove large portions of the code and keep the class shorter. You could also expand it by an __enter__ and __exit__ method and call finished in __exit__ so that you could easily use it as context.Based on all the above I suggest a slightly modified version (python3):Code:None of the Pythonic solutions worked for me.

It turned out that proc.stdout.read() or similar may block forever.Therefore, I use tee like this:This solution is convenient if you are already using shell=True.${PIPESTATUS} captures the success status of the entire command chain (only available in Bash).

If I omitted the && exit ${PIPESTATUS}, then this would always return zero since tee never fails.unbuffer might be necessary for printing each line immediately into the terminal, instead of waiting way too long until the "pipe buffer" gets filled.

However, unbuffer swallows the exit status of assert (SIG Abort)...2>&1 also logs stderror to the file.

Multiple levels of 'collection.defaultdict' in Python

Morlock

[Multiple levels of 'collection.defaultdict' in Python](https://stackoverflow.com/questions/2600790/multiple-levels-of-collection-defaultdict-in-python)

Thanks to some great folks on SO, I discovered the possibilities offered by collections.defaultdict, notably in readability and speed. I have put them to use with success.Now I would like to implement three levels of dictionaries, the two top ones being defaultdict and the lowest one being int. I don't find the appropriate way to do this. Here is my attempt:Now this works, but the following, which is the desired behavior, doesn't:I suspect that I should have declared somewhere that the second level defaultdict is of type int, but I didn't find where or how to do so.The reason I am using defaultdict in the first place is to avoid having to initialize the dictionary for each new key.Any more elegant suggestion?Thanks pythoneers!

2010-04-08 14:37:36Z

Thanks to some great folks on SO, I discovered the possibilities offered by collections.defaultdict, notably in readability and speed. I have put them to use with success.Now I would like to implement three levels of dictionaries, the two top ones being defaultdict and the lowest one being int. I don't find the appropriate way to do this. Here is my attempt:Now this works, but the following, which is the desired behavior, doesn't:I suspect that I should have declared somewhere that the second level defaultdict is of type int, but I didn't find where or how to do so.The reason I am using defaultdict in the first place is to avoid having to initialize the dictionary for each new key.Any more elegant suggestion?Thanks pythoneers!Use:This will create a new defaultdict(int) whenever a new key is accessed in d.Another way to make a pickleable, nested defaultdict is to use a partial object instead of a lambda:This will work because the defaultdict class is globally accessible at the module level:Look at nosklo's answer here for a more general solution.Testing:Output:As per @rschwieb's request for D['key'] += 1, we can expand on previous by overriding addition by defining __add__ method, to make this behave more like a collections.Counter()First __missing__ will be called to create a new empty value, which will be passed into __add__.  We test the value, counting on empty values to be False.See emulating numeric types for more information on overriding. Examples:Rather than checking argument is a Number (very non-python, amirite!) we could just provide a default 0 value and then attempt the operation:Late to the party, but for arbitrary depth I just found myself doing something like this:The trick here is basically to make the DeepDict instance itself a valid factory for constructing missing values. Now we can do things likeNo errors again. 

No matter how many levels  nested.

pop  no error alsodd=DefaultDict({"1":333333})

Binary search (bisection) in Python

rslite

[Binary search (bisection) in Python](https://stackoverflow.com/questions/212358/binary-search-bisection-in-python)

Is there a library function that performs binary search on a list/tuple and return the position of the item if found and 'False' (-1, None, etc.) if not?I found the functions bisect_left/right in the bisect module, but they still return a position even if the item is not in the list. That's perfectly fine for their intended usage, but I just want to know if an item is in the list or not (don't want to insert anything).I thought of using bisect_left and then checking if the item at that position is equal to what I'm searching, but that seems cumbersome (and I also need to do bounds checking if the number can be larger than the largest number in my list). If there is a nicer method I'd like to know about it.Edit To clarify what I need this for: I'm aware that a dictionary would be very well suited for this, but I'm trying to keep the memory consumption as low as possible. My intended usage would be a sort of double-way look-up table. I have in the table a list of values and I need to be able to access the values based on their index. And also I want to be able to find the index of a particular value or None if the value is not in the list.Using a dictionary for this would be the fastest way, but would (approximately) double the memory requirements.I was asking this question thinking that I may have overlooked something in the Python libraries. It seems I'll have to write my own code, as Moe suggested.

2008-10-17 14:23:17Z

Is there a library function that performs binary search on a list/tuple and return the position of the item if found and 'False' (-1, None, etc.) if not?I found the functions bisect_left/right in the bisect module, but they still return a position even if the item is not in the list. That's perfectly fine for their intended usage, but I just want to know if an item is in the list or not (don't want to insert anything).I thought of using bisect_left and then checking if the item at that position is equal to what I'm searching, but that seems cumbersome (and I also need to do bounds checking if the number can be larger than the largest number in my list). If there is a nicer method I'd like to know about it.Edit To clarify what I need this for: I'm aware that a dictionary would be very well suited for this, but I'm trying to keep the memory consumption as low as possible. My intended usage would be a sort of double-way look-up table. I have in the table a list of values and I need to be able to access the values based on their index. And also I want to be able to find the index of a particular value or None if the value is not in the list.Using a dictionary for this would be the fastest way, but would (approximately) double the memory requirements.I was asking this question thinking that I may have overlooked something in the Python libraries. It seems I'll have to write my own code, as Moe suggested.Why not look at the code for bisect_left/right and adapt it to suit your purpose.like this:This is a little off-topic (since Moe's answer seems complete to the OP's question), but it might be worth looking at the complexity for your whole procedure from end to end.  If you're storing thing in a sorted lists (which is where a binary search would help), and then just checking for existence, you're incurring (worst-case, unless specified):Sorted ListsWhereas with a set(), you're incurringThe thing a sorted list really gets you are "next", "previous", and "ranges" (including inserting or deleting ranges), which are O(1) or O(|range|), given a starting index.  If you aren't using those sorts of operations often, then storing as sets, and sorting for display might be a better deal overall.  set() incurs very little additional overhead in python.  It might be worth mentioning that the bisect docs now provide searching examples:

http://docs.python.org/library/bisect.html#searching-sorted-lists(Raising ValueError instead of returning -1 or None is more pythonic – list.index() does it, for example. But of course you can adapt the examples to your needs.)Simplest is to use bisect and check one position back to see if the item is there:This is right from the manual:http://docs.python.org/2/library/bisect.html8.5.1. Searching Sorted ListsThe above bisect() functions are useful for finding insertion points but can be tricky or awkward to use for common searching tasks. The following five functions show how to transform them into the standard lookups for sorted lists:So with the slight modification your code should be:I agree that @DaveAbrahams's answer using the bisect module is the correct approach.  He did not mention one important detail in his answer.From the docs  bisect.bisect_left(a, x, lo=0, hi=len(a))The bisection module does not require the search array to be precomputed ahead of time.  You can just present the endpoints to the bisect.bisect_left instead of it using the defaults of 0 and len(a).Even more important for my use, looking for a value X such that the error of a given function is minimized.  To do that, I needed a way to have the bisect_left's algorithm call my computation instead.  This is really simple.Just provide an object that defines __getitem__ as aFor example, we could use the bisect algorithm to find a square root with arbitrary precision!If you just want to see if it's present, try turning the list into a dict:On my machine, "if n in l" took 37 seconds, while "if n in d" took 0.4 seconds.This one is:Dave Abrahams' solution is good. Although I have would have done it minimalistic:While there's no explicit binary search algorithm in Python, there is a module - bisect - designed to find the insertion point for an element in a sorted list using a binary search. This can be "tricked" into performing a binary search. The biggest advantage of this is the same advantage most library code has - it's high-performing, well-tested and just works (binary searches in particular can be quite difficult to implement successfully - particularly if edge cases aren't carefully considered).For basic types like Strings or ints it's pretty easy - all you need is the bisect module and a sorted list:You can also use this to find duplicates:Obviously you could just return the index rather than the value at that index if desired.For custom types or objects, things are a little bit trickier: you have to make sure to implement rich comparison methods to get bisect to compare correctly.This should work in at least Python 2.7 -> 3.3Using a dict wouldn't like double your memory usage unless the objects you're storing are really tiny, since the values are only pointers to the actual objects:In that example, 'foo' is only stored once.  Does that make a difference for you?  And exactly how many items are we talking about anyway?This code works with integer lists in a recursive way. Looks for the simplest case scenario, which is: list length less than 2. It means the answer is already there and  a test is performed to check for the correct answer.

If not, a middle value is set and tested to be the correct, if not bisection is performed by calling again the function, but setting middle value as the upper or lower limit, by shifting it to the left or rightCheck out the examples on Wikipedia http://en.wikipedia.org/wiki/Binary_search_algorithmI guess this is much better and effective. please correct me :) . Thank youBinary Search :    // To call above function use :I needed binary search in python and generic for Django models. In Django models, one model can have foreign key to another model and I wanted to perform some search on the retrieved models objects. I wrote following function you can use this.Many good solutions above but I haven't seen a simple (KISS keep it simple (cause  I'm) stupid use of the Python built in/generic bisect function to do a binary search.  With a bit of code around the bisect function, I think I have an example below where I have tested all cases for a small string array of names. Some of the above solutions allude to/say this, but hopefully the simple code below will help anyone confused like I was.  Python bisect is used to indicate where to insert an a new value/search item into a sorted list. The below code which uses bisect_left which will return the index of the hit if the search item in the list/array is found (Note bisect and bisect_right will return the index of the element after the hit or match as the insertion point) If not found, bisect_left will return an index to the next item in the sorted list which will not == the search value.  The only other case is where the search item would go at the end of the list where the index returned would be beyond the end of the list/array, and which in the code below the early exit by Python with "and" logic handles. (first condition False Python does not check subsequent conditions)

How to find a thread id in Python

Charles Anderson

[How to find a thread id in Python](https://stackoverflow.com/questions/919897/how-to-find-a-thread-id-in-python)

I have a multi-threading Python program, and a utility function, writeLog(message), that writes out a timestamp followed by the message. Unfortunately, the resultant log file gives no indication of which thread is generating which message. I would like writeLog() to be able to add something to the message to identify which thread is calling it. Obviously I could just make the threads pass this information in, but that would be a lot more work. Is there some thread equivalent of os.getpid() that I could use?

2009-05-28 09:04:24Z

I have a multi-threading Python program, and a utility function, writeLog(message), that writes out a timestamp followed by the message. Unfortunately, the resultant log file gives no indication of which thread is generating which message. I would like writeLog() to be able to add something to the message to identify which thread is calling it. Obviously I could just make the threads pass this information in, but that would be a lot more work. Is there some thread equivalent of os.getpid() that I could use?threading.get_ident() works, or threading.current_thread().ident (or threading.currentThread().ident for Python < 2.6).Using the logging module you can automatically add the current thread identifier in each log entry. 

Just use one of these LogRecord mapping keys in your logger format string:and set up your default handler with it:The thread.get_ident() function returns a long integer on Linux. It's not really a thread id.I use this method to really get the thread id on Linux:This functionality is now supported by Python 3.8+ :)https://github.com/python/cpython/commit/4959c33d2555b89b494c678d99be81a65ee864b0https://github.com/python/cpython/pull/11993I saw examples of thread IDs like this:The threading module docs lists name attribute as well:You can get the ident of the current running thread. The ident could be reused for other threads, if the current thread ends.When you crate an instance of Thread, a name is given implicit to the thread, which is the pattern: Thread-numberThe name has no meaning and the name don't have to be unique. The ident of all running threads is unique.The function threading.current_thread() returns the current running thread. This object holds the whole information of the thread.I created multiple threads in Python, I printed the thread objects, and I printed the id using the ident variable. I see all the ids are same:Similarly to @brucexin I needed to get OS-level thread identifier (which != thread.get_ident()) and use something like below not to depend on particular numbers and being amd64-only:andthis depends on Cython though.

Keep only date part when using pandas.to_datetime

EdChum - Reinstate Monica

[Keep only date part when using pandas.to_datetime](https://stackoverflow.com/questions/16176996/keep-only-date-part-when-using-pandas-to-datetime)

I use pandas.to_datetime to parse the dates in my data. Pandas by default represents the dates with datetime64[ns] even though the dates are all daily only.

I wonder whether there is an elegant/clever way to convert the dates to datetime.date or datetime64[D] so that, when I write the data to CSV, the dates are not appended with 00:00:00. I know I can convert the type manually element-by-element:But this is really slow since I have many rows and it sort of defeats the purpose of using pandas.to_datetime. Is there a way to convert the dtype of the entire column at once? Or alternatively, does pandas.to_datetime support a precision specification so that I can get rid of the time part while working with daily data?

2013-04-23 18:50:36Z

I use pandas.to_datetime to parse the dates in my data. Pandas by default represents the dates with datetime64[ns] even though the dates are all daily only.

I wonder whether there is an elegant/clever way to convert the dates to datetime.date or datetime64[D] so that, when I write the data to CSV, the dates are not appended with 00:00:00. I know I can convert the type manually element-by-element:But this is really slow since I have many rows and it sort of defeats the purpose of using pandas.to_datetime. Is there a way to convert the dtype of the entire column at once? Or alternatively, does pandas.to_datetime support a precision specification so that I can get rid of the time part while working with daily data?Since version 0.15.0 this can now be easily done using .dt to access just the date component:The above returns a datetime.date dtype, if you want to have a datetime64 then you can just normalize the time component to midnight so it sets all the values to 00:00:00:This keeps the dtype as datetime64 but the display shows just the date value.While I upvoted EdChum's answer, which is the most direct answer to the question the OP posed, it does not really solve the performance problem (it still relies on python datetime objects, and hence any operation on them will be not vectorized - that is, it will be slow).A better performing alternative is to use df['dates'].dt.floor('d'). Strictly speaking, it does not "keep only date part", since it just sets the time to 00:00:00. But it does work as desired by the OP when, for instance:... and it is much more efficient, since the operation is vectorized.EDIT: in fact, the answer the OP's would have preferred is probably "recent versions of pandas do not write the time to csv if it is 00:00:00 for all observations".Simple Solution:Pandas DatetimeIndex and Series have a method called normalize that does exactly what you want.You can read more about it in this answer.It can be used as ser.dt.normalize()Avoid, where possible, converting your datetime64[ns] series to an object dtype series of datetime.date objects. The latter, often constructed using pd.Series.dt.date, is stored as an array of pointers and is inefficient relative to a pure NumPy-based series.Since your concern is format when writing to CSV, just use the date_format parameter of to_csv. For example:See Python's strftime directives for formatting conventions.This is a simple way to extract the date:Converting to datetime64[D]:Though re-assigning that to a DataFrame col will revert it back to [ns].If you wanted actual datetime.date:Just giving a more up to date answer in case someone sees this old post. Adding "utc=False" when converting to datetime will remove the timezone component and keep only the date in a datetime64[ns] data type. You will be able to save it in excel without getting the error "ValueError: Excel does not support datetimes with timezones. Please ensure that datetimes are timezone unaware before writing to Excel."

Generating matplotlib graphs without a running X server [duplicate]

Jonathan

[Generating matplotlib graphs without a running X server [duplicate]](https://stackoverflow.com/questions/4931376/generating-matplotlib-graphs-without-a-running-x-server)

Matplotlib seems to require the $DISPLAY environment variable which means a running X server.Some web hosting services do not allow a running X server session.Is there a way to generate graphs using matplotlib without a running X server?

2011-02-08 09:29:38Z

Matplotlib seems to require the $DISPLAY environment variable which means a running X server.Some web hosting services do not allow a running X server session.Is there a way to generate graphs using matplotlib without a running X server?@Neil's answer is one (perfectly valid!) way of doing it, but you can also simply call matplotlib.use('Agg') before importing matplotlib.pyplot, and then continue as normal.  E.g.You don't have to use the Agg backend, as well.  The pdf, ps, svg, agg, cairo, and gdk backends can all be used without an X-server.  However, only the Agg backend will be built by default (I think?), so there's a good chance that the other backends may not be enabled on your particular install.Alternately, you can just set the backend parameter in your .matplotlibrc file to automatically have matplotlib.pyplot use the given renderer.You need to use the matplotlib API directly rather than going through the pylab interface. There's a good example here:http://www.dalkescientific.com/writings/diary/archive/2005/04/23/matplotlib_without_gui.html

How do I check if a string is valid JSON in Python?

Joey Blake

[How do I check if a string is valid JSON in Python?](https://stackoverflow.com/questions/5508509/how-do-i-check-if-a-string-is-valid-json-in-python)

In Python, is there a way to check if a string is valid JSON before trying to parse it?  For example working with things like the Facebook Graph API, sometimes it returns JSON, sometimes it could return an image file.

2011-04-01 02:16:38Z

In Python, is there a way to check if a string is valid JSON before trying to parse it?  For example working with things like the Facebook Graph API, sometimes it returns JSON, sometimes it could return an image file.You can try to do json.loads(), which will throw a ValueError if the string you pass can't be decoded as JSON.In general, the "Pythonic" philosophy for this kind of situation is called EAFP, for Easier to Ask for Forgiveness than Permission.Which prints:Convert a JSON string to a Python dictionary:Convert a python object to JSON string:If you want access to low-level parsing, don't roll your own, use an existing library: http://www.json.org/Great tutorial on python JSON module:  https://pymotw.com/2/json/Prints: json_xs is capable of syntax checking, parsing, prittifying, encoding, decoding and more:https://metacpan.org/pod/json_xsI would say parsing it is the only way you can really entirely tell. Exception will be raised by python's json.loads() function (almost certainly) if not the correct format. However, the the purposes of your example you can probably just check the first couple of non-whitespace characters...I'm not familiar with the JSON that facebook sends back, but most JSON strings from web apps will start with a open square [ or curly { bracket. No images formats I know of start with those characters. Conversely if you know what image formats might show up, you can check the start of the string for their signatures to identify images, and assume you have JSON if it's not an image. Another simple hack to identify a graphic, rather than a text string, in the case you're looking for a graphic, is just to test for non-ASCII characters in the first couple of dozen characters of the string (assuming the JSON is ASCII).I came up with an generic, interesting solution to this problem:and you can use it like so:

Explain Python entry points?

Brad Wright

[Explain Python entry points?](https://stackoverflow.com/questions/774824/explain-python-entry-points)

I've read the documentation on egg entry points in Pylons and on the Peak pages, and I still don't really understand. Could someone explain them to me?

2009-04-21 21:59:55Z

I've read the documentation on egg entry points in Pylons and on the Peak pages, and I still don't really understand. Could someone explain them to me?An "entry point" is typically a function (or other callable function-like object) that a developer or user of your Python package might want to use, though a non-callable object can be supplied as an entry point as well (as correctly pointed out in the comments!).The most popular kind of entry point is the console_scripts entry point, which points to a function that you want made available as a command-line tool to whoever installs your package.  This goes into your setup.py like:I have a package I've just deployed called "cursive.tools", and I wanted it to make available a "cursive" command that someone could run from the command line, like:The way to do this is define a function, like maybe a "cursive_command" function in cursive/tools/cmd.py that looks like:and so forth; it should assume that it's been called from the command line, parse the arguments that the user has provided, and ... well, do whatever the command is designed to do.Install the docutils package for a great example of entry-point use: it will install something like a half-dozen useful commands for converting Python documentation to other formats.EntryPoints provide a persistent, filesystem-based object name registration and name-based direct object import mechanism (implemented by the setuptools package). They associate names of Python objects with free-form identifiers. So any other code using the same Python installation and knowing the identifier can access an object with the associated name, no matter where the object is defined. The associated names can be any names existing in a Python module; for example name of a class, function or variable. The entry point mechanism does not care what the name refers to, as long as it is importable.As an example, let's use (the name of) a function, and an imaginary python module with a fully-qualified name 'myns.mypkg.mymodule':Entry points are registered via an entry points declaration in setup.py. To register the_function under entrypoint called 'my_ep_func':As the example shows, entry points are grouped; there's corresponding API to look up all entry points belonging to a group (example below).Upon a package installation (ie. running 'python setup.py install'), the above declaration is parsed by setuptools. It then writes the parsed information in special file. After that, the pkg_resources API (part of setuptools) can be used to look up the entry point and access the object(s) with the associated name(s):Here, setuptools read the entry point information that was written in special files. It found the entry point, imported the module (myns.mypkg.mymodule), and retrieved the_function defined there, upon call to pkg_resources.load().Assuming there were no other entry point registrations for the same group id, calling the_function would then be simple:Thus, while perhaps a bit difficult to grasp at first, the entry point mechanism is actually quite simple to use. It provides an useful tool for pluggable Python software development.From abstract point of view, entry points are used to create a system-wide registry of Python callables that implement certain interfaces.  There are APIs in pkg_resources to see which entry points are advertised by a given package as well as APIs to determine which packages advertise a certain entry point.Entry points are useful for allowing one package do use plugins that are in another package.  For instance, Ian Bicking's Paste project uses entry points extensively.  In this case, you can write a package that advertises its WSGI application factory using the entry point paste.app_factory.  Another use for entry points is enumerating all the packages on the system that provide some plugin functionality.  The TurboGears web framework uses the python.templating.engines entry point to look up templating libraries that are installed and available.  

How to check a string for specific characters? [closed]

The Woo

[How to check a string for specific characters? [closed]](https://stackoverflow.com/questions/5188792/how-to-check-a-string-for-specific-characters)

How can I check if a string has several specific characters in it using Python 2?For example, given the following string:How do I detect if it has dollar signs ("$"), commas (","), and numbers?

2011-03-04 01:47:24Z

How can I check if a string has several specific characters in it using Python 2?For example, given the following string:How do I detect if it has dollar signs ("$"), commas (","), and numbers?Assuming your string is s:And so on for other characters.... or... or[Edit: added the '$' in s answers]user Jochen Ritzel said this in a comment to an answer to this question from user dappawit.

It should work:'1', '2', etc. should be replaced with the characters you are looking for.See this page in the Python 2.7 documentation for some information on strings, including about using the in operator for substring tests.Update: This does the same job as my above suggestion with less repetition:Quick comparison of timings in response to the post by Abbafei:Output:So the code is more compact with any, but faster with the conditional.EDIT : TL;DR -- For long strings, if-then is still much faster than any! I decided to compare the timing for a long random string based on some of the valid points raised in the comments:Output:If-then is almost an order of magnitude faster than any!This will test if strings are made up of some combination or digits, the dollar sign, and a commas. Is that what you're looking for?else:

     print("Non Space Special Character")   

How to make a Python script run like a service or daemon in Linux

adhanlon

[How to make a Python script run like a service or daemon in Linux](https://stackoverflow.com/questions/1603109/how-to-make-a-python-script-run-like-a-service-or-daemon-in-linux)

I have written a Python script that checks a certain e-mail address and passes new e-mails to an external program.  How can I get this script to execute 24/7, such as turning it into daemon or service in Linux.  Would I also need a loop that never ends in the program, or can it be done by just having the code re executed multiple times?

2009-10-21 19:36:34Z

I have written a Python script that checks a certain e-mail address and passes new e-mails to an external program.  How can I get this script to execute 24/7, such as turning it into daemon or service in Linux.  Would I also need a loop that never ends in the program, or can it be done by just having the code re executed multiple times?You have two options here.I wouldn't recommend you to choose 2., because you would be, in fact, repeating cron functionality.  The Linux system paradigm is to let multiple simple tools interact and solve your problems.  Unless there are additional reasons why you should make a daemon (in addition to trigger periodically), choose the other approach.Also, if you use daemonize with a loop and a crash happens, no one will check the mail after that (as pointed out by Ivan Nevostruev in comments to this answer).  While if the script is added as a cron job, it will just trigger again.Here's a nice class that is taken from here:  You should use the python-daemon library, it takes care of everything.From PyPI: Library to implement a well-behaved Unix daemon process.You can use fork() to detach your script from the tty and have it continue to run, like so:Of course you also need to implement an endless loop, likeHope this get's you started.You can also make the python script run as a service using a shell script. First create a shell script to run the python script like this  (scriptname arbitary name)now make a file in /etc/init.d/scriptname Now you can start and stop your python script using the command /etc/init.d/scriptname start or stop.  cron is clearly a great choice for many purposes.  However it doesn't create a service or daemon as you requested in the OP.  cron just runs jobs periodically (meaning the job starts and stops), and no more often than once / minute.  There are issues with cron -- for example, if a prior instance of your script is still running the next time the cron schedule comes around and launches a new instance, is that OK?  cron doesn't handle dependencies; it just tries to start a job when the schedule says to.If you find a situation where you truly need a daemon (a process that never stops running), take a look at supervisord.  It provides a simple way to wrapper a normal, non-daemonized script or program and make it operate like a daemon.  This is a much better way than creating a native Python daemon.A simple and supported version is Daemonize.Install it from Python Package Index (PyPI):and then use like:how about using $nohup command on linux?I use it for running my commands on my Bluehost server.Please advice if I am wrong.If you are using terminal(ssh or something) and you want to keep a long-time script working after you log out from the terminal, you can try this:screenapt-get install screencreate a virtual terminal inside( namely abc): screen -dmS abcnow we connect to abc: screen -r abcSo, now we can run python script: python Keep_sending_mail.pyfrom now on, you can directly close your terminal, however, the python script will keep running rather than being shut downIf you want to go back check your script running status, you can use screen -r abc againFirst, read up on mail aliases.  A mail alias will do this inside the mail system without you having to fool around with daemons or services or anything of the sort.You can write a simple script that will be executed by sendmail each time a mail message is sent to a specific mailbox.See http://www.feep.net/sendmail/tutorial/intro/aliases.htmlIf you really want to write a needlessly complex server, you can do this.That's all it takes.  Your script simply loops and sleeps.I would recommend this solution. You need to inherit and override method run.to creating some thing that is running like service you can use this thing :The first thing that you must do is installing the Cement framework:

Cement frame work is a CLI frame work that you can deploy your application on it.command line interface of the app :interface.pyYourApp.py class:Keep in mind that your app must run on a thread to be daemonTo run the app just do this in command line Use whatever service manager your system offers - for example under Ubuntu use upstart. This will handle all the details for you such as start on boot, restart on crash, etc.

Getting list of lists into pandas DataFrame

Joop

[Getting list of lists into pandas DataFrame](https://stackoverflow.com/questions/19112398/getting-list-of-lists-into-pandas-dataframe)

I am reading contents of a spreadsheet into pandas.   DataNitro has a method that returns a rectangular selection of cells as a list of lists.   SogivesI am busy writing code to translate this, but my guess is that it is such a simple use that there must be method to do this.    Cant seem to find it in documentation.   Any pointers to the method that would simplify this?

2013-10-01 09:19:40Z

I am reading contents of a spreadsheet into pandas.   DataNitro has a method that returns a rectangular selection of cells as a list of lists.   SogivesI am busy writing code to translate this, but my guess is that it is such a simple use that there must be method to do this.    Cant seem to find it in documentation.   Any pointers to the method that would simplify this?Call the pd.DataFrame constructor directly:With approach explained by EdChum above, the values in the list are shown as rows. To show the values of lists as columns in DataFrame instead, simply use transpose() as following:The output then is:Even without pop the list we can do with set_index Update from_records

Which is better in python, del or delattr?

pydanny

[Which is better in python, del or delattr?](https://stackoverflow.com/questions/1120927/which-is-better-in-python-del-or-delattr)

This may be silly, but it's been nagging the back of my brain for a while. Python gives us two built-in ways to delete attributes from objects, the del command word and the delattr built-in function. I prefer delattr because it I think its a bit more explicit:But I'm wondering if there might be under-the-hood differences between them.

2009-07-13 17:33:33Z

This may be silly, but it's been nagging the back of my brain for a while. Python gives us two built-in ways to delete attributes from objects, the del command word and the delattr built-in function. I prefer delattr because it I think its a bit more explicit:But I'm wondering if there might be under-the-hood differences between them.The first is more efficient than the second.  del foo.bar compiles to two bytecode instructions:whereas delattr(foo, "bar") takes five:This translates into the first running slightly faster (but it's not a huge difference – .15 μs on my machine).Like the others have said, you should really only use the second form when the attribute that you're deleting is determined dynamically.[Edited to show the bytecode instructions generated inside a function, where the compiler can use LOAD_FAST and LOAD_GLOBAL]Consider the following examples:or:You can't do it with del.Unquestionably the former.  In my view this is like asking whether foo.bar is better than getattr(foo, "bar"), and I don't think anyone is asking that question :)It's really a matter of preference, but the first is probably preferable.  I'd only use the second one if you don't know the name of the attribute that you're deleting ahead of time.Just like getattr and setattr, delattr should only be used when the attribute name is unknown.In that sense, it's roughly equivalent to several python features that are used to access built-in functionality at a lower level than you normally have available, such as __import__ instead of import and operator.add instead of +Not sure about the inner workings, but from a code reusability and don't be a jerk coworker perspective, use del. It's more clear and understood by people coming from other languages as well.If you think delattr is more explicit, then why not used getattr all the time rather than object.attr?As for under the hood... your guess is as good as mine. If not significantly better.It is an old question, but I would like to put my 2 cents in. Though, del foo.bar is more elegant, at times you will need delattr(foo, "bar"). Say, if you have an interactive command line interface that allows a user to dynamically delete any member in the object by typing the name, then you have no choice but to use the latter form.

Is there a math nCr function in python? [duplicate]

James Mertz

[Is there a math nCr function in python? [duplicate]](https://stackoverflow.com/questions/4941753/is-there-a-math-ncr-function-in-python)

I'm looking to see if built in with the math library in python is the nCr (n Choose r) function: I understand that this can be programmed but I thought that I'd check to see if it's already built in before I do.  

2011-02-09 05:51:13Z

I'm looking to see if built in with the math library in python is the nCr (n Choose r) function: I understand that this can be programmed but I thought that I'd check to see if it's already built in before I do.  The following program calculates nCr in an efficient manner (compared to calculating factorials etc.)Do you want iteration? itertools.combinations.  Common usage:If you just need to compute the formula, use math.factorial:In Python 3, use the integer division // instead of / to avoid overflows:return f(n) // f(r) // f(n-r)

BaseException.message deprecated in Python 2.6

desolat

[BaseException.message deprecated in Python 2.6](https://stackoverflow.com/questions/1272138/baseexception-message-deprecated-in-python-2-6)

I get a warning that BaseException.message is deprecated in Python 2.6 when I use the following user-defined exception:This is the warning:What's wrong with this? What do I have to change to get rid of the deprecation warning?

2009-08-13 13:59:21Z

I get a warning that BaseException.message is deprecated in Python 2.6 when I use the following user-defined exception:This is the warning:What's wrong with this? What do I have to change to get rid of the deprecation warning?Just inherit your exception class from Exception and pass the message as the first parameter to the constructorExample:You can use str(my) or (less elegant) my.args[0] to access the custom message.In the newer versions of Python (from 2.6) we are supposed to inherit our custom exception classes from Exception which (starting from Python 2.5) inherits from BaseException. The background is described in detail in PEP 352.__str__ and __repr__ are already implemented in a meaningful way, 

especially for the case of only one arg (that can be used as message).You do not need to repeat __str__ or __init__ implementation or create _get_message as suggested by others.Yes, it's deprecated in Python 2.6 because it's going away in Python 3.0BaseException class does not provide a way to store error message anymore. You'll have to implement it yourself. You can do this with a subclass that uses a property for storing the message.Hope this helpsThis is your class in Python2.6 style. The new exception takes an arbitrary number of arguments.Let me clarify the problem, as one cannot replicate this with the question's sample code, this will replicate the warning in Python 2.6 and 2.7, if you have warnings turned on (via the -W flag, the PYTHONWARNINGS environment variable, or the warnings module):I prefer repr(error), which returns a string that contains the name of the error type, the repr of the message, if there is one, and the repr of the remaining arguments.And the way you get rid of the DeprecationWarning is to subclass a builtin exception as the Python designers intended:If you know there was one argument, a message, to the Exception and that's what you want, it is preferable to avoid the message attribute and just take the str of the error. Say for a subclassed Exception:And usage:See also this answer:Proper way to declare custom exceptions in modern Python?As far as I can tell, simply using a different name for the message attribute avoids the conflict with the base class, and thus stops the deprecation warning:Seems like a hack to me.Maybe someone can explain why the warning is issued even when the subclass defines a message attribute explicitly. If the base class no longer has this attribute, there shouldn't be a problem.Continuing on from geekQ's answer, the preferred code replacement depends on what you need to do:Sometimes exceptions have more than one argument, so my.args[0] is not guaranteed to provide all the relevant information.For instance:Prints as output:However it's a context sensitive trade off, because for instance:The advice to use str(myexception) leads to unicode problems in python 2.7, e.g.::(works as expected, and is preferred in cases where some of the content of the error string includes user inputpzrq's post says to use:This was exactly what I needed.(If you are in a unicode environment, it appears that:will work, and it appears to work fine in a non-unicode environment)Pzrq said a lot of other good stuff, but I almost missed their answer due to all the good stuff.  Since I don't have 50 points I cannot comment on their answer to attempt to draw attention to the simple solution that works, and since I don't have 15 I cannot vote that answer up, but I can post (feels backward, but oh well) - so here I am posting - probably lose points for that...Since my point is to draw attention to pzrq's answer, please don't glaze over and miss it in all the below.  the first few lines of this post are the most important.My story:The problem I came here for was if you want to catch an exception from a class that you have no control over - what then???  I'm certainly not going to subclass all possible classes my code uses in an attempt to be able to get a message out of all possible exceptions!I was using:which, as we all now know, gives the warning OP asked about (which brought me here), and this, which pzrq gives as a way to do it:did not. I'm not in a unicode environment, but jjc's answer made me wonder, so I had to try it. In this context this becomes:which, to my surprise, worked exactly like str(e) - so now that's what I'm using.Don't know if 'str(e)/unicode(e)' is the 'approved Python way', and I'll probably find out why that's not good when I get to 3.0, but one hopes that the ability to handle an unexpected exception (*) without dying and still get some information from it won't ever go away...(*) Hmm.  "unexpected exception" - I think I just stuttered!

Django: Redirect to previous page after login

jörg

[Django: Redirect to previous page after login](https://stackoverflow.com/questions/806835/django-redirect-to-previous-page-after-login)

I'm trying to build a simple website with login functionality very similar to the one here on SO.

The user should be able to browse the site as an anonymous user and there will be a login link on every page. When clicking on the login link the user will be taken to the login form. After a successful login the user should be taken back to the page from where he clicked the login link in the first place.

I'm guessing that I have to somehow pass the url of the current page to the view that handles the login form but I can't really get it to work.EDIT:

I figured it out. I linked to the login form by passing the current page as a GET parameter and then used 'next' to redirect to that page. Thanks!EDIT 2:

My explanation did not seem to be clear so as requested here is my code:

Lets say we are on a page foo.html and we are not logged in. Now we would like to have a link on foo.html that links to login.html. There we can login and are then redirected back to foo.html.

The link on foo.html looks like this:Now I wrote a custom login view that looks somewhat like this:And the important line in login.html:So yeah thats pretty much it, hope that makes it clear.

2009-04-30 13:14:14Z

I'm trying to build a simple website with login functionality very similar to the one here on SO.

The user should be able to browse the site as an anonymous user and there will be a login link on every page. When clicking on the login link the user will be taken to the login form. After a successful login the user should be taken back to the page from where he clicked the login link in the first place.

I'm guessing that I have to somehow pass the url of the current page to the view that handles the login form but I can't really get it to work.EDIT:

I figured it out. I linked to the login form by passing the current page as a GET parameter and then used 'next' to redirect to that page. Thanks!EDIT 2:

My explanation did not seem to be clear so as requested here is my code:

Lets say we are on a page foo.html and we are not logged in. Now we would like to have a link on foo.html that links to login.html. There we can login and are then redirected back to foo.html.

The link on foo.html looks like this:Now I wrote a custom login view that looks somewhat like this:And the important line in login.html:So yeah thats pretty much it, hope that makes it clear.You do not need to make an extra view for this, the functionality is already built in.First each page with a login link needs to know the current path, and the easiest way is to add the request context preprosessor to settings.py (the 4 first are default), then the request object will be available in each request:settings.py:Then add in the template you want the Login link:base.html:This will add a GET argument to the login page that points back to the current page.The login template can then be as simple as this:registration/login.html:This may not be a "best practice", but I've successfully used this before:To support full urls with param/values you'd need:instead of just:Django's built-in authentication works the way you want.Their login pages include a next query string which is the page to return to after login.Look at http://docs.djangoproject.com/en/dev/topics/auth/#django.contrib.auth.decorators.login_required I linked to the login form by passing the current page as a GET parameter and then used 'next' to redirect to that page. Thanks!I encountered the same problem. This solution allows me to keep using the generic login view:In registration/login.html (nested within templates folder) if you insert the following line, the page will render like Django's original admin login page:Note: The file should contain above lines only.See django docs for views.login(), you supply a 'next' value (as a hidden field) on the input form to redirect to after a successful login.You can also do this

Python memory leaks [closed]

Fragsworth

[Python memory leaks [closed]](https://stackoverflow.com/questions/1435415/python-memory-leaks)

I have a long-running script which, if let to run long enough, will consume all the memory on my system.Without going into details about the script, I have two questions:

2009-09-16 20:56:04Z

I have a long-running script which, if let to run long enough, will consume all the memory on my system.Without going into details about the script, I have two questions:Have a look at this article: Tracing python memory leaksAlso, note that the garbage collection module actually can have debug flags set. Look at the set_debug function. Additionally, look at this code by Gnibbler for determining the types of objects that have been created after a call.I tried out most options mentioned previously but found this small and intuitive package to be the best: pymplerIt's quite straight forward to trace objects that were not garbage-collected, check this small example:install package via pip install pymplerThe output shows you all the objects that have been added, plus the memory they consumed.Sample output:This package provides a number of more features. Check pympler's documentation, in particular the section Identifying memory leaks.Let me recommend mem_top tool,

that helped me to solve a similar issue.It just instantly shows top suspects for memory leaks in a Python program.Tracemalloc module was integrated as a built-in module starting from Python 3.4, and appearently, it's also available for prior versions of Python as a third-party library (haven't tested it though).This module is able to output the precise files and lines that allocated the most memory. IMHO, this information is infinitly more valuable than the number of allocated instances for each type (which ends up being a lot of tuples 99% of the time, which is a clue, but barely helps in most cases).I recommend you use tracemalloc in combination with pyrasite. 9 times out of 10, running the top 10 snippet in a pyrasite-shell will give you enough information and hints to to fix the leak within 10 minutes. Yet, if you're still unable to find the leak cause, pyrasite-shell in combination with the other tools mentioned in this thread will probably give you some more hints too. You should also take a look on all the extra helpers provided by pyrasite (such as the memory viewer).You should specially have a look on your global or static data (long living data).When this data grows without restriction, you can also get troubles in Python.The garbage collector can only collect data, that is not referenced any more. But your static data can hookup data elements that should be freed.Another problem can be memory cycles, but at least in theory the Garbage collector should find and eliminate cycles -- at least as long as they are not hooked on some long living data.What kinds of long living data are specially troublesome? Have a good look on any lists and dictionaries -- they can grow without any limit. In dictionaries you might even don't see the trouble coming since when you access dicts, the number of keys in the dictionary might not be of big visibility to you ...To detect and locate memory leaks for long running processes, e.g. in production environments, you can now use stackimpact. It uses tracemalloc underneath. More info in this post.As far as best practices, keep an eye for recursive functions. In my case I ran into issues with recursion (where there didn't need to be). A simplified example of what I was doing:operating in this recursive manner won't trigger the garbage collection and clear out the remains of the function, so every time through memory usage is growing and growing.My solution was to pull the recursive call out of my_function() and have main() handle when to call it again. this way the function ends naturally and cleans up after itself.Not sure about "Best Practices" for memory leaks in python, but python should clear it's own memory by it's garbage collector. So mainly I would start by checking for circular list of some short, since they won't be picked up by the garbage collector.This is by no means exhaustive advice. But number one thing to keep in mind when writing with the thought of avoiding future memory leaks (loops) is to make sure that anything which accepts a reference to a call-back, should store that call-back as a weak reference.

assertEquals vs. assertEqual in python

Janusz

