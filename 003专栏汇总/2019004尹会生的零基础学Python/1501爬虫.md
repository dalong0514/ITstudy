# 1501. 爬虫

## 01. 网页数据的采集与 urllibi 库

常用的网络库：

1、urlib 库，http 协议常用库，自带的标准库。

2、requests 库，http 协议常用库。

3、Beautifulsoup 库 xml 格式处理库。

编码的有关概念：对于英语我们一般不会遇到什么编码问题，因为一个英文字符在系统里占一个位置。对于中文的话，微软设置了一个编码 gbk，一个汉字在系统里占有 2 个位置，gbk 编码虽然节省空间，但只包含有常用字。对于那些生僻的字，就用 utf-8 编码，这个编码一个汉字是占有 3 个位置。如果 resquest 得到的对象直接 read() 的话，它会按一个字符占一个位置的规则去读，那么读出来的就是乱码。解决的办法是在 read() 方法里指定解码的方式。

	response.read().decode('utf-8')

read() 读出来的信息是 html 语法形式的。

## 02. 网页常见的两种请求方式 get 和 post

请求网络一般有两种方式：get 和 post。

[httpbin.org](http://httpbin.org/)这个网站是专门用来测试的网站。可以在里面尝试不同的 http 协议的功能。get 是可以把我们要传递给网络服务器的信息写在 url 地址里面。

比如：

	http://httpbin.org/get?a=123&b=456

get 的好处是，往服务器传递参数时格式非常简单，坏处是传递数据的大小有限制。还有种方式是 post，一般提交用户名密码的时候是用 post 来提交的。这种情况下输入的登录信息是没有在最上面的地址栏里出现的。请求网页还有很多其他的方式，但这两种是最常用的。

以 post 方式请求的话，15.2.py 的代码如下：

```
from urllib import parse
from urllib import request

data = bytes(parse.urlencode({'word':'hello'}), encoding='utf8')

response = request.urlopen('http://httpbin.org/post', data=data)
print(response.read().decode('utf-8'))

# response2 = request.urlopen('http://httpbin.org/get', timeout=1)
# print(response2.read())
```

输出：

```
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "word": "hello"
  }, 
  "headers": {
    "Accept-Encoding": "identity", 
    "Content-Length": "10", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "Python-urllib/3.7"
  }, 
  "json": null, 
  "origin": "139.180.218.86, 139.180.218.86", 
  "url": "https://httpbin.org/post"
}

```

## 03. HTTP 头部信息的模拟

如果我们用 urllib 来请求浏览器的话，请求的内容是有细微差别的，这个差别在 http 协议里面叫做 http 头部信息，从上面的输出结果里可以看到：

    "User-Agent": "Python-urllib/3.7"

如果通过浏览器请求的话：

	http://httpbin.org/get?a=123&b=456

输出结果里是：

	"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36"

用 urllib 来请求是可以伪装的。15.3.py 的代码如下：

## 04. requests 库的基本使用

urllib 库还是相当繁琐的，所有有了更方便的 requests 库，第三方库。

使用 request 库的例子，15.4-request.py 的代码如下：

## 05. 结合正则表达式爬取图片链接

关键点和难点是如何编写正则。

```
import re

for result in results:
	url, name = result
	print(url, re.sub('\s', ' ', name))

```

### 黑板墙

#### 01

作者回复：正则表达式是个很大的话题，完整掌握给你推荐两本书《python核心编程》第三版，《正则表达式》方便你系统学习。我说下我的经验，我在第一次接触的时候忽略掉了那些我不需要的信息也要用 .* 方式匹配，导致匹配失败，另一个是 * 号有正则贪婪性，尽可能多的匹配，它们是我学习的第一个鸿沟，第二个是元字符覆盖的范围经常匹配过长，其实现在有很多在线正则表达式测试网页，方便你来查看你匹配的是否正确，希望能对你学习正则表达式有帮助。

#### 02

老师您好我想问下在 re.compile 那一行，最后面的 re.S 是什么意思呢？

作者回复：re.S叫做单行模式，简单来说，就是你用正则要匹配的内容在多行里，会增加你要匹配的难度，这时候使用 re.S 把每行最后的换行符 \n 当做正常的一个字符串来进行匹配的一种小技巧。

## 06. Beautiful Soup 的安装和使用

先安装 Beautiful Soup：

	pip3 install bs4

接着安装：

	pip3 install lxml

几个关键的函数：

	soup.prettify()
	
	soup.title()
	
	soup.title.string()

Beautiful Soup 用法十分灵活，可以根据我们的需要找到想要的标签和内容。

### 黑板墙

#### 01

请问一下你那个 html_doc 咋导入的。

作者回复：html_doc 是网页点鼠标右键查看源代码获取的。

#### 02

	print(soup.p['class'])

为什么匹配的是第一个 title 而不是 story。

作者回复：soup.p['class'] 默认取第一个，soup.find_all('p') 取所有的 p 标签。

## 07. 使用爬虫爬取新闻网站

所有的新闻内容是在 div 标签下面的。

```
def craw2(url):
	response = requests.get(url, headers=headers)
	
	soup = BeautifulSoup(response.text, 'lxml')
	
	for title_href in soup.find_all('div', class_='news_type_block'):
		print([title.get('title')
			for title in title_href.find_all('a') if title.get('title')])
craw2(url)

# 翻页来实现查看「更多」的页面，还可以用多线程进行优化
for i in range(15, 46,15):
	url = 'http://www.infoq.com/cn/news/' + str(i)
	craw2(url)
```

### 黑板墙

#### 01

最近想爬一个 aspx 网站，发现 aspx 网站爬虫方法挺复杂的，网上介绍都挺笼统，请问老师有没有这方面比较详细的教程资料？

作者回复：如果是想系统的爬取一个网站，建议使用框架来实现，视频介绍的是爬虫的原理和自己编写爬虫，建议你参考一下 scrapy 框架，提供一个中文文档地址：[Scrapy入门教程 — Scrapy 1.0.5 文档](https://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/tutorial.html)

#### 02

请问老师，那个 headers={} 中的内容怎么获取？

作者回复：headers内容来源于标准的 http 协议的定义，一般我会先使用浏览器访问目标网站，发起第一次请求前，按 F12 出现浏览器的调试界面，在请求时就可以抓到对应的 headers 。当然还能抓到很多其他有用的信息。

## 08. 使用爬虫爬取图片链接并下载图片

获取图片比获取文字更复杂，文字的话只是内容，图片的话，需要获取图片的链接地址，通过链接地址来去下载文件到本地。

1、如何获取图片的链接地址。

网页里如果嵌入图面的话，一定会有 img 标签。img src= 后面跟的就是图片地址。
