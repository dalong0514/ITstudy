## 01. What does the「yield」keyword do?

Alex. S.

[What does the「yield」keyword do?](https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do)

What is the use of the yield keyword in Python, and what does it do? For example, I'm trying to understand this code 1:

```
def _get_child_candidates(self, distance, min_dist, max_dist):
    if self._leftchild and distance - max_dist < self._median:
        yield self._leftchild
    if self._rightchild and distance + max_dist >= self._median:
        yield self._rightchild 
```

And this is the caller: 

```
result, candidates = [], [self]
while candidates:
    node = candidates.pop()
    distance = node._get_dist(obj)
    if distance <= max_dist and distance >= min_dist:
        result.extend(node._values)
    candidates.extend(node._get_child_candidates(distance, min_dist, max_dist))
return result
```

What happens when the method \_get\_child\_candidates is called?

Is a list returned? A single element? Is it called again? When will subsequent calls stop?

1. This piece of code was written by Jochen Schulz (jrschulz), who made a great Python library for metric spaces. This is the link to the complete source: Module mspace.

2008-10-23 22:21:11Z

### 01

To understand what yield does, you must understand what generators are. And before you can understand generators, you must understand iterables. 

When you create a list, you can read its items one by one. Reading its items one by one is called iteration: 

```
>>> mylist = [1, 2, 3]
>>> for i in mylist:
...    print(i)
1
2
3
```

mylist is an iterable. When you use a list comprehension, you create a list, and so an iterable:

```
>>> mylist = [x*x for x in range(3)]
>>> for i in mylist:
...    print(i)
0
1
4
```

Everything you can use "for... in..." on is an iterable; lists, strings, files...These iterables are handy because you can read them as much as you wish, but you store all the values in memory and this is not always what you want when you have a lot of values. 

Generators are iterators, a kind of iterable you can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly:

```
>>> mygenerator = (x*x for x in range(3))
>>> for i in mygenerator:
...    print(i)
0
1
4
```

It is just the same except you used () instead of []. BUT, you cannot perform for i in mygenerator a second time since generators can only be used once: they calculate 0, then forget about it and calculate 1, and end calculating 4, one by one.

yield is a keyword that is used like return, except the function will return a generator. 

1『yield 与 return 类似，特殊的是 yield 返回的是一个 generator。』

```
>>> def createGenerator():
...    mylist = range(3)
...    for i in mylist:
...        yield i*i
...
>>> mygenerator = createGenerator() # create a generator
>>> print(mygenerator) # mygenerator is an object!
<generator object createGenerator at 0xb7555c34>
>>> for i in mygenerator:
...     print(i)
0
1
4
```

Here it's a useless example, but it's handy when you know your function will return a huge set of values that you will only need to read once.

To master yield, you must understand that when you call the function, the code you have written in the function body does not run. The function only returns the generator object, this is a bit tricky. Then, your code will continue from where it left off each time for uses the generator. 






Now the hard part: The first time the for calls the generator object created from your function, it will run the code in your function from the beginning until it hits yield, then it'll return the first value of the loop. Then, each subsequent call will run another iteration of the loop you have written in the function and return the next value. This will continue until the generator is considered empty, which happens when the function runs without hitting yield. That can be because the loop has come to an end, or because you no longer satisfy an "if/else".

#### Your code explained

Generator:

```
# Here you create the method of the node object that will return the generator
def _get_child_candidates(self, distance, min_dist, max_dist):

    # Here is the code that will be called each time you use the generator object:

    # If there is still a child of the node object on its left
    # AND if the distance is ok, return the next child
    if self._leftchild and distance - max_dist < self._median:
        yield self._leftchild

    # If there is still a child of the node object on its right
    # AND if the distance is ok, return the next child
    if self._rightchild and distance + max_dist >= self._median:
        yield self._rightchild

    # If the function arrives here, the generator will be considered empty
    # there is no more than two values: the left and the right children
```

Caller:

```
# Create an empty list and a list with the current object reference
result, candidates = list(), [self]

# Loop on candidates (they contain only one element at the beginning)
while candidates:

    # Get the last candidate and remove it from the list
    node = candidates.pop()

    # Get the distance between obj and the candidate
    distance = node._get_dist(obj)

    # If distance is ok, then you can fill the result
    if distance <= max_dist and distance >= min_dist:
        result.extend(node._values)

    # Add the children of the candidate in the candidate's list
    # so the loop will keep running until it will have looked
    # at all the children of the children of the children, etc. of the candidate
    candidates.extend(node._get_child_candidates(distance, min_dist, max_dist))

return result
```

This code contains several smart parts:

1. The loop iterates on a list, but the list expands while the loop is being iterated. It's a concise way to go through all these nested data even if it's a bit dangerous since you can end up with an infinite loop. In this case, candidates.extend(node.\_get\_child\_candidates(distance, min_dist, max_dist)) exhaust all the values of the generator, but while keeps creating new generator objects which will produce different values from the previous ones since it's not applied on the same node.

2. The extend() method is a list object method that expects an iterable and adds its values to the list.

Usually we pass a list to it:

```
>>> a = [1, 2]
>>> b = [3, 4]
>>> a.extend(b)
>>> print(a)
[1, 2, 3, 4]
```

But in your code, it gets a generator, which is good because:

1. You don't need to read the values twice.

2. You may have a lot of children and you don't want them all stored in memory.

And it works because Python does not care if the argument of a method is a list or not. Python expects iterables so it will work with strings, lists, tuples, and generators! This is called duck typing and is one of the reasons why Python is so cool. But this is another story, for another question...

You can stop here, or read a little bit to see an advanced use of a generator:

Controlling a generator exhaustion

```
>>> class Bank(): # Let's create a bank, building ATMs
...    crisis = False
...    def create_atm(self):
...        while not self.crisis:
...            yield "$100"
>>> hsbc = Bank() # When everything's ok the ATM gives you as much as you want
>>> corner_street_atm = hsbc.create_atm()
>>> print(corner_street_atm.next())
$100
>>> print(corner_street_atm.next())
$100
>>> print([corner_street_atm.next() for cash in range(5)])
['$100', '$100', '$100', '$100', '$100']
>>> hsbc.crisis = True # Crisis is coming, no more money!
>>> print(corner_street_atm.next())
<type 'exceptions.StopIteration'>
>>> wall_street_atm = hsbc.create_atm() # It's even true for new ATMs
>>> print(wall_street_atm.next())
<type 'exceptions.StopIteration'>
>>> hsbc.crisis = False # The trouble is, even post-crisis the ATM remains empty
>>> print(corner_street_atm.next())
<type 'exceptions.StopIteration'>
>>> brand_new_atm = hsbc.create_atm() # Build a new one to get back in business
>>> for cash in brand_new_atm:
...    print cash
$100
$100
$100
$100
$100
$100
$100
$100
$100
...
```

Note: For Python 3, useprint(corner_street_atm.__next__()) or print(next(corner_street_atm))

It can be useful for various things like controlling access to a resource.

#### Itertools, your best friend

The itertools module contains special functions to manipulate iterables. Ever wish to duplicate a generator?

Chain two generators? Group values in a nested list with a one-liner? Map / Zip without creating another list? Then just import itertools.An example? Let's see the possible orders of arrival for a four-horse race:

```
>>> horses = [1, 2, 3, 4]
>>> races = itertools.permutations(horses)
>>> print(races)
<itertools.permutations object at 0xb754f1dc>
>>> print(list(itertools.permutations(horses)))
[(1, 2, 3, 4),
 (1, 2, 4, 3),
 (1, 3, 2, 4),
 (1, 3, 4, 2),
 (1, 4, 2, 3),
 (1, 4, 3, 2),
 (2, 1, 3, 4),
 (2, 1, 4, 3),
 (2, 3, 1, 4),
 (2, 3, 4, 1),
 (2, 4, 1, 3),
 (2, 4, 3, 1),
 (3, 1, 2, 4),
 (3, 1, 4, 2),
 (3, 2, 1, 4),
 (3, 2, 4, 1),
 (3, 4, 1, 2),
 (3, 4, 2, 1),
 (4, 1, 2, 3),
 (4, 1, 3, 2),
 (4, 2, 1, 3),
 (4, 2, 3, 1),
 (4, 3, 1, 2),
 (4, 3, 2, 1)]
```

#### Understanding the inner mechanisms of iteration

Iteration is a process implying iterables (implementing the \__iter\__() method) and iterators (implementing the \__next\__() method).

Iterables are any objects you can get an iterator from. Iterators are objects that let you iterate on iterables. There is more about it in this article about how for loops work ([Understanding Python's "for" statement](http://effbot.org/zone/python-for-statement.htm)).

yield is not as magical this answer suggests. When you call a function that contains a yield statement anywhere, you get a generator object, but no code runs. Then each time you extract an object from the generator, Python executes code in the function until it comes to a yield statement, then pauses and delivers the object. When you extract another object, Python resumes just after the yield and continues until it reaches another yield (often the same one, but one iteration later). This continues until the function runs off the end, at which point the generator is deemed exhausted. – Matthias Fripp May 23 '17 at 21:41

"These iterables are handy... but you store all the values in memory and this is not always what you want", is either wrong or confusing. An iterable returns an iterator upon calling the iter() on the iterable, and an iterator doesn't always have to store its values in memory, depending on the implementation of the iter method, it can also generate values in the sequence on demand. – picmate 涅 Feb 15 '18 at 19:21





When you see a function with yield statements, apply this easy trick to understand what will happen:This trick may give you an idea of the logic behind the function, but what actually happens with yield is significantly different than what happens in the list based approach. In many cases, the yield approach will be a lot more memory efficient and faster too. In other cases, this trick will get you stuck in an infinite loop, even though the original function works just fine. Read on to learn more...First, the iterator protocol - when you writePython performs the following two steps:The truth is Python performs the above two steps anytime it wants to loop over the contents of an object - so it could be a for loop, but it could also be code like otherlist.extend(mylist) (where otherlist is a Python list).Here mylist is an iterable because it implements the iterator protocol. In a user-defined class, you can implement the __iter__() method to make instances of your class iterable. This method should return an iterator. An iterator is an object with a next() method. It is possible to implement both __iter__() and next() on the same class, and have __iter__() return self. This will work for simple cases, but not when you want two iterators looping over the same object at the same time.So that's the iterator protocol, many objects implement this protocol:Note that a for loop doesn't know what kind of object it's dealing with - it just follows the iterator protocol, and is happy to get item after item as it calls next(). Built-in lists return their items one by one, dictionaries return the keys one by one, files return the lines one by one, etc. And generators return... well that's where yield comes in:Instead of yield statements, if you had three return statements in f123() only the first would get executed, and the function would exit. But f123() is no ordinary function. When f123() is called, it does not return any of the values in the yield statements! It returns a generator object. Also, the function does not really exit - it goes into a suspended state. When the for loop tries to loop over the generator object, the function resumes from its suspended state at the very next line after the yield it previously returned from, executes the next line of code, in this case, a yield statement, and returns that as the next item. This happens until the function exits, at which point the generator raises StopIteration, and the loop exits. So the generator object is sort of like an adapter - at one end it exhibits the iterator protocol, by exposing __iter__() and next() methods to keep the for loop happy. At the other end, however, it runs the function just enough to get the next value out of it, and puts it back in suspended mode.Usually, you can write code that doesn't use generators but implements the same logic. One option is to use the temporary list 'trick' I mentioned before. That will not work in all cases, for e.g. if you have infinite loops, or it may make inefficient use of memory when you have a really long list. The other approach is to implement a new iterable class SomethingIter that keeps the state in instance members and performs the next logical step in it's next() (or next() in Python 3) method. Depending on the logic, the code inside the next() method may end up looking very complex and be prone to bugs. Here generators provide a clean and easy solution.Think of it this way:An iterator is just a fancy sounding term for an object that has a next() method.  So a yield-ed function ends up being something like this:Original version:This is basically what the Python interpreter does with the above code:For more insight as to what's happening behind the scenes, the for loop can be rewritten to this:Does that make more sense or just confuse you more?  :)I should note that this is an oversimplification for illustrative purposes. :)The yield keyword is reduced to two simple facts:In a nutshell: a generator is a lazy, incrementally-pending list, and yield statements allow you to use function notation to program the list values the generator should incrementally spit out.Let's define a function makeRange that's just like Python's range. Calling makeRange(n) RETURNS A GENERATOR:To force the generator to immediately return its pending values, you can pass it into list() (just like you could any iterable):The above example can be thought of as merely creating a list which you append to and return:There is one major difference, though; see the last section.An iterable is the last part of a list comprehension, and all generators are iterable, so they're often used like so:To get a better feel for generators, you can play around with the itertools module (be sure to use chain.from_iterable rather than chain when warranted). For example, you might even use generators to implement infinitely-long lazy lists like itertools.count(). You could implement your own def enumerate(iterable): zip(count(), iterable), or alternatively do so with the yield keyword in a while-loop.Please note: generators can actually be used for many more things, such as implementing coroutines or non-deterministic programming or other elegant things. However, the "lazy lists" viewpoint I present here is the most common use you will find.This is how the "Python iteration protocol" works. That is, what is going on when you do list(makeRange(5)). This is what I describe earlier as a "lazy, incremental list".The built-in function next() just calls the objects .next() function, which is a part of the "iteration protocol" and is found on all iterators. You can manually use the next() function (and other parts of the iteration protocol) to implement fancy things, usually at the expense of readability, so try to avoid doing that...Normally, most people would not care about the following distinctions and probably want to stop reading here.In Python-speak, an iterable is any object which "understands the concept of a for-loop" like a list [1,2,3], and an iterator is a specific instance of the requested for-loop like [1,2,3].__iter__(). A generator is exactly the same as any iterator, except for the way it was written (with function syntax).When you request an iterator from a list, it creates a new iterator. However, when you request an iterator from an iterator (which you would rarely do), it just gives you a copy of itself.Thus, in the unlikely event that you are failing to do something like this...... then remember that a generator is an iterator; that is, it is one-time-use. If you want to reuse it, you should call myRange(...) again. If you need to use the result twice, convert the result to a list and store it in a variable x = list(myRange(5)). Those who absolutely need to clone a generator (for example, who are doing terrifyingly hackish metaprogramming) can use itertools.tee if absolutely necessary, since the copyable iterator Python PEP standards proposal has been deferred.yield is only legal inside of a function definition, and the inclusion of yield in a function definition makes it return a generator.The idea for generators comes from other languages (see footnote 1) with varying implementations. In Python's Generators, the execution of the code is frozen at the point of the yield. When the generator is called (methods are discussed below) execution resumes and then freezes at the next yield.yield provides an 

easy way of implementing the iterator protocol, defined by the following two methods: 

__iter__ and next (Python 2) or __next__ (Python 3).  Both of those methods

make an object an iterator that you could type-check with the Iterator Abstract Base 

Class from the collections module.The generator type is a sub-type of iterator:And if necessary, we can type-check like this:A feature of an Iterator is that once exhausted, you can't reuse or reset it:You'll have to make another if you want to use its functionality again (see footnote 2):One can yield data programmatically, for example:The above simple generator is also equivalent to the below - as of Python 3.3 (and not available in Python 2), you can use yield from:However, yield from also allows for delegation to subgenerators, 

which will be explained in the following section on cooperative delegation with sub-coroutines.yield forms an expression that allows data to be sent into the generator (see footnote 3)Here is an example, take note of the received variable, which will point to the data that is sent to the generator:First, we must queue up the generator with the builtin function, next. It will 

call the appropriate next or __next__ method, depending on the version of

Python you are using:And now we can send data into the generator. (Sending None is 

the same as calling next.) :Now, recall that yield from is available in Python 3. This allows us to delegate

coroutines to a subcoroutine:And now we can delegate functionality to a sub-generator and it can be used

by a generator just as above:You can read more about the precise semantics of yield from in PEP 380.The close method raises GeneratorExit at the point the function 

execution was frozen. This will also be called by __del__ so you 

can put any cleanup code where you handle the GeneratorExit:You can also throw an exception which can be handled in the generator

or propagated back to the user:I believe I have covered all aspects of the following question:It turns out that yield does a lot. I'm sure I could add even more 

thorough examples to this. If you want more or have some constructive criticism, let me know by commenting

below.The grammar currently allows any expression in a list comprehension. Since yield is an expression, it has been touted by some as interesting to use it in comprehensions or generator expression - in spite of citing no particularly good use-case.The CPython core developers are discussing deprecating its allowance.

Here's a relevant post from the mailing list:Further, there is an outstanding issue (10544) which seems to be pointing in the direction of this never being a good idea (PyPy, a Python implementation written in Python, is already raising syntax warnings.)Bottom line, until the developers of CPython tell us otherwise: Don't put yield in a generator expression or comprehension.In Python 2:An expression_list is basically any number of expressions separated by commas - essentially, in Python 2, you can stop the generator with return, but you can't return a value.In Python 3: yield is just like return - it returns whatever you tell it to (as a generator). The difference is that the next time you call the generator, execution starts from the last call to the yield statement. Unlike return, the stack frame is not cleaned up when a yield occurs, however control is transferred back to the caller, so its state will resume the next time the function is called.In the case of your code, the function get_child_candidates is acting like an iterator so that when you extend your list, it adds one element at a time to the new list.list.extend calls an iterator until it's exhausted. In the case of the code sample you posted, it would be much clearer to just return a tuple and append that to the list.There's one extra thing to mention: a function that yields doesn't actually have to terminate. I've written code like this:Then I can use it in other code like this:It really helps simplify some problems, and makes some things easier to work with. For those who prefer a minimal working example, meditate on this interactive Python session:TL;DRWhenever you find yourself building a list from scratch, yield each piece instead. This was my first "aha" moment with yield.yield is a sugary way to say Same behavior:Different behavior:Yield is single-pass: you can only iterate through once. When a function has a yield in it we call it a generator function. And an iterator is what it returns. Those terms are revealing. We lose the convenience of a container, but gain the power of a series that's computed as needed, and arbitrarily long.Yield is lazy, it puts off computation. A function with a yield in it doesn't actually execute at all when you call it. It returns an iterator object that remembers where it left off. Each time you call next() on the iterator (this happens in a for-loop) execution inches forward to the next yield. return raises StopIteration and ends the series (this is the natural end of a for-loop).Yield is versatile. Data doesn't have to be stored all together, it can be made available one at a time. It can be infinite.If you need multiple passes and the series isn't too long, just call list() on it:Brilliant choice of the word yield because both meanings apply:...provide the next data in the series....relinquish CPU execution until the iterator advances.Yield gives you a generator. As you can see, in the first case foo holds the entire list in memory at once. It's not a big deal for a list with 5 elements, but what if you want a list of 5 million? Not only is this a huge memory eater, it also costs a lot of time to build at the time that the function is called.In the second case, bar just gives you a generator. A generator is an iterable--which means you can use it in a for loop, etc, but each value can only be accessed once. All the values are also not stored in memory at the same time; the generator object "remembers" where it was in the looping the last time you called it--this way, if you're using an iterable to (say) count to 50 billion, you don't have to count to 50 billion all at once and store the 50 billion numbers to count through.Again, this is a pretty contrived example, you probably would use itertools if you really wanted to count to 50 billion. :)This is the most simple use case of generators. As you said, it can be used to write efficient permutations, using yield to push things up through the call stack instead of using some sort of stack variable. Generators can also be used for specialized tree traversal, and all manner of other things.It's returning a generator. I'm not particularly familiar with Python, but I believe it's the same kind of thing as C#'s iterator blocks if you're familiar with those.The key idea is that the compiler/interpreter/whatever does some trickery so that as far as the caller is concerned, they can keep calling next() and it will keep returning values - as if the generator method was paused. Now obviously you can't really "pause" a method, so the compiler builds a state machine for you to remember where you currently are and what the local variables etc look like. This is much easier than writing an iterator yourself.There is one type of answer that I don't feel has been given yet, among the many great answers that describe how to use generators. Here is the programming language theory answer:The yield statement in Python returns a generator. A generator in Python is a function that returns continuations (and specifically a type of coroutine, but continuations represent the more general mechanism to understand what is going on).Continuations in programming languages theory are a much more fundamental kind of computation, but they are not often used, because they are extremely hard to reason about and also very difficult to implement. But the idea of what a continuation is, is straightforward: it is the state of a computation that has not yet finished. In this state, the current values of variables, the operations that have yet to be performed, and so on, are saved. Then at some point later in the program the continuation can be invoked, such that the program's variables are reset to that state and the operations that were saved are carried out.Continuations, in this more general form, can be implemented in two ways. In the call/cc way, the program's stack is literally saved and then when the continuation is invoked, the stack is restored.In continuation passing style (CPS), continuations are just normal functions (only in languages where functions are first class) which the programmer explicitly manages and passes around to subroutines. In this style, program state is represented by closures (and the variables that happen to be encoded in them) rather than variables that reside somewhere on the stack. Functions that manage control flow accept continuation as arguments (in some variations of CPS, functions may accept multiple continuations) and manipulate control flow by invoking them by simply calling them and returning afterwards. A very simple example of continuation passing style is as follows:In this (very simplistic) example, the programmer saves the operation of actually writing the file into a continuation (which can potentially be a very complex operation with many details to write out), and then passes that continuation (i.e, as a first-class closure) to another operator which does some more processing, and then calls it if necessary. (I use this design pattern a lot in actual GUI programming, either because it saves me lines of code or, more importantly, to manage control flow after GUI events trigger.)The rest of this post will, without loss of generality, conceptualize continuations as CPS, because it is a hell of a lot easier to understand and read.Now let's talk about generators in Python. Generators are a specific subtype of continuation. Whereas continuations are able in general to save the state of a computation (i.e., the program's call stack), generators are only able to save the state of iteration over an iterator. Although, this definition is slightly misleading for certain use cases of generators. For instance:This is clearly a reasonable iterable whose behavior is well defined -- each time the generator iterates over it, it returns 4 (and does so forever). But it isn't probably the prototypical type of iterable that comes to mind when thinking of iterators (i.e., for x in collection: do_something(x)). This example illustrates the power of generators: if anything is an iterator, a generator can save the state of its iteration.To reiterate: Continuations can save the state of a program's stack and generators can save the state of iteration. This means that continuations are more a lot powerful than generators, but also that generators are a lot, lot easier. They are easier for the language designer to implement, and they are easier for the programmer to use (if you have some time to burn, try to read and understand this page about continuations and call/cc).But you could easily implement (and conceptualize) generators as a simple, specific case of continuation passing style:Whenever yield is called, it tells the function to return a continuation.  When the function is called again, it starts from wherever it left off. So, in pseudo-pseudocode (i.e., not pseudocode, but not code) the generator's next method is basically as follows:where the yield keyword is actually syntactic sugar for the real generator function, basically something like:Remember that this is just pseudocode and the actual implementation of generators in Python is more complex. But as an exercise to understand what is going on, try to use continuation passing style to implement generator objects without use of the yield keyword.Here is an example in plain language. I will provide a correspondence between high-level human concepts to low-level Python concepts.I want to operate on a sequence of numbers, but I don't want to bother my self with the creation of that sequence, I want only to focus on the operation I want to do. So, I do the following:This is what a generator does (a function that contains a yield); it starts executing, pauses whenever it does a yield, and when asked for a .next() value it continues from the point it was last. It fits perfectly by design with the iterator protocol of Python, which describes how to sequentially request values.The most famous user of the iterator protocol is the for command in Python. So, whenever you do a:it doesn't matter if sequence is a list, a string, a dictionary or a generator object like described above; the result is the same: you read items off a sequence one by one.Note that defining a function which contains a yield keyword is not the only way to create a generator; it's just the easiest way to create one.For more accurate information, read about iterator types, the yield statement and generators in the Python documentation.While a lot of answers show why you'd use a yield to create a generator, there are more uses for yield.  It's quite easy to make a coroutine, which enables the passing of information between two blocks of code.  I won't repeat any of the fine examples that have already been given about using yield to create a generator.To help understand what a yield does in the following code, you can use your finger to trace the cycle through any code that has a yield.  Every time your finger hits the yield, you have to wait for a next or a send to be entered.  When a next is called, you trace through the code until you hit the yield… the code on the right of the yield is evaluated and returned to the caller… then you wait.  When next is called again, you perform another loop through the code.  However, you'll note that in a coroutine, yield can also be used with a send… which will send a value from the caller into the yielding function. If a send is given, then yield receives the value sent, and spits it out the left hand side… then the trace through the code progresses until you hit the yield again (returning the value at the end, as if next was called).For example:There is another yield use and meaning (since Python 3.3):From PEP 380 -- Syntax for Delegating to a Subgenerator:Moreover this will introduce (since Python 3.5):to avoid coroutines being confused with a regular generator (today yield is used in both).All great answers, however a bit difficult for newbies.I assume you have learned the return statement.As an analogy, return and yield are twins. return means 'return and stop' whereas 'yield` means 'return, but continue'Run it:See, you get only a single number rather than a list of them. return never allows you prevail happily, just implements once and quit.Replace return with yield:Now, you win to get all the numbers.Comparing to return which runs once and stops, yield runs times you planed.

You can interpret return as return one of them, and yield as return all of them. This is called iterable.It's the core about yield.The difference between a list return outputs and the object yield output is:You will always get [0, 1, 2] from a list object but only could retrieve them from 'the object yield output' once. So, it has a new name generator object as displayed in Out[11]: <generator object num_list at 0x10327c990>.In conclusion, as a metaphor to grok it:Here are some Python examples of how to actually implement generators as if Python did not provide syntactic sugar for them:As a Python generator:Using lexical closures instead of generatorsUsing object closures instead of generators (because ClosuresAndObjectsAreEquivalent)From a programming viewpoint, the iterators are implemented as thunks.To implement iterators, generators, and thread pools for concurrent execution, etc. as thunks (also called anonymous functions), one uses messages sent to a closure object, which has a dispatcher, and the dispatcher answers to "messages".http://en.wikipedia.org/wiki/Message_passing"next" is a message sent to a closure, created by the "iter" call.There are lots of ways to implement this computation. I used mutation, but it is easy to do it without mutation, by returning the current value and the next yielder.Here is a demonstration which uses the structure of R6RS, but the semantics is absolutely identical to Python's. It's the same model of computation, and only a change in syntax is required to rewrite it in Python.I was going to post "read page 19 of Beazley's 'Python: Essential Reference' for a quick description of generators", but so many others have posted good descriptions already.Also, note that yield can be used in coroutines as the dual of their use in generator functions.  Although it isn't the same use as your code snippet, (yield) can be used as an expression in a function.  When a caller sends a value to the method using the send() method, then the coroutine will execute until the next (yield) statement is encountered.Generators and coroutines are a cool way to set up data-flow type applications.  I thought it would be worthwhile knowing about the other use of the yield statement in functions.Here is a simple example:Output:I am not a Python developer, but it looks to me yield holds the position of program flow and the next loop start from "yield" position. It seems like it is waiting at that position, and just before that, returning a value outside, and next time continues to work.It seems to be an interesting and nice ability :DHere is a mental image of what yield does.I like to think of a thread as having a stack (even when it's not implemented that way).When a normal function is called, it puts its local variables on the stack, does some computation, then clears the stack and returns. The values of its local variables are never seen again.With a yield function, when its code begins to run (i.e. after the function is called, returning a generator object, whose next() method is then invoked), it similarly puts its local variables onto the stack and computes for a while. But then, when it hits the yield statement, before clearing its part of the stack and returning, it takes a snapshot of its local variables and stores them in the generator object. It also writes down the place where it's currently up to in its code (i.e. the particular yield statement).So it's a kind of a frozen function that the generator is hanging onto.When next() is called subsequently, it retrieves the function's belongings onto the stack and re-animates it. The function continues to compute from where it left off, oblivious to the fact that it had just spent an eternity in cold storage.Compare the following examples:When we call the second function, it behaves very differently to the first. The yield statement might be unreachable, but if it's present anywhere, it changes the nature of what we're dealing with.Calling yielderFunction() doesn't run its code, but makes a generator out of the code. (Maybe it's a good idea to name such things with the yielder prefix for readability.)The gi_code and gi_frame fields are where the frozen state is stored. Exploring them with dir(..), we can confirm that our mental model above is credible.Like every answer suggests, yield is used for creating a sequence generator. It's used for generating some sequence dynamically. For example, while reading a file line by line on a network, you can use the yield function as follows:You can use it in your code as follows:Execution Control Transfer gotchaThe execution control will be transferred from getNextLines() to the for loop when yield is executed. Thus, every time getNextLines() is invoked, execution begins from the point where it was paused last time.Thus in short, a function with the following codewill print(My below answer only speaks from the perspective of using Python generator, not the underlying implementation of generator mechanism, which involves some tricks of stack and heap manipulation.)When yield is used instead of a return in a python function, that function is turned into something special called generator function. That function will return an object of generator type. The yield keyword is a flag to notify the python compiler to treat such function specially. Normal functions will terminate once some value is returned from it. But with the help of the compiler, the generator function can be thought of as resumable. That is, the execution context will be restored and the execution will continue from last run. Until you explicitly call return, which will raise a StopIteration exception (which is also part of the iterator protocol), or reach the end of the function. I found a lot of references about generator but this one from the functional programming perspective is the most digestable.(Now I want to talk about the rationale behind generator, and the iterator based on my own understanding. I hope this can help you grasp the essential motivation of iterator and generator. Such concept shows up in other languages as well such as C#.)As I understand, when we want to process a bunch of data, we usually first store the data somewhere and then process it one by one. But this naive approach is problematic. If the data volume is huge, it's expensive to store them as a whole beforehand. So instead of storing the data itself directly, why not store some kind of metadata indirectly, i.e. the logic how the data is computed. There are 2 approaches to wrap such metadata.Either way, an iterator is created, i.e. some object that can give you the data you want. The OO approach may be a bit complex. Anyway, which one to use is up to you.An easy example to understand what it is: yieldThe output is: Yield is an objectA return in a function will return a single value.If you want a function to return a huge set of values, use yield.More importantly, yield is a barrier.That is, it will run the code in your function from the beginning until it hits yield. Then, it’ll return the first value of the loop.Then, every other call will run the loop you have written in the function one more time, returning the next value until there isn't any value to return.Many people use return rather than yield, but in some cases yield can be more efficient and easier to work with.Here is an example which yield is definitely best for:Both functions do the same thing, but yield uses three lines instead of five and has one less variable to worry about.As you can see both functions do the same thing. The only difference is return_dates() gives a list and yield_dates() gives a generator.A real life example would be something like reading a file line by line or if you just want to make a generator.In summary, the yield statement transforms your function into a factory that produces a special object called a generator which wraps around the body of your original function. When the generator is iterated, it executes your function  until it reaches the next yield then suspends execution and evaluates to the value passed to yield. It repeats this process on each iteration until the path of execution exits the function. For instance,simply outputsThe power comes from using the generator with a loop that calculates a sequence, the generator executes the loop stopping each time to 'yield' the next result of the calculation, in this way it calculates a list on the fly, the benefit being the memory saved for especially large calculationsSay you wanted to create a your own range function that produces an iterable range of numbers, you could do it like so,and use it like this;But this is inefficient becauseLuckily Guido and his team were generous enough to develop generators so we could just do this;Now upon each iteration a function on the generator called next() executes the function until it either reaches a 'yield' statement in which it stops and  'yields' the value or reaches the end of the function. In this case on the first call, next() executes up to the yield statement and yield 'n', on the next call it will execute the  increment statement, jump back to the 'while', evaluate it, and if true, it will stop and yield 'n' again, it will continue that way until the while condition returns false and the generator jumps to the end of the function.yield is like a return element for a function. The difference is, that the yield element turns a function into a generator. A generator behaves just like a function until something is 'yielded'. The generator stops until it is next called, and continues from exactly the same point as it started. You can get a sequence of all the 'yielded' values in one, by calling list(generator()).The yield keyword simply collects returning results. Think of yield like return +=Here's a simple yield based approach, to compute the fibonacci series, explained:When you enter this into your REPL and then try and call it, you'll get a mystifying result:This is because the presence of yield signaled to Python that you want to create a generator, that is, an object that generates values on demand.So, how do you generate these values? This can either be done directly by using the built-in function next, or, indirectly by feeding it to a construct that consumes values. Using the built-in next() function, you directly invoke .next/__next__, forcing the generator to produce a value:Indirectly, if you provide fib to a for loop, a list initializer, a tuple initializer, or anything else that expects an object that generates/produces values, you'll "consume" the generator until no more values can be produced by it (and it returns):Similarly, with a tuple initializer: A generator differs from a function in the sense that it is lazy. It accomplishes this by maintaining it's local state and allowing you to resume whenever you need to. When you first invoke fib by calling it:Python compiles the function, encounters the yield keyword and simply returns a generator object back at you. Not very helpful it seems. When you then request it generates the first value, directly or indirectly, it executes all statements that it finds, until it encounters a yield, it then yields back the value you supplied to yield and pauses. For an example that better demonstrates this, let's use some print calls (replace with print "text" if on Python 2):Now, enter in the REPL:you have a generator object now waiting for a command for it to generate a value. Use next and see what get's printed:The unquoted results are what's printed. The quoted result is what is returned from yield. Call next again now:The generator remembers it was paused at yield value and resumes from there. The next message is printed and the search for the yield statement to pause at it performed again (due to the while loop).

How do I list all files of a directory?

duhhunjonn

[How do I list all files of a directory?](https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory)

How can I list all files of a directory in Python and add them to a list?

2010-07-08 19:31:22Z

How can I list all files of a directory in Python and add them to a list?os.listdir() will get you everything that's in a directory - files and directories.If you want just files, you could either filter this down using os.path:or you could use os.walk() which will yield two lists for each directory it visits - splitting into files and dirs for you. If you only want the top directory you can just break the first time it yieldsI prefer using the glob module, as it does pattern matching and expansion.It will return a list with the queried files:will return a list of all files and directories in "somedirectory".How to get all the files (and directories) in the current directory (Python 3)Following, are simple methods to retrieve only files in the current directory, using os  and the listdir() function, in Python 3.  Further exploration, will demonstrate how to return folders in the directory, but you will not have the file in the subdirectory, for that you can use walk - discussed later).I found glob easier to select the file of the same type or with something in common. Look at the following example:The function returns a list of the given extension (.txt, .docx ecc.) in the argumentThe function now returns a list of file that matched with the string you pass as argumentoutputAs you noticed, you don't have the full path of the file in the code above. If you need to have the absolute path, you can use another function of the os.path module called _getfullpathname, putting the file that you get from os.listdir() as an argument. There are other ways to have the full path, as we will check later (I replaced, as suggested by mexmex, _getfullpathname with abspath).I find this very useful to find stuff in many directories, and it helped me find a file about which I didn't remember the name:In Python 2, if you want the list of the files in the current directory, you have to give the argument as '.' or os.getcwd() in the os.listdir method.If I should need the absolute path of the files:With list comprehension:Alternatively, use pathlib.Path() instead of pathlib.Path(".")In this example, we look for the number of files that are included in all the directory and its subdirectories.A script to make order in your computer finding all files of a type (default: pptx) and copying them in a new folder.In case you want to create a txt file with all the file names:With this function you can create a txt file that will have the name of a type of file that you look for (ex. pngfile.txt) with all the full path of all the files of that type. It can be useful sometimes, I think.A one-line solution to get only list of files (no subdirectories):or absolute pathnames:Getting Full File Paths From a Directory and All Its SubdirectoriesIf you'd like, you can open and read the contents, or focus only on files with the extension ".dat" like in the code below:/Users/johnny/Desktop/TEST/SUBFOLDER/file3.datSince version 3.4 there are builtin iterators for this which are a lot more efficient than os.listdir():pathlib: New in version 3.4.According to PEP 428, the aim of the pathlib library is to provide a simple hierarchy of classes to handle filesystem paths and the common operations users do over them.os.scandir(): New in version 3.5.Note that os.walk() uses os.scandir() instead of os.listdir() from version 3.5, and its speed got increased by 2-20 times according to PEP 471.Let me also recommend reading ShadowRanger's comment below.Notes:I really liked adamk's answer, suggesting that you use glob(), from the module of the same name. This allows you to have pattern matching with *s.But as other people pointed out in the comments, glob() can get tripped up over inconsistent slash directions. To help with that, I suggest you use the join() and expanduser() functions in the os.path module, and perhaps the getcwd() function in the os module, as well.As examples:The above is terrible - the path has been hardcoded and will only ever work on Windows between the drive name and the \s being hardcoded into the path.The above works better, but it relies on the folder name Users which is often found on Windows and not so often found on other OSs. It also relies on the user having a specific name, admin.This works perfectly across all platforms.Another great example that works perfectly across platforms and does something a bit different:Hope these examples help you see the power of a few of the functions you can find in the standard Python library modules.If you are looking for a Python implementation of find, this is a recipe I use rather frequently:So I made a PyPI package out of it and there is also a GitHub repository. I hope that someone finds it potentially useful for this code.For greater results, you can use listdir() method of the os module along with a generator (a generator is a powerful iterator that keeps its state, remember?). The following code works fine with both versions: Python 2 and Python 3.Here's a code:The listdir() method returns the list of entries for the given directory. The method os.path.isfile() returns True if the given entry is a file. And the yield operator quits the func but keeps its current state, and it returns only the name of the entry detected as a file. All the above allows us to loop over the generator function.Returning a list of absolute filepaths, does not recurse into subdirectoriesHere I use a recursive structure.A wise teacher told me once that:I will thus add a solution for a subset of the problem: quite often, we only want to check whether a file matches a start string and an end string, without going into subdirectories. We would thus like a function that returns a list of filenames, like:If you care to first declare two functions, this can be done:This solution could be easily generalized with regular expressions (and you might want to add a pattern argument, if you do not want your patterns to always stick to the start or end of the filename).Using generatorsAnother very readable variant for Python 3.4+ is using pathlib.Path.glob:It is simple to make more specific, e.g. only look for Python source files which are not symbolic links, also in all subdirectories:Here's my general-purpose function for this.  It returns a list of file paths rather than filenames since I found that to be more useful.  It has a few optional arguments that make it versatile.  For instance, I often use it with arguments like pattern='*.txt' or subfolders=True.I will provide a sample one liner where sourcepath and file type can be provided as input. The code returns a list of filenames with csv extension. Use . in case all files needs to be returned. This will also recursively scans the subdirectories. [y for x in os.walk(sourcePath) for y in glob(os.path.join(x[0], '*.csv'))]Modify file extensions and source path as needed. For python2:

pip install rglobdircache is  "Deprecated since version 2.6: The dircache module has been removed in Python 3.0."

Accessing the index in 'for' loops?

Joan Venge

[Accessing the index in 'for' loops?](https://stackoverflow.com/questions/522563/accessing-the-index-in-for-loops)

How do I access the index in a for loop like the following?I want to get this output:When I loop through it using a for loop, how do I access the loop index, from 1 to 5 in this case?

2009-02-06 22:47:54Z

How do I access the index in a for loop like the following?I want to get this output:When I loop through it using a for loop, how do I access the loop index, from 1 to 5 in this case?Using an additional state variable, such as an index variable (which you would normally use in languages such as C or PHP), is considered non-pythonic.The better option is to use the built-in function enumerate(), available in both Python 2 and 3:Check out PEP 279 for more.Use enumerate to get the index with the element as you iterate:And note that Python's indexes start at zero, so you would get 0 to 4 with the above. If you want the count, 1 to 5, do this:What you are asking for is the Pythonic equivalent of the following, which is the algorithm most programmers of lower-level languages would use:Or in languages that do not have a for-each loop:or sometimes more commonly (but unidiomatically) found in Python:Python's enumerate function reduces the visual clutter by hiding the accounting for the indexes, and encapsulating the iterable into another iterable (an enumerate object) that yields a two-item tuple of the index and the item that the original iterable would provide. That looks like this:This code sample is fairly well the canonical example of the difference between code that is idiomatic of Python and code that is not. Idiomatic code is sophisticated (but not complicated) Python, written in the way that it was intended to be used. Idiomatic code is expected by the designers of the language, which means that usually this code is not just more readable, but also more efficient.Even if you don't need indexes as you go, but you need a count of the iterations (sometimes desirable) you can start with 1 and the final number will be your count.The count seems to be more what you intend to ask for (as opposed to index) when you said you wanted from 1 to 5.To break these examples down, say we have a list of items that we want to iterate over with an index:Now we pass this iterable to enumerate, creating an enumerate object:We can pull the first item out of this iterable that we would get in a loop with the next function:And we see we get a tuple of 0, the first index, and 'a', the first item:we can use what is referred to as "sequence unpacking" to extract the elements from this two-tuple:and when we inspect index, we find it refers to the first index, 0, and item refers to the first item, 'a'.So do this:It's pretty simple to start it from 1 other than 0:Important hint, though a little misleading since index will be a tuple (idx, item) here.

Good to go.As is the norm in Python there are several ways to do this. In all examples assume: lst = [1, 2, 3, 4, 5]This is also the safest option in my opinion because the chance of going into infinite recursion has been eliminated. Both the item and its index are held in variables and there is no need to write any further code

to access the item.As explained before, there are other ways to do this that have not been explained here and they may even apply more in other situations. e.g using itertools.chain with for. It handles nested loops better than the other examples.Old fashioned way:List comprehension:The fastest way to access indexes of list within loop in Python 2.7 is to use the range method for small lists and enumerate method for medium and huge size lists.Please see different approaches which can be used to iterate over list and access index value and their performance metrics (which I suppose would be useful for you) in code samples below:See performance metrics for each method below:As the result, using range method is the fastest one up to list with 1000 items. For list with size > 10 000 items enumerate is the winner.Adding some useful links below:First of all, the indexes will be from 0 to 4. Programming languages start counting from 0; don't forget that or you will come across an index out of bounds exception. All you need in the for loop is a variable counting from 0 to 4 like so:Keep in mind that I wrote 0 to 5 because the loop stops one number before the max. :)To get the value of an index useAccording to this discussion: http://bytes.com/topic/python/answers/464012-objects-list-indexLoop counter iterationThe current idiom for looping over the indices makes use of the built-in range function:Looping over both elements and indices can be achieved either by the old idiom or by using the new zip built-in function:orvia http://www.python.org/dev/peps/pep-0212/You can do it with this code:Use this code if you need to reset the index value at the end of the loop:Best solution for this problem is use enumerate in-build python function. 

enumerate return tuple 

first value is index 

second value is element of array at that indexResult:Result:Result:If I were to iterate nums = [1, 2, 3, 4, 5] I would doOr get the length as l = len(nums)If there is no duplicate value in the list:In your question, you write "how do I access the loop index, from 1 to 5 in this case?"However, the index for a list runs from zero.  So, then we need to know if what you actually want is the index and item for each item in a list, or whether you really want numbers starting from 1.  Fortunately, in Python, it is easy to do either or both.First, to clarify, the enumerate function iteratively returns the index and corresponding item for each item in a list.The output for the above is then,Notice that the index runs from 0. This kind of indexing is common among modern programming languages including Python and C.If you want your loop to span a part of the list, you can use the standard Python syntax for a part of the list. For example, to loop from the second item in a list up to but not including the last item, you could useNote that once again, the output index runs from 0,That brings us to the start=n switch for enumerate().  This simply offsets the index, you can equivalently simply add a number to the index inside the loop.for which the output isYou can also try this:The output is You can use the index methodEDIT

Highlighted in the comment that this method doesn’t work if there are duplicates in ints, the method below should work for any values in ints:Or alternativelyif you want to get both the index and the value in ints as a list of tuples.It uses the method of enumerate in the selected answer to this question, but with list comprehension, making it faster with less code.To print tuple of (index, value) in list comprehension using a for loop:Output:This serves the purpose well enough:

How do I sort a dictionary by value?

Gern Blanston

[How do I sort a dictionary by value?](https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value)

I have a dictionary of values read from two fields in a database: a string field and a numeric field. The string field is unique, so that is the key of the dictionary.I can sort on the keys, but how can I sort based on the values?Note: I have read Stack Overflow question here How do I sort a list of dictionaries by a value of the dictionary? and probably could change my code to have a list of dictionaries, but since I do not really need a list of dictionaries I wanted to know if there is a simpler solution to sort either in ascending or descending order.

2009-03-05 00:49:05Z

I have a dictionary of values read from two fields in a database: a string field and a numeric field. The string field is unique, so that is the key of the dictionary.I can sort on the keys, but how can I sort based on the values?Note: I have read Stack Overflow question here How do I sort a list of dictionaries by a value of the dictionary? and probably could change my code to have a list of dictionaries, but since I do not really need a list of dictionaries I wanted to know if there is a simpler solution to sort either in ascending or descending order.It is not possible to sort a dictionary, only to get a representation of a dictionary that is sorted. Dictionaries are inherently orderless, but other types, such as lists and tuples, are not. So you need an ordered data type to represent sorted values, which will be a list—probably a list of tuples.For instance,sorted_x will be a list of tuples sorted by the second element in each tuple. dict(sorted_x) == x.And for those wishing to sort on keys instead of values:In Python3 since unpacking is not allowed [1] we can use If you want the output as a dict, you can use collections.OrderedDict:Well, it is actually possible to do a "sort by dictionary values". Recently I had to do that in a Code Golf (Stack Overflow question Code golf: Word frequency chart). Abridged, the problem was of the kind: given a text, count how often each word is encountered and display a list of the top words, sorted by decreasing frequency. If you construct a dictionary with the words as keys and the number of occurrences of each word as value, simplified here as:then you can get a list of the words, ordered by frequency of use with sorted(d, key=d.get) - the sort iterates over the dictionary keys, using the number of word occurrences as a sort key . I am writing this detailed explanation to illustrate what people often mean by "I can easily sort a dictionary by key, but how do I sort by value" - and I think the original post was trying to address such an issue. And the solution is to do sort of list of the keys, based on the values, as shown above.You could use:This will sort the dictionary by the values of each entry within the dictionary from smallest to largest.To sort it in descending order just add reverse=True:Input:Output:Dicts can't be sorted, but you can build a sorted list from them.A sorted list of dict values:A list of (key, value) pairs, sorted by value:In recent Python 2.7, we have the new OrderedDict type, which remembers the order in which the items were added.To make a new ordered dictionary from the original, sorting by the values:The OrderedDict behaves like a normal dict:UPDATE: 5 DECEMBER 2015 using Python 3.5Whilst I found the accepted answer useful, I was also surprised that it hasn't been updated to reference OrderedDict from the standard library collections module as a viable, modern alternative - designed to solve exactly this type of problem.The official OrderedDict documentation offers a very similar example too, but using a lambda for the sort function:Pretty much the same as Hank Gay's answer:Or optimized slightly as suggested by John Fouhy:It can often be very handy to use namedtuple. For example, you have a dictionary of 'name' as keys and 'score' as values and you want to sort on 'score':sorting with lowest score first:sorting with highest score first:Now you can get the name and score of, let's say the second-best player (index=1) very Pythonically like this:Good news, so the OP's original use case of mapping pairs retrieved from a database with unique string ids as keys and numeric values as values into a built-in Python v3.6+ dict, should now respect the insert order.If say the resulting two column table expressions from a database query like:would be stored in two Python tuples, k_seq and v_seq (aligned by numerical index and with the same length of course), then:Allow to output later as:yielding in this case (for the new Python 3.6+ built-in dict!):in the same ordering per value of v.Where in the Python 3.5 install on my machine it currently yields:As proposed in 2012 by Raymond Hettinger (cf. mail on python-dev with subject "More compact dictionaries with faster iteration") and now (in 2016) announced in a mail by Victor Stinner to python-dev with subject "Python 3.6 dict becomes compact and gets a private version; and keywords become ordered" due to the fix/implementation of issue 27350 "Compact and ordered dict" in Python 3.6 we will now be able, to use a built-in dict to maintain insert order!!Hopefully this will lead to a thin layer OrderedDict implementation as a first step. As @JimFasarakis-Hilliard indicated, some see use cases for the OrderedDict type also in the future. I think the Python community at large will carefully inspect, if this will stand the test of time, and what the next steps will be.Time to rethink our coding habits to not miss the possibilities opened by stable ordering of:The first because it eases dispatch in the implementation of functions and methods in some cases.The second as it encourages to more easily use dicts as intermediate storage in processing pipelines.Raymond Hettinger kindly provided documentation explaining "The Tech Behind Python 3.6 Dictionaries" - from his San Francisco Python Meetup Group presentation 2016-DEC-08.And maybe quite some Stack Overflow high decorated question and answer pages will receive variants of this information and many high quality answers will require a per version update too.As @ajcr rightfully notes: "The order-preserving aspect of this new implementation is considered an implementation detail and should not be relied upon." (from the whatsnew36) not nit picking, but the citation was cut a bit pessimistic ;-). It continues as " (this may change in the future, but it is desired to have this new dict implementation in the language for a few releases before changing the language spec to mandate order-preserving semantics for all current and future Python implementations; this also helps preserve backwards-compatibility with older versions of the language where random iteration order is still in effect, e.g. Python 3.5)."So as in some human languages (e.g. German), usage shapes the language, and the will now has been declared ... in whatsnew36.In a mail to the python-dev list, Guido van Rossum declared:So, the version 3.6 CPython side-effect of dict insertion ordering is now becoming part of the language spec (and not anymore only an implementation detail). That mail thread also surfaced some distinguishing design goals for collections.OrderedDict as reminded by Raymond Hettinger during discussion.I had the same problem, and I solved it like this:(People who answer "It is not possible to sort a dict" did not read the question! In fact, "I can sort on the keys, but how can I sort based on the values?" clearly means that he wants a list of the keys sorted according to the value of their values.)Please notice that the order is not well defined (keys with the same value will be in an arbitrary order in the output list).In Python 2.7, simply do:copy-paste from : http://docs.python.org/dev/library/collections.html#ordereddict-examples-and-recipesEnjoy ;-)If values are numeric you may also use Counter from collections.This is the code:Here are the results:OriginalRoflRank Try the following approach. Let us define a dictionary called mydict with the following data:If one wanted to sort the dictionary by keys, one could do something like:This should return the following output:On the other hand, if one wanted to sort a dictionary by value (as is asked in the question), one could do the following:The result of this command (sorting the dictionary by value) should return the following:You can create an "inverted index", alsoNow your inverse has the values; each value has a list of applicable keys.You can use the collections.Counter. Note, this will work for both numeric and non-numeric values.Starting from Python 3.6, dict objects are now ordered by insertion order. It's officially in the specs of Python 3.7.Before that, you had to use OrderedDict.Python 3.7 documentation says:You can use a skip dict which is a dictionary that's permanently sorted by value.If you use keys(), values() or items() then you'll iterate in sorted order by value.It's implemented using the skip list datastructure.You can also use custom function that can be passed to key.Here is a solution using zip on d.values() and d.keys().  A few lines down this link (on Dictionary view objects) is:So we can do the following:As pointed out by Dilettant, Python 3.6 will now keep the order! I thought I'd share a function I wrote that eases the sorting of an iterable (tuple, list, dict). In the latter case, you can sort either on keys or values, and it can take numeric comparison into account. Only for >= 3.6!When you try using sorted on an iterable that holds e.g. strings as well as ints, sorted() will fail. Of course you can force string comparison with str(). However, in some cases you want to do actual numeric comparison where 12 is smaller than 20 (which is not the case in string comparison). So I came up with the following. When you want explicit numeric comparison you can use the flag num_as_num which will try to do explicit numeric sorting by trying to convert all values to floats. If that succeeds, it will do numeric sorting, otherwise it'll resort to string comparison.Comments for improvement or push requests welcome.Of course, remember, you need to use OrderedDict because regular Python dictionaries don't keep the original order. If you do not have Python 2.7 or higher, the best you can do is iterate over the values in a generator function. (There is an OrderedDict for 2.4 and 2.6  here, but a) I don't know about how well it works and b) You have to download and install it of course. If you do not have administrative access, then I'm afraid the option's out.)You can also print out every valuePlease remember to remove the parentheses after print if not using Python 3.0 or aboveUse ValueSortedDict from dicts:Iterate through a dict and sort it by its values in descending order:If your values are integers, and you use Python 2.7 or newer, you can use collections.Counter instead of dict. The most_common method will give you all items, sorted by the value.Just learned relevant skill from Python for Everybody.You may use a temporary list to help you to sort the dictionary:If you want to sort the list in descending order, simply change the original sorting line to:Using list comprehension, the one liner would be:Sample Output:This works in 3.1.x:For the sake of completeness, I am posting a solution using heapq. Note, this method will work for both numeric and non-numeric valuesBecause of requirements to retain backward compatability with older versions of Python I think the OrderedDict solution is very unwise. You want something that works with Python 2.7 and older versions.But the collections solution mentioned in another answer is absolutely superb, because you retrain a connection between the key and value which in the case of dictionaries is extremely important.I don't agree with the number one choice presented in another answer, because it throws away the keys.I used the solution mentioned above (code shown below) and retained access to both keys and values and in my case the ordering was on the values, but the importance was the ordering of the keys after ordering the values.

How do I check if a list is empty?

Ray Vega

[How do I check if a list is empty?](https://stackoverflow.com/questions/53513/how-do-i-check-if-a-list-is-empty)

For example, if passed the following:How do I check to see if a is empty?

2008-09-10 06:20:11Z

For example, if passed the following:How do I check to see if a is empty?Using the implicit booleanness of the empty list is quite pythonic.The pythonic way to do it is from the PEP 8 style guide (where Yes means「recommended」and No means「not recommended」):I prefer it explicitly:This way it's 100% clear that li is a sequence (list) and we want to test its size. My problem with if not li: ... is that it gives the false impression that li is a boolean variable.This is the first google hit for "python test empty array" and similar queries, plus other people seem to be generalizing the question beyond just lists, so I thought I'd add a caveat for a different type of sequence that a lot of people might use.You need to be careful with NumPy arrays, because other methods that work fine for lists or other standard containers fail for NumPy arrays.  I explain why below, but in short, the preferred method is to use size.The "pythonic" way fails with NumPy arrays because NumPy tries to cast the array to an array of bools, and if x tries to evaluate all of those bools at once for some kind of aggregate truth value.  But this doesn't make any sense, so you get a ValueError:But at least the case above tells you that it failed.  If you happen to have a NumPy array with exactly one element, the if statement will "work", in the sense that you don't get an error.  However, if that one element happens to be 0 (or 0.0, or False, ...), the if statement will incorrectly result in False:But clearly x exists and is not empty!  This result is not what you wanted.For example,returns 1, even though the array has zero elements.As explained in the SciPy FAQ, the correct method in all cases where you know you have a NumPy array is to use if x.size:If you're not sure whether it might be a list, a NumPy array, or something else, you could combine this approach with the answer @dubiousjim gives to make sure the right test is used for each type.  Not very "pythonic", but it turns out that NumPy intentionally broke pythonicity in at least this sense.If you need to do more than just check if the input is empty, and you're using other NumPy features like indexing or math operations, it's probably more efficient (and certainly more common) to force the input to be a NumPy array.  There are a few nice functions for doing this quickly — most importantly numpy.asarray.  This takes your input, does nothing if it's already an array, or wraps your input into an array if it's a list, tuple, etc., and optionally converts it to your chosen dtype.  So it's very quick whenever it can be, and it ensures that you just get to assume the input is a NumPy array.  We usually even just use the same name, as the conversion to an array won't make it back outside of the current scope:This will make the x.size check work in all cases I see on this page.Place the list in a boolean context (for example, with an if or while statement). It will test False if it is empty, and True otherwise. For example:PEP 8, the official Python style guide for Python code in Python's standard library, asserts:We should expect that standard library code should be as performant and correct as possible. But why is that the case, and why do we need this guidance?I frequently see code like this from experienced programmers new to Python:And users of lazy languages may be tempted to do this:These are correct in their respective other languages. And this is even semantically correct in Python. But we consider it un-Pythonic because Python supports these semantics directly in the list object's interface via boolean coercion.From the docs (and note specifically the inclusion of the empty list, []):And the datamodel documentation:and So instead of this:or this:Do this:Does it pay off? (Note that less time to perform an equivalent operation is better:)For scale, here's the cost of calling the function and constructing and returning an empty list, which you might subtract from the costs of the emptiness checks used above:We see that either checking for length with the builtin function len compared to 0 or checking against an empty list is much less performant than using the builtin syntax of the language as documented.Why?For the len(a) == 0 check:First Python has to check the globals to see if len is shadowed. Then it must call the function, load 0, and do the equality comparison in Python (instead of with C):And for the [] == [] it has to build an unnecessary list and then, again, do the comparison operation in Python's virtual machine (as opposed to C)The "Pythonic" way is a much simpler and faster check since the length of the list is cached in the object instance header:From the c source in Include/listobject.h:IPython magic, %timeit, is not entirely useless here:We can see there's a bit of linear cost for each additional not here. We want to see the costs, ceteris paribus, that is, all else equal - where all else is minimized as far as possible:Now let's look at the case for an unempty list:What we can see here is that it makes little difference whether you pass in an actual bool to the condition check or the list itself, and if anything, giving the list, as is, is faster.Python is written in C; it uses its logic at the C level. Anything you write in Python will be slower. And it will likely be orders of magnitude slower unless you're using the mechanisms built into Python directly.An empty list is itself considered false in true value testing (see python documentation):@Daren ThomasYour duckCollection should implement __nonzero__ or __len__ so the if a: will work without problems.Patrick's (accepted) answer is right: if not a: is the right way to do it. Harley Holcombe's answer is right that this is in the PEP 8 style guide. But what none of the answers explain is why it's a good idea to follow the idiom—even if you personally find it's not explicit enough or confusing to Ruby users or whatever.Python code, and the Python community, has very strong idioms. Following those idioms makes your code easier to read for anyone experienced in Python. And when you violate those idioms, that's a strong signal.It's true that if not a: doesn't distinguish empty lists from None, or numeric 0, or empty tuples, or empty user-created collection types, or empty user-created not-quite-collection types, or single-element NumPy array acting as scalars with falsey values, etc. And sometimes it's important to be explicit about that. And in that case, you know what you want to be explicit about, so you can test for exactly that. For example, if not a and a is not None: means "anything falsey except None", while if len(a) != 0: means "only empty sequences—and anything besides a sequence is an error here", and so on. Besides testing for exactly what you want to test, this also signals to the reader that this test is important.But when you don't have anything to be explicit about, anything other than if not a: is misleading the reader. You're signaling something as important when it isn't. (You may also be making the code less flexible, or slower, or whatever, but that's all less important.) And if you habitually mislead the reader like this, then when you do need to make a distinction, it's going to pass unnoticed because you've been "crying wolf" all over your code.No one seems to have addressed questioning your need to test the list in the first place.  Because you provided no additional context, I can imagine that you may not need to do this check in the first place, but are unfamiliar with list processing in Python.I would argue that the most pythonic way is to not check at all, but rather to just process the list.  That way it will do the right thing whether empty or full.This has the benefit of handling any contents of a, while not requiring a specific check for emptiness.  If a is empty, the dependent block will not execute and the interpreter will fall through to the next line.If you do actually need to check the array for emptiness, the other answers are sufficient.len() is an O(1) operation for Python lists, strings, dicts, and sets. Python internally keeps track of the number of elements in these containers.JavaScript has a similar notion of truthy/falsy.I had written:which was voted -1. I'm not sure if that's because readers objected to the strategy or thought the answer wasn't helpful as presented. I'll pretend it was the latter, since---whatever counts as "pythonic"---this is the correct strategy. Unless you've already ruled out, or are prepared to handle cases where a is, for example, False, you need a test more restrictive than just if not a:. You could use something like this:the first test is in response to @Mike's answer, above. The third line could also be replaced with:if you only want to accept instances of particular types (and their subtypes), or with:You can get away without the explicit type check, but only if the surrounding context already assures you that a is a value of the types you're prepared to handle, or if you're sure that types you're not prepared to handle are going to raise errors (e.g., a TypeError if you call len on a value for which it's undefined) that you're prepared to handle. In general, the "pythonic" conventions seem to go this last way. Squeeze it like a duck and let it raise a DuckError if it doesn't know how to quack. You still have to think about what type assumptions you're making, though, and whether the cases you're not prepared to handle properly really are going to error out in the right places. The Numpy arrays are a good example where just blindly relying on len or the boolean typecast may not do precisely what you're expecting.From documentation on truth value testing:All values other than what is listed here are considered TrueAs can be seen, empty list [] is falsy, so doing what would be done to a boolean value sounds most efficient:Here are a few ways you can check if a list is empty:1) The pretty simple pythonic way:In Python, empty containers such as lists,tuples,sets,dicts,variables etc are seen as False. One could simply treat the list as a predicate (returning a Boolean value). And  a True value would indicate that it's non-empty.2) A much explicit way: using the len() to find the length and check if it equals to 0:3) Or comparing it to an anonymous empty list:4) Another yet silly way to do is using exception and iter():I prefer the following:Method 1 (Preferred):Method 2 :Method 3: It is sometimes good to test for None and for emptiness separately as those are two different states. The code above produces the following output:Although it's worth nothing that None is falsy. So if you don't want to separate test for None-ness, you don't have to do that. produces expectedMany answers have been given, and a lot of them are pretty good. I just wanted to add that the checkwill also pass for None and other types of empty structures. If you truly want to check for an empty list, you can do this:we could use a simple if else:Being inspired by @dubiousjim's solution, I propose to use an additional general check of whether is it something iterableNote: a string is considered to be iterable. - add and not isinstance(a,(str,unicode)) if you want the empty string to be excludedTest:If you want to check if a list is empty:If you want to check whether all the values in list is empty. However it will be True for an empty list:If you want to use both cases together:Now you can use:You can even try using bool() like thisI love this way for checking list is empty or not. Very handy and useful.From python3 onwards you can useto check if the list is emptyEDIT : This works with python2.7 too.. I am not sure why there are so many complicated answers.

It's pretty clear and straightforwarda little more practical:and shertest version:Simple way is checking the length is equal zero.Simply use is_empty() or make function like:- It can be used for any data_structure like a list,tuples, dictionary and many more. By these, you can call it many times using just is_empty(any_structure). The truth value of an empty list is False whereas for a non-empty list it is True.What brought me here is a special use-case: I actually wanted a function to tell me if a list is empty or not. I wanted to avoid writing my own function or using a lambda-expression here (because it seemed like it should be simple enough):And, of course, there is a very natural way to do it:Of course, do not use bool in if (i.e., if bool(L):) because it's implied. But, for the cases when "is not empty" is explicitly needed as a function, bool is the best choice.Hope this helps.

How to make a flat list out of list of lists?

Emma

[How to make a flat list out of list of lists?](https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists)

I wonder whether there is a shortcut to make a simple list out of list of lists in Python.I can do that in a for loop, but maybe there is some cool "one-liner"? I tried it with reduce(), but I get an error.CodeError message

2009-06-04 20:30:05Z

I wonder whether there is a shortcut to make a simple list out of list of lists in Python.I can do that in a for loop, but maybe there is some cool "one-liner"? I tried it with reduce(), but I get an error.CodeError messageGiven a list of lists l,flat_list = [item for sublist in l for item in sublist]which means:is faster than the shortcuts posted so far. (l is the list to flatten.)Here is the corresponding function:As evidence, you can use the timeit module in the standard library:Explanation: the shortcuts based on + (including the implied use in sum) are, of necessity, O(L**2) when there are L sublists -- as the intermediate result list keeps getting longer, at each step a new intermediate result list object gets allocated, and all the items in the previous intermediate result must be copied over (as well as a few new ones added at the end). So, for simplicity and without actual loss of generality, say you have L sublists of I items each: the first I items are copied back and forth L-1 times, the second I items L-2 times, and so on; total number of copies is I times the sum of x for x from 1 to L excluded, i.e., I * (L**2)/2.The list comprehension just generates one list, once, and copies each item over (from its original place of residence to the result list) also exactly once.You can use itertools.chain():Or you can use itertools.chain.from_iterable() which doesn't require unpacking the list with the * operator:This approach is arguably more readable than [item for sublist in l for item in sublist] and appears to be faster too:Note from the author: This is inefficient. But fun, because monoids are awesome. It's not appropriate for production Python code.This just sums the elements of iterable passed in the first argument, treating second argument as the initial value of the sum (if not given, 0 is used instead and this case will give you an error).Because you are summing nested lists, you actually get [1,3]+[2,4] as a result of sum([[1,3],[2,4]],[]), which is equal to [1,3,2,4].Note that only works on lists of lists. For lists of lists of lists, you'll need another solution.I tested most suggested solutions with perfplot (a pet project of mine, essentially a wrapper around timeit), and foundto be the fastest solution. (operator.iadd is equally fast.)Code to reproduce the plot:The extend() method in your example modifies x instead of returning a useful value (which reduce() expects).A faster way to do the reduce version would beHere is a general approach that applies to numbers, strings, nested lists and mixed containers.CodeNotes: DemoReference...Pandas:...Itertools:...Matplotlib...Unipath:...Setuptools:If you want to flatten a data-structure where you don't know how deep it's nested you could use iteration_utilities.deepflatten1It's a generator so you need to cast the result to a list or explicitly iterate over it.To flatten only one level and if each of the items is itself iterable you can also use iteration_utilities.flatten which itself is just a thin wrapper around itertools.chain.from_iterable:Just to add some timings (based on Nico Schlömer answer that didn't include the function presented in this answer):It's a log-log plot to accommodate for the huge range of values spanned. For qualitative reasoning: Lower is better.The results show that if the iterable contains only a few inner iterables then sum will be fastest, however for long iterables only the itertools.chain.from_iterable, iteration_utilities.deepflatten or the nested comprehension have reasonable performance with itertools.chain.from_iterable being the fastest (as already noticed by Nico Schlömer).1 Disclaimer: I'm the author of that libraryI take my statement back. sum is not the winner. Although it is faster when the list is small. But the performance degrades significantly with larger lists. The sum version is still running for more than a minute and it hasn't done processing yet!For medium lists:Using small lists and timeit: number=1000000There seems to be a confusion with operator.add! When you add two lists together, the correct term for that is concat, not add. operator.concat is what you need to use.If you're thinking functional, it is as easy as this::You see reduce respects the sequence type, so when you supply a tuple, you get back a tuple. Let's try with a list::Aha, you get back a list.How about performance::from_iterable is pretty fast! But it's no comparison to reduce with concat.Why do you use extend?This should work fine.Consider installing the more_itertools package.It ships with an implementation for flatten (source, from the itertools recipes):As of version 2.4, you can flatten more complicated, nested iterables with more_itertools.collapse (source, contributed by  abarnet).The reason your function didn't work is because the extend extends an array in-place and doesn't return it. You can still return x from lambda, using something like this:Note: extend is more efficient than + on lists.matplotlib.cbook.flatten() will work for nested lists even if they nest more deeply than the example.Result:This is 18x faster than underscore._.flatten:Recursive versionThe accepted answer did not work for me when dealing with text-based lists of variable lengths. Here is an alternate approach that did work for me.An bad feature of Anil's function above is that it requires the user to always manually specify the second argument to be an empty list []. This should instead be a default. Due to the way Python objects work, these should be set inside the function, not in the arguments.Here's a working function:Testing:Following seem simplest to me:One can also use NumPy's flat:Edit 11/02/2016: Only works when sublists have identical dimensions.You can use numpy :

flat_list = list(np.concatenate(list_of_list))If you are willing to give up a tiny amount of speed for a cleaner look, then you could use numpy.concatenate().tolist() or numpy.concatenate().ravel().tolist():You can find out more here in the docs numpy.concatenate and numpy.ravelSimple code for underscore.py package fanIt solves all flatten problems (none list item or complex nesting)You can install underscore.py with pipNote: Below applies to Python 3.3+ because it uses yield_from.  six is also a third-party package, though it is stable.  Alternately, you could use sys.version.In the case of obj = [[1, 2,], [3, 4], [5, 6]], all of the solutions here are good, including list comprehension and itertools.chain.from_iterable.However, consider this slightly more complex case:There are several problems here:You can remedy this as follows:Here, you check that the sub-element (1) is iterable with Iterable, an ABC from itertools, but also want to ensure that (2) the element is not "string-like."This Code also works fine as it just extend the list all the way. Although it is much similar but only have one for loop. So It have less complexity than adding 2 for loops.The advantage of this solution over most others here is that if you have a list like:while most other solutions throw an error this solution handles them.Fastest solution I have found (for large list anyway):Done! You can of course turn it back into a list by executing list(l)This may not be the most efficient way but I thought to put a one-liner (actually a two-liner). Both versions will work on arbitrary hierarchy nested lists, and exploits language features (Python3.5) and recursion.The output isThis works in a depth first manner. The recursion goes down until it finds a non-list element, then extends the local variable flist and then rolls back it to the parent. Whenever flist is returned, it is extended to the parent's flist in the list comprehension. Therefore, at the root, a flat list is returned.The above one creates several local lists and returns them which are used to extend the parent's list. I think the way around for this may be creating a gloabl flist, like below.The output is againAlthough I am not sure at this time about the efficiency.another fun way to do this:

Catch multiple exceptions in one line (except block)

inspectorG4dget

[Catch multiple exceptions in one line (except block)](https://stackoverflow.com/questions/6470428/catch-multiple-exceptions-in-one-line-except-block)

I know that I can do:I can also do this:But if I want to do the same thing inside two different exceptions, the best I can think of right now is to do this:Is there any way that I can do something like this (since the action to take in both exceptions is to say please):Now this really won't work, as it matches the syntax for:So, my effort to catch the two distinct exceptions doesn't exactly come through.Is there a way to do this?

2011-06-24 15:55:08Z

I know that I can do:I can also do this:But if I want to do the same thing inside two different exceptions, the best I can think of right now is to do this:Is there any way that I can do something like this (since the action to take in both exceptions is to say please):Now this really won't work, as it matches the syntax for:So, my effort to catch the two distinct exceptions doesn't exactly come through.Is there a way to do this?From Python Documentation:Or, for Python 2 only:Separating the exception from the variable with a comma will still work in Python 2.6 and 2.7, but is now deprecated and does not work in Python 3; now you should be using as.Do this:The parentheses are required due to older syntax that used the commas to assign the error object to a name. The as keyword is used for the assignment. You can use any name for the error object, I prefer error personally.To do this in a manner currently and forward compatible with Python, you need to separate the Exceptions with commas and wrap them with parentheses to differentiate from earlier syntax that assigned the exception instance to a variable name by following the Exception type to be caught with a comma. Here's an example of simple usage:I'm specifying only these exceptions to avoid hiding bugs, which if I encounter I expect the full stack trace from.This is documented here: https://docs.python.org/tutorial/errors.htmlYou can assign the exception to a variable, (e is common, but you might prefer a more verbose variable if you have long exception handling or your IDE only highlights selections larger than that, as mine does.) The instance has an args attribute. Here is an example:Note that in Python 3, the err object falls out of scope when the except block is concluded.You may see code that assigns the error with a comma. This usage, the only form available in Python 2.5 and earlier, is deprecated, and if you wish your code to be forward compatible in Python 3, you should update the syntax to use the new form:If you see the comma name assignment in your codebase, and you're using Python 2.5 or higher, switch to the new way of doing it so your code remains compatible when you upgrade.The accepted answer is really 4 lines of code, minimum:The try, except, pass lines can be handled in a single line with the suppress context manager, available in Python 3.4:So when you want to pass on certain exceptions, use suppress.From Python documentation -> 8.3 Handling Exceptions:If you frequently use a large number of exceptions, you can pre-define a tuple, so you don't have to re-type them many times. NOTES: One of the way to do this is..and another way is to create method which performs task executed by except block and call it through all of the except block that you write..I know that second one is not the best way to do this, but i'm just showing number of ways to do this thing.

How do I pass a variable by reference?

David Sykes

[How do I pass a variable by reference?](https://stackoverflow.com/questions/986006/how-do-i-pass-a-variable-by-reference)

The Python documentation seems unclear about whether parameters are passed by reference or value, and the following code produces the unchanged value 'Original'Is there something I can do to pass the variable by actual reference?

2009-06-12 10:23:51Z

The Python documentation seems unclear about whether parameters are passed by reference or value, and the following code produces the unchanged value 'Original'Is there something I can do to pass the variable by actual reference?Arguments are passed by assignment. The rationale behind this is twofold:So:To make it even more clear, let's have some examples. Let's try to modify the list that was passed to a method:Output:Since the parameter passed in is a reference to outer_list, not a copy of it, we can use the mutating list methods to change it and have the changes reflected in the outer scope.Now let's see what happens when we try to change the reference that was passed in as a parameter:Output:Since the the_list parameter was passed by value, assigning a new list to it had no effect that the code outside the method could see. The the_list was a copy of the outer_list reference, and we had the_list point to a new list, but there was no way to change where outer_list pointed.It's immutable, so there's nothing we can do to change the contents of the stringNow, let's try to change the referenceOutput:Again, since the the_string parameter was passed by value, assigning a new string to it had no effect that the code outside the method could see. The the_string was a copy of the outer_string reference, and we had the_string point to a new string, but there was no way to change where outer_string pointed.I hope this clears things up a little.EDIT: It's been noted that this doesn't answer the question that @David originally asked, "Is there something I can do to pass the variable by actual reference?". Let's work on that.As @Andrea's answer shows, you could return the new value. This doesn't change the way things are passed in, but does let you get the information you want back out:If you really wanted to avoid using a return value, you could create a class to hold your value and pass it into the function or use an existing class, like a list:Although this seems a little cumbersome.The problem comes from a misunderstanding of what variables are in Python. If you're used to most traditional languages, you have a mental model of what happens in the following sequence:You believe that a is a memory location that stores the value 1, then is updated to store the value 2. That's not how things work in Python. Rather, a starts as a reference to an object with the value 1, then gets reassigned as a reference to an object with the value 2. Those two objects may continue to coexist even though a doesn't refer to the first one anymore; in fact they may be shared by any number of other references within the program.When you call a function with a parameter, a new reference is created that refers to the object passed in. This is separate from the reference that was used in the function call, so there's no way to update that reference and make it refer to a new object. In your example:self.variable is a reference to the string object 'Original'. When you call Change you create a second reference var to the object. Inside the function you reassign the reference var to a different string object 'Changed', but the reference self.variable is separate and does not change.The only way around this is to pass a mutable object. Because both references refer to the same object, any changes to the object are reflected in both places.I found the other answers rather long and complicated, so I created this simple diagram to explain the way Python treats variables and parameters.

It is neither pass-by-value or pass-by-reference - it is call-by-object. See this, by Fredrik Lundh: http://effbot.org/zone/call-by-object.htmHere is a significant quote:In your example, when the Change method is called--a namespace is created for it; and var becomes a name, within that namespace, for the string object 'Original'. That object then has a name in two namespaces. Next, var = 'Changed' binds var to a new string object, and thus the method's namespace forgets about 'Original'. Finally, that namespace is forgotten, and the string 'Changed' along with it.Think of stuff being passed by assignment instead of by reference/by value. That way, it is always clear, what is happening as long as you understand what happens during the normal assignment.So, when passing a list to a function/method, the list is assigned to the parameter name. Appending to the list will result in the list being modified. Reassigning the list inside the function will not change the original list, since:Since immutable types cannot be modified, they seem like being passed by value - passing an int into a function means assigning the int to the function's parameter. You can only ever reassign that, but it won't change the original variables value.Effbot (aka Fredrik Lundh) has described Python's variable passing style as call-by-object:  http://effbot.org/zone/call-by-object.htmObjects are allocated on the heap and pointers to them can be passed around anywhere.  Hope that clarifies the issue for you. Technically, Python always uses pass by reference values. I am going to repeat my other answer to support my statement.Python always uses pass-by-reference values. There isn't any exception. Any variable assignment means copying the reference value. No exception. Any variable is the name bound to the reference value. Always.You can think about a reference value as the address of the target object. The address is automatically dereferenced when used. This way, working with the reference value, it seems you work directly with the target object. But there always is a reference in between, one step more to jump to the target.Here is the example that proves that Python uses passing by reference:If the argument was passed by value, the outer lst could not be modified. The green are the target objects (the black is the value stored inside, the red is the object type), the yellow is the memory with the reference value inside -- drawn as the arrow. The blue solid arrow is the reference value that was passed to the function (via the dashed blue arrow path). The ugly dark yellow is the internal dictionary. (It actually could be drawn also as a green ellipse. The colour and the shape only says it is internal.)You can use the id() built-in function to learn what the reference value is (that is, the address of the target object).In compiled languages, a variable is a memory space that is able to capture the value of the type. In Python, a variable is a name (captured internally as a string) bound to the reference variable that holds the reference value to the target object. The name of the variable is the key in the internal dictionary, the value part of that dictionary item stores the reference value to the target.Reference values are hidden in Python. There isn't any explicit user type for storing the reference value. However, you can use a list element (or element in any other suitable container type) as the reference variable, because all containers do store the elements also as references to the target objects. In other words, elements are actually not contained inside the container -- only the references to elements are.The key to understanding parameter passing is to stop thinking about "variables". There are names and objects in Python and together they

appear like variables, but it is useful to always distinguish the three.That is all there is to it. Mutability is irrelevant for this question.Example: This binds the name a to an object of type integer that holds the value 1.This binds the name b to the same object that the name x is currently bound to.

Afterwards, the name b has nothing to do with the name x any more.See sections 3.1 and 4.2 in the Python 3 language reference.In the code shown in the question, the statement self.Change(self.variable) binds the name var (in the scope of function Change) to the object that holds the value 'Original' and the assignment var = 'Changed' (in the body of function Change) assigns that same name again: to some other object (that happens to hold a string as well but could have been something else entirely).(edit 2019-04-28)So if the thing you want to change is a mutable object, there is no problem, as everything is effectively passed by reference.If it is an immutable object (e.g. a bool, number, string), the way to go is to wrap it in a mutable object.

The quick-and-dirty solution for this is a one-element list (instead of self.variable, pass [self.variable] and in the function modify var[0]).

The more pythonic approach would be to introduce a trivial, one-attribute class. The function receives an instance of the class and manipulates the attribute.A simple trick I normally use is to just wrap it in a list:(Yeah I know this can be inconvenient, but sometimes it is simple enough to do this.)(edit - Blair has updated his enormously popular answer so that it is now accurate)I think it is important to note that the current post with the most votes (by Blair Conrad), while being correct with respect to its result, is misleading and is borderline incorrect based on its definitions.  While there are many languages (like C) that allow the user to either pass by reference or pass by value, Python is not one of them.David Cournapeau's answer points to the real answer and explains why the behavior in Blair Conrad's post seems to be correct while the definitions are not.To the extent that Python is pass by value, all languages are pass by value since some piece of data (be it a "value" or a "reference") must be sent. However, that does not mean that Python is pass by value in the sense that a C programmer would think of it.If you want the behavior, Blair Conrad's answer is fine.  But if you want to know the nuts and bolts of why Python is neither pass by value or pass by reference, read David Cournapeau's answer.You got some really good answers here.In this case the variable titled var in the method Change is assigned a reference to self.variable, and you immediately assign a string to var. It's no longer pointing to self.variable. The following code snippet shows what would happen if you modify the data structure pointed to by var and self.variable, in this case a list:I'm sure someone else could clarify this further.Python’s pass-by-assignment scheme isn’t quite the same as C++’s reference parameters option, but it turns out to be very similar to the argument-passing model of the C language (and others) in practice:As you can state you need to have a mutable object, but let me suggest you to check over the global variables as they can help you or even solve this kind of issue!http://docs.python.org/3/faq/programming.html#what-are-the-rules-for-local-and-global-variables-in-pythonexample:A lot of insights in answers here, but i think an additional point is not clearly mentioned here explicitly.   Quoting from python documentation https://docs.python.org/2/faq/programming.html#what-are-the-rules-for-local-and-global-variables-in-python  "In Python, variables that are only referenced inside a function are implicitly global. If a variable is assigned a new value anywhere within the function’s body, it’s assumed to be a local. If a variable is ever assigned a new value inside the function, the variable is implicitly local, and you need to explicitly declare it as ‘global’.

Though a bit surprising at first, a moment’s consideration explains this. On one hand, requiring global for assigned variables provides a bar against unintended side-effects. On the other hand, if global was required for all global references, you’d be using global all the time. You’d have to declare as global every reference to a built-in function or to a component of an imported module. This clutter would defeat the usefulness of the global declaration for identifying side-effects."Even when passing a mutable object to a function this still applies. And to me clearly explains the reason for the difference in behavior between assigning to the object and operating on the object in the function.gives:The assignment to an global variable that is not declared global therefore creates a new local object and breaks the link to the original object.Here is the simple (I hope) explanation of the concept pass by object used in Python.

Whenever you pass an object to the function, the object itself is passed (object in Python is actually what you'd call a value in other programming languages) not the reference to this object. In other words, when you call:The actual object - [0, 1] (which would be called a value in other programming languages) is being passed. So in fact the function change_me will try to do something like:which obviously will not change the object passed to the function. If the function looked like this:Then the call would result in:which obviously will change the object. This answer explains it well.Aside from all the great explanations on how this stuff works in Python, I don't see a simple suggestion for the problem. As you seem to do create objects and instances, the pythonic way of handling instance variables and changing them is the following:In instance methods, you normally refer to self to access instance attributes. It is normal to set instance attributes in __init__ and read or change them in instance methods. That is also why you pass self als the first argument to def Change.Another solution would be to create a static method like this:There is a little trick to pass an object by reference, even though the language doesn't make it possible. It works in Java too, it's the list with one item. ;-)It's an ugly hack, but it works. ;-PI used the following method to quickly convert a couple of Fortran codes to Python.  True, it's not pass by reference as the original question was posed, but is a simple work around in some cases.While pass by reference is nothing that fits well into python and should be rarely used there are some workarounds that actually can work to get the object currently assigned to a local variable or even reassign a local variable from inside of a called function.The basic idea is to have a function that can do that access and can be passed as object into other functions or stored in a class.One way is to use global (for global variables) or nonlocal (for local variables in a function) in a wrapper function.The same idea works for reading and deleting a variable.For just reading there is even a shorter way of just using lambda: x which returns a callable that when called returns the current value of x. This is somewhat like "call by name" used in languages in the distant past.Passing 3 wrappers to access a variable is a bit unwieldy so those can be wrapped into a class that has a proxy attribute:Pythons "reflection" support makes it possible to get a object that is capable of reassigning a name/variable in a given scope without defining functions explicitly in that scope:Here the ByRef class wraps a dictionary access. So attribute access to wrapped is translated to a item access in the passed dictionary. By passing the result of the builtin locals and the name of a local variable this ends up accessing a local variable. The python documentation as of 3.5 advises that changing the dictionary might not work but it seems to work for me.given the way python handles values and references to them, the only way you can reference an arbitrary instance attribute is by name:in real code you would, of course, add error checking on the dict lookup.Pass-By-Reference in Python is quite different from the concept of pass by reference in C++/Java. Since your example happens to be object-oriented, you could make the following change to achieve a similar result:Since dictionaries are passed by reference, you can use a dict variable to store any referenced values inside it.Since it seems to be nowhere mentioned an approach to simulate references as known from e.g. C++ is to use an "update" function and pass that instead of the actual variable (or rather, "name"):This is mostly useful for "out-only references" or in a situation with multiple threads / processes (by making the update function thread / multiprocessing safe).Obviously the above does not allow reading the value, only updating it.You can merely use an empty class as an instance to store reference objects because internally object attributes are stored in an instance dictionary. See the example.

「Least Astonishment」and the Mutable Default Argument

Stefano Borini

[「Least Astonishment」and the Mutable Default Argument](https://stackoverflow.com/questions/1132941/least-astonishment-and-the-mutable-default-argument)

Anyone tinkering with Python long enough has been bitten (or torn to pieces) by the following issue:Python novices would expect this function to always return a list with only one element: [5]. The result is instead very different, and very astonishing (for a novice):A manager of mine once had his first encounter with this feature, and called it "a dramatic design flaw" of the language. I replied that the behavior had an underlying explanation, and it is indeed very puzzling and unexpected if you don't understand the internals. However, I was not able to answer (to myself) the following question: what is the reason for binding the default argument at function definition, and not at function execution? I doubt the experienced behavior has a practical use (who really used static variables in C, without breeding bugs?)Edit: Baczek made an interesting example. Together with most of your comments and Utaal's in particular, I elaborated further:To me, it seems that the design decision was relative to where to put the scope of parameters: inside the function or "together" with it?Doing the binding inside the function would mean that x is effectively bound to the specified default when the function is called, not defined, something that would present a deep flaw: the def line would be "hybrid" in the sense that part of the binding (of the function object) would happen at definition, and part (assignment of default parameters) at function invocation time.The actual behavior is more consistent: everything of that line gets evaluated when that line is executed, meaning at function definition.

2009-07-15 18:00:37Z

Anyone tinkering with Python long enough has been bitten (or torn to pieces) by the following issue:Python novices would expect this function to always return a list with only one element: [5]. The result is instead very different, and very astonishing (for a novice):A manager of mine once had his first encounter with this feature, and called it "a dramatic design flaw" of the language. I replied that the behavior had an underlying explanation, and it is indeed very puzzling and unexpected if you don't understand the internals. However, I was not able to answer (to myself) the following question: what is the reason for binding the default argument at function definition, and not at function execution? I doubt the experienced behavior has a practical use (who really used static variables in C, without breeding bugs?)Edit: Baczek made an interesting example. Together with most of your comments and Utaal's in particular, I elaborated further:To me, it seems that the design decision was relative to where to put the scope of parameters: inside the function or "together" with it?Doing the binding inside the function would mean that x is effectively bound to the specified default when the function is called, not defined, something that would present a deep flaw: the def line would be "hybrid" in the sense that part of the binding (of the function object) would happen at definition, and part (assignment of default parameters) at function invocation time.The actual behavior is more consistent: everything of that line gets evaluated when that line is executed, meaning at function definition.Actually, this is not a design flaw, and it is not because of internals, or performance.

It comes simply from the fact that functions in Python are first-class objects, and not only a piece of code.As soon as you get to think into this way, then it completely makes sense: a function is an object being evaluated on its definition; default parameters are kind of "member data" and therefore their state may change from one call to the other - exactly as in any other object.In any case, Effbot has a very nice explanation of the reasons for this behavior in Default Parameter Values in Python.

I found it very clear, and I really suggest reading it for a better knowledge of how function objects work.Suppose you have the following codeWhen I see the declaration of eat, the least astonishing thing is to think that if the first parameter is not given, that it will be equal to the tuple ("apples", "bananas", "loganberries")However, supposed later on in the code, I do something likethen if default parameters were bound at function execution rather than function declaration then I would be astonished (in a very bad way) to discover that fruits had been changed.  This would be more astonishing IMO than discovering that your foo function above was mutating the list.The real problem lies with mutable variables, and all languages have this problem to some extent. Here's a question: suppose in Java I have the following code:Now, does my map use the value of the StringBuffer key when it was placed into the map, or does it store the key by reference?  Either way, someone is astonished; either the person who tried to get the object out of the Map using a value identical to the one they put it in with, or the person who can't seem to retrieve their object even though the key they're using is literally the same object that was used to put it into the map (this is actually why Python doesn't allow its mutable built-in data types to be used as dictionary keys).Your example is a good one of a case where Python newcomers will be surprised and bitten.  But I'd argue that if we "fixed" this, then that would only create a different situation where they'd be bitten instead, and that one would be even less intuitive. Moreover, this is always the case when dealing with mutable variables; you always run into cases where someone could intuitively expect one or the opposite behavior depending on what code they're writing.I personally like Python's current approach: default function arguments are evaluated when the function is defined and that object is always the default. I suppose they could special-case using an empty list, but that kind of special casing would cause even more astonishment, not to mention be backwards incompatible.The relevant part of the documentation:I know nothing about the Python interpreter inner workings (and I'm not an expert in compilers and interpreters either) so don't blame me if I propose anything unsensible or impossible.Provided that python objects are mutable I think that this should be taken into account when designing the default arguments stuff.

When you instantiate a list:you expect to get a new list referenced by a.Why should the a=[] ininstantiate a new list on function definition and not on invocation?

It's just like you're asking "if the user doesn't provide the argument then instantiate a new list and use it as if it was produced by the caller".

I think this is ambiguous instead:user, do you want a to default to the datetime corresponding to when you're defining or executing x?

In this case, as in the previous one, I'll keep the same behaviour as if the default argument "assignment" was the first instruction of the function (datetime.now() called on function invocation).

On the other hand, if the user wanted the definition-time mapping he could write:I know, I know: that's a closure. Alternatively Python might provide a keyword to force definition-time binding:Well, the reason is quite simply that bindings are done when code is executed, and the function definition is executed, well... when the functions is defined.Compare this:This code suffers from the exact same unexpected happenstance. bananas is a class attribute, and hence, when you add things to it, it's added to all instances of that class. The reason is exactly the same.It's just "How It Works", and making it work differently in the function case would probably be complicated, and in the class case likely impossible, or at least slow down object instantiation a lot, as you would have to keep the class code around and execute it when objects are created.Yes, it is unexpected. But once the penny drops, it fits in perfectly with how Python works in general. In fact, it's a good teaching aid, and once you understand why this happens, you'll grok python much better.That said it should feature prominently in any good Python tutorial. Because as you mention, everyone runs into this problem sooner or later.I'm really surprised no one has performed the insightful introspection offered by Python (2 and 3 apply) on callables. Given a simple little function func defined as:When Python encounters it, the first thing it will do is compile it in order to create a code object for this function. While this compilation step is done, Python evaluates* and then stores the default arguments (an empty list [] here) in the function object itself. As the top answer mentioned: the list a can now be considered a member of the function func.So, let's do some introspection, a before and after to examine how the list gets expanded inside the function object. I'm using Python 3.x for this, for Python 2 the same applies (use __defaults__ or func_defaults in Python 2; yes, two names for the same thing).After Python executes this definition it will take any default parameters specified (a = [] here) and cram them in the __defaults__ attribute for the function object (relevant section: Callables):     O.k, so an empty list as the single entry in __defaults__, just as expected. Let's now execute this function:Now, let's see those __defaults__ again: Astonished? The value inside the object changes! Consecutive calls to the function will now simply append to that embedded list object:So, there you have it, the reason why this 'flaw' happens, is because default arguments are part of the function object. There's nothing weird going on here, it's all just a bit surprising.The common solution to combat this is to use None as the default and then initialize in the function body:Since the function body is executed anew each time, you always get a fresh new empty list if no argument was passed for a.To further verify that the list in __defaults__ is the same as that used in the function func you can just change your function to return the id of the list a used inside the function body. Then, compare it to the list in __defaults__ (position [0] in __defaults__) and you'll see how these are indeed refering to the same list instance:All with the power of introspection! * To verify that Python evaluates the default arguments during compilation of the function, try executing the following:as you'll notice, input() is called before the process of building the function and binding it to the name bar is made.I used to think that creating the objects at runtime would be the better approach.  I'm less certain now, since you do lose some useful features, though it may be worth it regardless simply to prevent newbie confusion.  The disadvantages of doing so are:1. PerformanceIf call-time evaluation is used, then the expensive function is called every time your function is used without an argument.  You'd either pay an expensive price on each call, or need to manually cache the value externally, polluting your namespace and adding verbosity.2. Forcing bound parametersA useful trick is to bind parameters of a lambda to the current binding of a variable when the lambda is created.  For example:This returns a list of functions that return 0,1,2,3... respectively.  If the behaviour is changed, they will instead bind i to the call-time value of i, so you would get a list of functions that all returned 9.The only way to implement this otherwise would be to create a further closure with the i bound, ie:3. IntrospectionConsider the code:We can get information about the arguments and defaults using the inspect module, which This information is very useful for things like document generation, metaprogramming, decorators etc.Now, suppose the behaviour of defaults could be changed so that this is the equivalent of:However, we've lost the ability to introspect, and see what the default arguments are.  Because the objects haven't been constructed, we can't ever get hold of them without actually calling the function.  The best we could do is to store off the source code and return that as a string.This behavior is easy explained by:So:What you're asking is why this:isn't internally equivalent to this:except for the case of explicitly calling func(None, None), which we'll ignore.In other words, instead of evaluating default parameters, why not store each of them, and evaluate them when the function is called?One answer is probably right there--it would effectively turn every function with default parameters into a closure.  Even if it's all hidden away in the interpreter and not a full-blown closure, the data's got to be stored somewhere.  It'd be slower and use more memory.1)  The so-called problem of "Mutable Default Argument" is in general a special example demonstrating that:

"All functions with this problem suffer also from similar side effect problem on the actual parameter,"

That is against the rules of functional programming, usually undesiderable and should be fixed both together.Example:Solution:  a copy

An absolutely safe solution is to copy or deepcopy the input object first and then to do whatever with the copy.Many builtin mutable types have a copy method like some_dict.copy() or some_set.copy() or can be copied easy like somelist[:] or list(some_list). Every object can be also copied by copy.copy(any_object) or more thorough by copy.deepcopy() (the latter useful if the mutable object is composed from mutable objects). Some objects are fundamentally based on side effects like "file" object and can not be meaningfully reproduced by copy. copyingExample problem for a similar SO questionIt shouldn't be neither saved in any public attribute of an instance returned by this function. (Assuming that private attributes of instance should not be modified from outside of this class or subclasses by convention. i.e. _var1 is a private attribute )Conclusion:

Input parameters objects shouldn't be modified in place (mutated) nor they should not be binded into an object returned by the function. (If we prefere programming without side effects which is strongly recommended. see Wiki about "side effect" (The first two paragraphs are relevent in this context.)

.)2)

Only if the side effect on the actual parameter is required but unwanted on the default parameter then the useful solution is def ...(var1=None): if var1 is None: var1 = [] More..3) In some cases is the mutable behavior of default parameters useful.This actually has nothing to do with default values, other than that it often comes up as an unexpected behaviour when you write functions with mutable default values.No default values in sight in this code, but you get exactly the same problem.The problem is that foo is modifying a mutable variable passed in from the caller, when the caller doesn't expect this. Code like this would be fine if the function was called something like append_5; then the caller would be calling the function in order to modify the value they pass in, and the behaviour would be expected. But such a function would be very unlikely to take a default argument, and probably wouldn't return the list (since the caller already has a reference to that list; the one it just passed in).Your original foo, with a default argument, shouldn't be modifying a whether it was explicitly passed in or got the default value. Your code should leave mutable arguments alone unless it is clear from the context/name/documentation that the arguments are supposed to be modified. Using mutable values passed in as arguments as local temporaries is an extremely bad idea, whether we're in Python or not and whether there are default arguments involved or not.If you need to destructively manipulate a local temporary in the course of computing something, and you need to start your manipulation from an argument value, you need to make a copy.Already busy topic, but from what I read here, the following helped me realizing how it's working internally:It's a performance optimization.  As a result of this functionality, which of these two function calls do you think is faster?I'll give you a hint.  Here's the disassembly (see http://docs.python.org/library/dis.html):As you can see, there is a performance benefit when using immutable default arguments.  This can make a difference if it's a frequently called function or the default argument takes a long time to construct.  Also, bear in mind that Python isn't C.  In C you have constants that are pretty much free.  In Python you don't have this benefit.Default arguments get evaluated at the time the function is compiled into a function object. When used by the function, multiple times by that function, they are and remain the same object. When they are mutable, when mutated (for example, by adding an element to it) they remain mutated on consecutive calls.They stay mutated because they are the same object each time.Since the list is bound to the function when the function object is compiled and instantiated, this:is almost exactly equivalent to this:Here's a demonstration - you can verify that they are the same object each time they are referenced by example.pyand running it with python example.py:This order of execution is frequently confusing to new users of Python. If you understand the Python execution model, then it becomes quite expected. But this is why the usual instruction to new users is to create their default arguments like this instead:This uses the None singleton as a sentinel object to tell the function whether or not we've gotten an argument other than the default. If we get no argument, then we actually want to use a new empty list, [], as the default.As the tutorial section on control flow says:The shortest answer would probably be "definition is execution", therefore the whole argument makes no strict sense. As a more contrived example, you may cite this:Hopefully it's enough to show that not executing the default argument expressions at the execution time of the def statement isn't easy or doesn't make sense, or both.I agree it's a gotcha when you try to use default constructors, though.A simple workaround using NoneThis behavior is not surprising if you take the following into consideration:The role of (2) has been covered extensively in this thread. (1) is likely the astonishment causing factor, as this behavior is not "intuitive" when coming from other languages.(1) is described in the Python tutorial on classes. In an attempt to assign a value to a read-only class attribute:Look back to the original example and consider the above points:Here foo is an object and a is an attribute of foo (available at foo.func_defs[0]). Since a is a list, a is mutable and is thus a read-write attribute of foo. It is initialized to the empty list as specified by the signature when the function is instantiated, and is available for reading and writing as long as the function object exists. Calling foo without overriding a default uses that default's value from foo.func_defs. In this case, foo.func_defs[0] is used for a within function object's code scope. Changes to a change foo.func_defs[0], which is part of the foo object and persists between execution of the code in foo.Now, compare this to the example from the documentation on emulating the default argument behavior of other languages, such that the function signature defaults are used every time the function is executed:Taking (1) and (2) into account, one can see why this accomplishes the the desired behavior: I am going to demonstrate an alternative structure to pass a default list value to a function (it works equally well with dictionaries).  As others have extensively commented, the list parameter is bound to the function when it is defined as opposed to when it is executed.  Because lists and dictionaries are mutable, any alteration to this parameter will affect other calls to this function.  As a result, subsequent calls to the function will receive this shared list which may have been altered by any other calls to the function.  Worse yet, two parameters are using this function's shared parameter at the same time oblivious to the changes made by the other.Wrong Method (probably...):You can verify that they are one and the same object by using id:Per Brett Slatkin's "Effective Python: 59 Specific Ways to Write Better Python", Item 20: Use None and Docstrings to specify dynamic default arguments (p. 48)This implementation ensures that each call to the function either receives the default list or else the list passed to the function.Preferred Method:There may be legitimate use cases for the 'Wrong Method' whereby the programmer intended the default list parameter to be shared, but this is more likely the exception than the rule.The solutions here are:The second option is nice because users of the function can pass in a callable, which may be already existing (such as a type)When we do this:... we assign the argument a to an unnamed list, if the caller does not pass the value of a.To make things simpler for this discussion, let's temporarily give the unnamed list a name. How about pavlo ?At any time, if the caller doesn't tell us what a is, we reuse pavlo.If pavlo is mutable (modifiable), and foo ends up modifying it, an effect we notice the next time foo is called without specifying a.So this is what you see (Remember, pavlo is initialized to []):Now, pavlo is [5].Calling foo() again modifies pavlo again:Specifying a when calling foo() ensures pavlo is not touched.So, pavlo is still [5, 5].I sometimes exploit this behavior as an alternative to the following pattern:If singleton is only used by use_singleton, I like the following pattern as a replacement:I've used this for instantiating client classes that access external resources, and also for creating dicts or lists for memoization.Since I don't think this pattern is well known, I do put a short comment in to guard against future misunderstandings.You can get round this by replacing the object (and therefore the tie with the scope):Ugly, but it works.It may be true that:it is entirely consistent to hold to both of the features above and still make another point:The other answers, or at least some of them either make points 1 and 2 but not 3, or make point 3 and downplay points 1 and 2. But all three are true.It may be true that switching horses in midstream here would be asking for significant breakage, and that there could be more problems created by changing Python to intuitively handle Stefano's opening snippet. And it may be true that someone who knew Python internals well could explain a minefield of consequences. However,The existing behavior is not Pythonic, and Python is successful because very little about the language violates the principle of least astonishment anywhere near this badly. It is a real problem, whether or not it would be wise to uproot it. It is a design flaw. If you understand the language much better by trying to trace out the behavior, I can say that C++ does all of this and more; you learn a lot by navigating, for instance, subtle pointer errors. But this is not Pythonic: people who care about Python enough to persevere in the face of this behavior are people who are drawn to the language because Python has far fewer surprises than other language. Dabblers and the curious become Pythonistas when they are astonished at how little time it takes to get something working--not because of a design fl--I mean, hidden logic puzzle--that cuts against the intuitions of programmers who are drawn to Python because it Just Works.This is not a design flaw. Anyone who trips over this is doing something wrong.There are 3 cases I see where you might run into this problem:The example in the question could fall into category 1 or 3. It's odd that it both modifies the passed list and returns it; you should pick one or the other.This "bug" gave me a lot of overtime work hours! But I'm beginning to see a potential use of it (but I would have liked it to be at the execution time, still)I'm gonna give you what I see as a useful example.prints the followingJust change the function to be:I think the answer to this question lies in how python pass data to parameter (pass by value or by reference), not mutability or how python handle the "def" statement.A brief introduction. First, there are two type of data types in python, one is simple elementary data type, like numbers, and another data type is objects. Second, when passing data to parameters, python pass elementary data type by value, i.e., make a local copy of the value to a local variable, but pass object by reference, i.e., pointers to the object.Admitting the above two points, let's explain what happened to the python code. It's only because of passing by reference for objects, but has nothing to do with mutable/immutable, or arguably the fact that "def" statement is executed only once when it is defined.[] is an object, so python pass the reference of [] to a, i.e., a is only a pointer to [] which lies in memory as an object. There is only one copy of [] with, however, many references to it. For the first foo(), the list [] is changed to 1 by append method. But Note that there is only one copy of the list object and this object now becomes 1. When running the second foo(), what effbot webpage says (items is not evaluated any more) is wrong. a is evaluated to be the list object, although now the content of the object is 1. This is the effect of passing by reference! The result of foo(3) can be easily derived in the same way.To further validate my answer, let's take a look at two additional codes.====== No. 2 ========[] is an object, so is None (the former is mutable while the latter is immutable. But the mutability has nothing to do with the question). None is somewhere in the space but we know it's there and there is only one copy of None there. So every time foo is invoked, items is evaluated (as opposed to some answer that it is only evaluated once) to be None, to be clear, the reference (or the address) of None. Then in the foo, item is changed to [], i.e., points to another object which has a different address. ====== No. 3 =======The invocation of foo(1) make items point to a list object [] with an address, say, 11111111. the content of the list is changed to 1 in the foo function in the sequel, but the address is not changed, still 11111111. Then foo(2,[]) is coming. Although the [] in foo(2,[]) has the same content as the default parameter [] when calling foo(1), their address are different! Since we provide the parameter explicitly, items has to take the address of this new [], say 2222222, and return it after making some change. Now foo(3) is executed. since only x is provided, items has to take its default value again. What's the default value? It is set when defining the foo function: the list object located in 11111111. So the items is evaluated to be the address 11111111 having an element 1. The list located at 2222222 also contains one element 2, but it is not pointed by items any more. Consequently, An append of 3 will make items [1,3]. From the above explanations, we can see that the effbot webpage recommended in the accepted answer failed to give a relevant answer to this question. What is more, I think a point in the effbot webpage is wrong. I think the code regarding the UI.Button is correct:Each button can hold a distinct callback function which will display different value of i. I can provide an example to show this:If we execute x[7]() we'll get 7 as expected, and x[9]() will gives 9, another value of i.TLDR: Define-time defaults are consistent and strictly more expressive.Defining a function affects two scopes: the defining scope containing the function, and the execution  scope contained by the function. While it is pretty clear how blocks map to scopes, the question is where def <name>(<args=defaults>): belongs to:The def name part must evaluate in the defining scope - we want name to be available there, after all. Evaluating the function only inside itself would make it inaccessible.Since parameter is a constant name, we can "evaluate" it at the same time as def name. This also has the advantage it produces the function with a known signature as name(parameter=...):, instead of a bare name(...):.Now, when to evaluate default?Consistency already says "at definition": everything else of def <name>(<args=defaults>): is best evaluated at definition as well. Delaying parts of it would be the astonishing choice.The two choices are not equivalent, either: If default is evaluated at definition time, it can still affect execution time. If default is evaluated at execution time, it cannot affect definition time. Choosing "at definition" allows expressing both cases, while choosing "at execution" can express only one:Every other answer explains why this is actually a nice and desired behavior, or why you shouldn't be needing this anyway. Mine is for those stubborn ones who want to exercise their right to bend the language to their will, not the other way around.We will "fix" this behavior with a decorator that will copy the default value instead of reusing the same instance for each positional argument left at its default value.Now let's redefine our function using this decorator:This is particularly neat for functions that take multiple arguments. Compare:withIt's important to note that the above solution breaks if you try to use keyword args, like so:The decorator could be adjusted to allow for that, but we leave this as an exercise for the reader ;)
