3.rd use types.FunctionType(...) to convert the imported and bound method to a function

you can also pass on the current global variables, as the reloaded method would be in a different namespace

4.th now you can continue as suggested by "Jason Pratt" 

  using the types.MethodType(...)Example:If it can be of any help, I recently released a Python library named Gorilla to make the process of monkey patching more convenient.Using a function needle() to patch a module named guineapig goes as follows:But it also takes care of more interesting use cases as shown in the FAQ from the documentation.The code is available on GitHub.This question was opened years ago, but hey, there's an easy way to simulate the binding of a function to a class instance using decorators:There, when you pass the function and the instance to the binder decorator, it will create a new function, with the same code object as the first one. Then, the given instance of the class is stored in an attribute of the newly created function. The decorator return a (third) function calling automatically the copied function, giving the instance as the first parameter.

  

  

In conclusion you get a function simulating it's binding to the class instance. Letting the original function unchanged.What Jason Pratt posted is correct.As you can see, Python doesn't consider b() any different than a(). In Python all methods are just variables that happen to be functions. I find it strange that nobody mentioned that all of the methods listed above creates a cycle reference between the added method and the instance, causing the object to be persistent till garbage collection. There was an old trick adding a descriptor by extending the class of the object:With this, you can use the self pointer

How to sort (list/tuple) of lists/tuples by the element at a given index?

Stan

[How to sort (list/tuple) of lists/tuples by the element at a given index?](https://stackoverflow.com/questions/3121979/how-to-sort-list-tuple-of-lists-tuples-by-the-element-at-a-given-index)

I have some data either in a list of lists or a list of tuples, like this:And I want to sort by the 2nd element in the subset. Meaning, sorting by 2,5,8 where 2 is from (1,2,3), 5 is from (4,5,6). What is the common way to do this? Should I store tuples or lists in my list?

2010-06-25 23:01:41Z

I have some data either in a list of lists or a list of tuples, like this:And I want to sort by the 2nd element in the subset. Meaning, sorting by 2,5,8 where 2 is from (1,2,3), 5 is from (4,5,6). What is the common way to do this? Should I store tuples or lists in my list?or:I just want to add to Stephen's answer if you want to sort the array from high to low, another way other than in the comments above is just to add this to the line: and the result will be as follows:For sorting by multiple criteria, namely for instance by the second and third elements in a tuple, letand so define a lambda that returns a tuple that describes priority, for instanceStephen's answer is the one I'd use.  For completeness, here's the DSU (decorate-sort-undecorate) pattern with list comprehensions:Or, more tersely:As noted in the Python Sorting HowTo, this has been unnecessary since Python 2.4, when key functions became available.In order to sort a list of tuples (<word>, <count>), for count in descending order and word in alphabetical order:I use this method:and it gives me the result:Without lambda:itemgetter() is somewhat faster than lambda tup: tup[1], but the increase is relatively modest (around 10 to 25 percent).(IPython session)@Stephen 's answer is to the point! Here is an example for better visualization,Shout out for the Ready Player One fans! =)key is a function that will be called to transform the collection's items for comparison.. like compareTo method in Java. The parameter passed to key must be something that is callable. Here, the use of lambda creates an anonymous function (which is a callable).

The syntax of lambda is the word lambda followed by a iterable name then a single block of code. Below example, we are sorting a list of tuple that holds the info abt time of certain event and actor name. We are sorting this list by time of event occurrence - which is the 0th element of a tuple.Note - s.sort([cmp[, key[, reverse]]])  sorts the items of s in placeSorting a tuple is quite simple:

How do I protect Python code?

Jordfräs

[How do I protect Python code?](https://stackoverflow.com/questions/261638/how-do-i-protect-python-code)

I am developing a piece of software in Python that will be distributed to my employer's customers. My employer wants to limit the usage of the software with a time restricted license file.If we distribute the .py files or even .pyc files it will be easy to (decompile and) remove the code that checks the license file.Another aspect is that my employer does not want the code to be read by our customers, fearing that the code may be stolen or at least the "novel ideas".Is there a good way to handle this problem? Preferably with an off-the-shelf solution.The software will run on Linux systems (so I don't think py2exe will do the trick).

2008-11-04 11:57:27Z

I am developing a piece of software in Python that will be distributed to my employer's customers. My employer wants to limit the usage of the software with a time restricted license file.If we distribute the .py files or even .pyc files it will be easy to (decompile and) remove the code that checks the license file.Another aspect is that my employer does not want the code to be read by our customers, fearing that the code may be stolen or at least the "novel ideas".Is there a good way to handle this problem? Preferably with an off-the-shelf solution.The software will run on Linux systems (so I don't think py2exe will do the trick).Python, being a byte-code-compiled interpreted language, is very difficult to lock down.  Even if you use a exe-packager like py2exe, the layout of the executable is well-known, and the Python byte-codes are well understood.Usually in cases like this, you have to make a tradeoff.  How important is it really to protect the code?  Are there real secrets in there (such as a key for symmetric encryption of bank transfers), or are you just being paranoid?  Choose the language that lets you develop the best product quickest, and be realistic about how valuable your novel ideas are.If you decide you really need to enforce the license check securely, write it as a small C extension so that the license check code can be extra-hard (but not impossible!) to reverse engineer, and leave the bulk of your code in Python."Is there a good way to handle this problem?"  No.  Nothing can be protected against reverse engineering.  Even the firmware on DVD machines has been reverse engineered and AACS Encryption key exposed.  And that's in spite of the DMCA making that a criminal offense.Since no technical method can stop your customers from reading your code, you have to apply ordinary commercial methods.You must use the right tool to do the right thing, and Python was not designed to be obfuscated. It's the contrary; everything is open or easy to reveal or modify in Python because that's the language's philosophy.If you want something you can't see through, look for another tool. This is not a bad thing, it is important that several different tools exist for different usages.Even compiled programs can be reverse-engineered so don't think that you can fully protect any code. You can analyze obfuscated PHP, break the flash encryption key, etc. Newer versions of Windows are cracked every time.You cannot prevent somebody from misusing your code, but you can easily discover if someone does. Therefore, it's just a casual legal issue.Nowadays, business models tend to go for selling services instead of products. You cannot copy a service, pirate nor steal it. Maybe it's time to consider to go with the flow...Sensible idea: Use Cython, Nuitka, Shed Skin or something similar to compile python to C code, then distribute your app as python binary libraries (pyd) instead.That way, no Python (byte) code is left and you've done any reasonable amount of obscurification anyone (i.e. your employer) could expect from regular Code, I think. (.NET or Java less safe than this case, as that bytecode is not obfuscated and can relatively easily be decompiled into reasonable source.)Cython is getting more and more compatible with CPython, so I think it should work. (I'm actually considering this for our product.. We're already building some thirdparty libs as pyd/dlls, so shipping our own python code as binaries is not a overly big step for us.)See This Blog Post (not by me) for a tutorial on how to do it. (thx @hithwen)Crazy idea:You could probably get Cython to store the C-files separately for each module, then just concatenate them all and build them with heavy inlining. That way, your Python module is pretty monolithic and difficult to chip at with common tools.Beyond crazy:You might be able to build a single executable if you can link to (and optimize with) the python runtime and all libraries (dlls) statically. That way, it'd sure be difficult to intercept calls to/from python and whatever framework libraries you use. This cannot be done if you're using LGPL code though.I understand that you want your customers to use the power of python but do not want expose the source code.Here are my suggestions:(a) Write the critical pieces of the code as C or C++ libraries and then use SIP or swig to expose the C/C++ APIs to Python namespace.(b) Use cython instead of Python(c) In both (a) and (b), it should be possible to distribute the libraries as licensed binary with a Python interface.Is your employer aware that he can "steal" back any ideas that other people get from your code? I mean, if they can read your work, so can you theirs. Maybe looking at how you can benefit from the situation would yield a better return of your investment than fearing how much you could lose.[EDIT] Answer to Nick's comment:Nothing gained and nothing lost. The customer has what he wants (and paid for it since he did the change himself). Since he doesn't release the change, it's as if it didn't happen for everyone else.Now if the customer sells the software, they have to change the copyright notice (which is illegal, so you can sue and will win -> simple case).If they don't change the copyright notice, the 2nd level customers will notice that the software comes from you original and wonder what is going on. Chances are that they will contact you and so you will learn about the reselling of your work.Again we have two cases: The original customer sold only a few copies. That means they didn't make much money anyway, so why bother. Or they sold in volume. That means better chances for you to learn about what they do and do something about it.But in the end, most companies try to comply to the law (once their reputation is ruined, it's much harder to do business). So they will not steal your work but work with you to improve it. So if you include the source (with a license that protects you from simple reselling), chances are that they will simply push back changes they made since that will make sure the change is in the next version and they don't have to maintain it. That's win-win: You get changes and they can make the change themselves if they really, desperately need it even if you're unwilling to include it in the official release.Have you had a look at pyminifier? It does Minify, obfuscate, and compress Python code. The example code looks pretty nasty for casual reverse engineering.Do not rely on obfuscation. As You have correctly concluded, it offers very limited protection.

UPDATE: Here is a link to paper which reverse engineered obfuscated python code in Dropbox. The approach - opcode remapping is a good barrier, but clearly it can be defeated.Instead, as many posters have mentioned make it:Alternatively, as the kick-ass Python IDE WingIDE does: Give away the code. That's right, give the code away and have people come back for upgrades and support.Shipping .pyc files has its problems - they are not compatible with any other python version than the python version they were created with, which means you must know which python version is running on the systems the product will run on. That's a very limiting factor.Use Cython. It will compile your modules to high-performant C files, which can then be compiled to native binary libraries. This is basically un-reversable, compared to .pyc bytecode!I've written a detailed article on how to set up Cython for a Python project, check it out:Protecting Python Sources With CythonIn some circumstances, it may be possible to move (all, or at least a key part) of the software into a web service that your organization hosts.That way, the license checks can be performed in the safety of your own server room.Though there's no perfect solution, the following can be done:If the call to the native code were to be removed, the program wouldn't start anyway. If it's not removed then the license will be enforced.Though this is not a cross-platform or a pure-Python solution, it will work.I think there is one more method to protect your Python code; part of the Obfuscation method. I believe there was a game like Mount and Blade or something that changed and recompiled their own python interpreter (the original interpreter which i believe is open source) and just changed the OP codes in the OP code table to be different then the standard python OP codes.So the python source is unmodified but the file extensions of the *.pyc files are different and the op codes don't match to the public python.exe interpreter. If you checked the games data files all the data was in Python source format.All sorts of nasty tricks can be done to mess with immature hackers this way. Stopping a bunch of inexperienced hackers is easy. It's the professional hackers that you will not likely beat. But most companies don't keep pro hackers on staff long I imagine (likely because things get hacked). But immature hackers are all over the place (read as curious IT staff).You could for example, in a modified interpreter, allow it to check for certain comments or doc strings in your source. You could have special OP codes for such lines of code. For example:OP 234 is for source line "# Copyright I wrote this"

or compile that line into op codes that are equivalent to "if False:" if "# Copyright" is missing. Basically disabling a whole block of code for what appears to be some obscure reason.One use case where recompiling a modified interpreter may be feasible is where you didn't write the app, the app is big, but you are paid to protect it, such as when you're a dedicated server admin for a financial app.I find it a little contradictory to leave the source or opcodes open for eyeballs, but use SSL for network traffic. SSL is not 100% safe either. But it's used to stop MOST eyes from reading it. A wee bit precaution is sensible.Also, if enough people deem that Python source and opcodes are too visible, it's likely someone will eventually develop at least a simple protection tool for it. So the more people asking "how to protect Python app" only promotes that development.The reliable only way to protect code is to run it on a server you control and provide your clients with a client which interfaces with that server.I was surprised in not seeing pyconcrete in any answer. Maybe because it's newer than the question?It could be exactly what you need(ed).Instead of obfuscating the code, it encrypts it and decrypts at load time.From pypi page:Depending in who the client is, a simple protection mechanism, combined with a sensible license agreement will be far more effective than any complex licensing/encryption/obfuscation system.The best solution would be selling the code as a service, say by hosting the service, or offering support - although that isn't always practical.Shipping the code as .pyc files will prevent your protection being foiled by a few #s, but it's hardly effective anti-piracy protection (as if there is such a technology), and at the end of the day, it shouldn't achieve anything that a decent license agreement with the company will.Concentrate on making your code as nice to use as possible - having happy customers will make your company far more money than preventing some theoretical piracy..Another attempt to make your code harder to steal is to use jython and then use java obfuscator. This should work pretty well as jythonc translate python code to java and then java is compiled to bytecode. So ounce you obfuscate the classes it will be really hard to understand what is going on after decompilation, not to mention recovering the actual code. The only problem with jython is that you can't use python modules written in c.What about signing your code with standard encryption schemes by hashing and signing important files and checking it with public key methods?In this way you can issue license file with a public key for each customer.Additional you can use an python obfuscator like this one (just googled it).You should take a look at how the guys at getdropbox.com do it for their client software, including Linux. It's quite tricky to crack and requires some quite creative disassembly to get past the protection mechanisms.The best you can do with Python is to obscure things.You may be able to add some additional obscurity by encrypting part of it and decrypting it on the fly and passing it to eval(). But no matter what you do someone can break it.None of this will stop a determined attacker from disassembling the bytecode or digging through your api with help, dir, etc.Idea of having time restricted license and check for it in locally installed program will not work. Even with perfect obfuscation, license check can be removed. However if you check license on remote system and run significant part of the program on your closed remote system, you will be able to protect your IP.Preventing competitors from using the source code as their own or write their inspired version of the same code, one way to protect is to add signatures to your program logic (some secrets to be able to prove that code was stolen from you) and obfuscate the python source code so, it's hard to read and utilize. Good obfuscation adds basically the same protection to your code, that compiling it to executable (and stripping binary) does. Figuring out how obfuscated complex code works might be even harder than actually writing your own implementation. This will not help preventing hacking of your program. Even with obfuscation code license stuff will be cracked and program may be modified to have slightly different behaviour (in the same way that compiling code to binary does not help protection of native programs). In addition to symbol obfuscation might be good idea to unrefactor the code, which makes everything even more confusing if e.g. call graphs points to many different places even if actually those different places does eventually the same thing. Logical signature inside obfuscated code (e.g. you may create table of values which are used by program logic, but also used as signature), which can be used to determine that code is originated from you. If someone decides to use your obfuscated code module as part of their own product (even after reobfuscating it to make it seem different) you can show, that code is stolen with your secret signature.I have looked at software protection in general for my own projects and the general philosophy is that complete protection is impossible.  The only thing that you can hope to achieve is to add protection to a level that would cost your customer more to bypass than it would to purchase another license.With that said I was just checking google for python obsfucation and not turning up a lot of anything.  In a .Net solution, obsfucation would be a first approach to your problem on a windows platform, but I am not sure if anyone has solutions on Linux that work with Mono.  The next thing would be to write your code in a compiled language, or if you really want to go all the way, then in assembler.  A stripped out executable would be a lot harder to decompile than an interpreted language.It all comes down to tradeoffs.  On one end you have ease of software development in python, in which it is also very hard to hide secrets.  On the other end you have software written in assembler which is much harder to write, but is much easier to hide secrets.Your boss has to choose a point somewhere along that continuum that supports his requirements.  And then he has to give you the tools and time so you can build what he wants.   However my bet is that he will object to real development costs versus potential monetary losses.Long story short:For more detail, look this answer.If you are interested in the topic, this project will help you - pyprotect.It is possible to have the py2exe byte-code in a crypted resource for a C launcher that loads and executes it in memory. Some ideas here and here.Some have also thought of a self modifying program to make reverse engineering expensive.You can also find tutorials for preventing debuggers, make the disassembler fail, set false debugger breakpoints and protect your code with checksums. Search for ["crypted code"  execute "in memory"] for more links.But as others already said, if your code is worth it, reverse engineers will succeed in the end.If we focus on software licensing, I would recommend to take a look at another Stack Overflow answer I wrote here to get some inspiration of how a license key verification system can be constructed.There is an open-source library on GitHub that can help you with the license verification bit.You can install it by pip install licensing and then add the following code:You can read more about the way the RSA public key, etc are configured here.Use the same way to protect binary file of c/c++, that is, obfuscate each function body in executable or library binary file, insert an instruction "jump" at the begin of each function entry, jump to special function to restore obfuscated code. Byte-code is binary code of Python script, so Those obfuscated file (.pyc or .pyo) can be used by normal python interpreter, when those code object is called first timeThere is a tool Pyarmor to obfuscate python scripts by this way.using cxfreeze ( py2exe for linux ) will do the job.http://cx-freeze.sourceforge.net/it is available in ubuntu repositoriesThere is a comprehensive answer on concealing the python source code, which can be find here.Possible techniques discussed are:

- use compiled bytecode (python -m compileall)

- executable creators (or installers like PyInstaller)

- software as an service (the best solution to conceal your code in my opinion)

- python source code obfuscators

How do you get the logical xor of two variables in Python?

Zach Hirsch

[How do you get the logical xor of two variables in Python?](https://stackoverflow.com/questions/432842/how-do-you-get-the-logical-xor-of-two-variables-in-python)

How do you get the logical xor of two variables in Python?For example, I have two variables that I expect to be strings. I want to test that only one of them contains a True value (is not None or the empty string):The ^ operator seems to be bitwise, and not defined on all objects:

2009-01-11 12:34:43Z

How do you get the logical xor of two variables in Python?For example, I have two variables that I expect to be strings. I want to test that only one of them contains a True value (is not None or the empty string):The ^ operator seems to be bitwise, and not defined on all objects:If you're already normalizing the inputs to booleans, then != is xor.You can always use the definition of xor to compute it from other logical operations:But this is a little too verbose for me, and isn't particularly clear at first glance. Another way to do it is:The xor operator on two booleans is logical xor (unlike on ints, where it's bitwise). Which makes sense, since bool is just a subclass of int, but is implemented to only have the values 0 and 1. And logical xor is equivalent to bitwise xor when the domain is restricted to 0 and 1.So the logical_xor function would be implemented like:Credit to Nick Coghlan on the Python-3000 mailing list.Bitwise exclusive-or is already built-in to Python, in the operator module (which is identical to the ^ operator):As Zach explained, you can use:Personally, I favor a slightly different dialect:This dialect is inspired from a logical diagramming language I learned in school where "OR" was denoted by a box containing ≥1 (greater than or equal to 1) and "XOR" was denoted by a box containing =1.This has the advantage of correctly implementing exclusive or on multiple operands.To keep most of that way of thinking, my logical xor definintion would be:That way it can return a, b, or False:I've tested several approaches and not a != (not b) appeared to be the fastest.Here are some testsEdit:

Examples 1 and 3 above are missing parenthes so result is incorrect. New results + truth() function as ShadowRanger suggested.Rewarding thread:Anoder idea... Just you try the (may be) pythonic expression «is not» in order to get behavior of logical «xor»The truth table would be:And for your example string:However; as they indicated above, it depends of the actual behavior you want to pull out about any couple strings, because strings aren't boleans... and even more: if you «Dive Into Python» you will find «The Peculiar Nature of "and" and "or"»

http://www.diveintopython.net/power_of_introspection/and_or.htmlSorry my writed English, it's not my born language.Regards.As I don't see the simple variant of xor using variable arguments and only operation on Truth values True or False, I'll just throw it here for anyone to use.

It's as noted by others, pretty (not to say very) straightforward.And usage is straightforward as well:As this is the generalized n-ary logical XOR, it's truth value will be True whenever the number of True operands is odd (and not only when exactly one is True, this is just one case in which n-ary XOR is True).Thus if you are in search of a n-ary predicate that is only True when exactly one of it's operands is, you might want to use:Exclusive Or is defined as followsSometimes I find myself working with 1 and 0 instead of boolean True and False values.  In this case xor can be defined as which has the following truth table:I know this is late, but I had a thought and it might be worth, just for documentation. Perhaps this would work:np.abs(x-y) The idea is that Python has a bitwise exclusive-OR operator, it's ^:You can use it by converting the inputs to booleans before applying xor (^):(Edited - thanks Arel)Simple, easy to understand:If an exclusive choice is what you're after, it can be expanded to multiple arguments:How about this?will give a if b is false

will give b if a is false

will give False otherwiseOr with the Python 2.5+ ternary expression:Some of the implementations suggested here will cause repeated evaluation of the operands  in some cases, which may lead to unintended side effects and therefore must be avoided.That said, a xor implementation that returns either True or False is fairly simple; one that returns one of the operands, if possible, is much trickier, because no consensus exists as to which operand should be the chosen one, especially when there are more than two operands. For instance, should xor(None, -1, [], True) return None, [] or False? I bet each answer appears to some people as the most intuitive one.For either the True- or the False-result, there are as many as five possible choices: return first operand (if it matches end result in value, else boolean), return first match (if at least one exists, else boolean), return last operand (if ... else ...), return last match (if ... else ...), or always return boolean. Altogether, that's 5 ** 2 = 25 flavors of xor.Many folks, including myself, need an xor function that behaves like an n-input xor circuit, where n is variable.  (See https://en.wikipedia.org/wiki/XOR_gate).  The following simple function implements this.Sample I/O follows:It's easy when you know what XOR does:This gets the logical exclusive XOR for two (or more) variablesThe first problem with this setup is that it most likely traverses the whole list twice and, at a minimum, will check at least one of the elements twice.  So it may increase code comprehension, but it doesn't lend to speed (which may differ negligibly depending on your use case).   The second problem with this setup is that it checks for exclusivity regardless of the number of variables.  This is may at first be regarded as a feature, but the first problem becomes a lot more significant as the number of variables increases (if they ever do).Xor is ^ in Python. It returns :If you intend to use them on strings anyway, casting them in bool makes your operation unambiguous (you could also mean set(str1) ^ set(str2)).To get the logical xor of two or more variables in Python:For example,When you convert the inputs to booleans, bitwise xor becomes logical xor.Note that the accepted answer is wrong: != is not the same as xor in Python because of the subtlety of operator chaining.For instance, the xor of the three values below is wrong when using !=:(P.S. I tried editing the accepted answer to include this warning, but my change was rejected.)XOR is implemented in operator.xor.We can easily find xor of two variables by the using:Example:

Convert list of dictionaries to a pandas DataFrame

appleLover

[Convert list of dictionaries to a pandas DataFrame](https://stackoverflow.com/questions/20638006/convert-list-of-dictionaries-to-a-pandas-dataframe)

I have a list of dictionaries like this:And I want to turn this into a pandas DataFrame like this:Note: Order of the columns does not matter.How can I turn the list of dictionaries into a pandas DataFrame as shown above?

2013-12-17 15:24:51Z

I have a list of dictionaries like this:And I want to turn this into a pandas DataFrame like this:Note: Order of the columns does not matter.How can I turn the list of dictionaries into a pandas DataFrame as shown above?Supposing d is your list of dicts, simply:The other answers are correct, but not much has been explained in terms of advantages and limitations of these methods. The aim of this post will be to show examples of these methods under different situations, discuss when to use (and when not to use), and suggest alternatives.Depending on the structure and format of your data, there are situations where either all three methods work, or some work better than others, or  some don't work at all. Consider a very contrived example.This list consists of "records" with every keys present. This is the simplest case you could encounter.Before continuing, it is important to make the distinction between the different types of dictionary orientations, and support with pandas. There are two primary types: "columns", and "index".orient='columns'

Dictionaries with the "columns" orientation will have their keys correspond to columns in the equivalent DataFrame. For example, data above is in the "columns" orient.Note: If you are using pd.DataFrame.from_records, the orientation is assumed to be "columns" (you cannot specify otherwise), and the dictionaries will be loaded accordingly.orient='index'

With this orient, keys are assumed to correspond to index values. This kind of data is best suited for pd.DataFrame.from_dict.This case is not considered in the OP, but is still useful to know.If you need a custom index on the resultant DataFrame, you can set it using the index=... argument.This is not supported by pd.DataFrame.from_dict.All methods work out-of-the-box when handling dictionaries with missing keys/column values. For example,"What if I don't want to read in every single column"? You can easily specify this using the columns=... parameter.For example, from the example dictionary of data2 above, if you wanted to read only columns "A', 'D', and 'F', you can do so by passing a list:This is not supported by pd.DataFrame.from_dict with the default orient "columns".Not supported by any of these methods directly. You will have to iterate over your data and perform a reverse delete in-place as you iterate. For example, to extract only the 0th and 2nd rows from data2 above, you can use:A strong, robust alternative to the methods outlined above is the json_normalize function which works with lists of dictionaries (records), and in addition can also handle nested dictionaries.Again, keep in mind that the data passed to json_normalize needs to be in the list-of-dictionaries (records) format.As mentioned, json_normalize can also handle nested dictionaries. Here's an example taken from the documentation.For more information on the meta and record_path arguments, check out the documentation.Here's a table of all the methods discussed above, along with supported features/functionality.* Use orient='columns' and then transpose to get the same effect as orient='index'.In pandas 16.2, I had to do pd.DataFrame.from_records(d) to get this to work.You can also use pd.DataFrame.from_dict(d) as :I know a few people will come across this and find nothing here helps. The easiest way I have found to do it is like this:Hope this helps someone!and simple call:

Best way to strip punctuation from a string

Lawrence Johnston

[Best way to strip punctuation from a string](https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string)

It seems like there should be a simpler way than:Is there?

2008-11-05 17:30:32Z

It seems like there should be a simpler way than:Is there?From an efficiency perspective, you're not going to beat For higher versions of Python use the following code:It's performing raw string operations in C with a lookup table - there's not much that will beat that but writing your own C code.If speed isn't a worry, another option though is:This is faster than s.replace with each char, but won't perform as well as non-pure python approaches such as regexes or string.translate, as you can see from the below timings.  For this type of problem, doing it at as low a level as possible pays off.Timing code:This gives the following results:Regular expressions are simple enough, if you know them. For the convenience of usage, I sum up the note of striping punctuation from a string in both Python 2 and Python 3. Please refer to other answers for the detailed description.Python 2Python 3I usually use something like this:Not necessarily simpler, but a different way, if you are more familiar with the re family. string.punctuation is ASCII only! A more correct (but also much slower) way is to use the unicodedata module:You can generalize and strip other types of characters as well:It will also strip characters like ~*+§$ which may or may not be "punctuation" depending on one's point of view.For Python 3 str or Python 2 unicode values, str.translate() only takes a dictionary; codepoints (integers) are looked up in that mapping and anything mapped to None is removed.To remove (some?) punctuation then, use:The dict.fromkeys() class method makes it trivial to create the mapping, setting all values to None based on the sequence of keys.To remove all punctuation, not just ASCII punctuation, your table needs to be a little bigger; see J.F. Sebastian's answer (Python 3 version):string.punctuation misses loads of punctuation marks that are commonly used in the real world. How about a solution that works for non-ASCII punctuation?Personally, I believe this is the best way to remove punctuation from a string in Python because:This uses Unicode character properties, which you can read more about on Wikipedia.Here's a one-liner for Python 3.5:I haven't seen this answer yet. Just use a regex; it removes all characters besides word characters (\w) and number characters (\d), followed by a whitespace character (\s):This might not be the best solution however this is how I did it.Here is a function I wrote. It's not very efficient, but it is simple and you can add or remove any punctuation that you desire:Just as an update, I rewrote the @Brian example in Python 3 and made changes to it to move regex compile step inside of the function. My thought here was to time every single step needed to make the function work. Perhaps you are using distributed computing and can't have regex object shared between your workers and need to have re.compile step at each worker. Also, I was curious to time two different implementations of maketrans for Python 3vs Plus I added another method to use set, where I take advantage of intersection function to reduce number of iterations.This is the complete code:This is my results:Here's a solution without regex.A one-liner might be helpful in not very strict cases:Why none of you use this?Too slow?Remove stop words from the text file using Python I like to use a function like this:

How to read/process command line arguments?

Ayman Hourieh

[How to read/process command line arguments?](https://stackoverflow.com/questions/1009860/how-to-read-process-command-line-arguments)

I am originally a C programmer. I have seen numerous tricks and "hacks" to read many different arguments. What are some of the ways Python programmers can do this?

2009-06-17 22:38:30Z

I am originally a C programmer. I have seen numerous tricks and "hacks" to read many different arguments. What are some of the ways Python programmers can do this?The canonical solution in the standard library is argparse (docs):Here is an example:argparse supports (among other things):sys.argv is a list that contains all the arguments passed to the script on the command line.Basically,Just going around evangelizing for argparse which is better for these reasons.. essentially:(copied from the link)And my personal favorite:There is also argparse stdlib module (an "impovement" on stdlib's optparse module). Example from the introduction to argparse:Usage:One way to do it is using sys.argv. This will print the script name as the first argument and all the other parameters that you pass to it.The docopt library is really slick.  It builds an argument dict from the usage string for your app.Eg from the docopt readme:If you need something fast and not very flexiblemain.py:Then run python main.py James Smithto produce the following output:I use optparse myself, but really like the direction Simon Willison is taking with his recently introduced optfunc library.  It works by:So, for example, this function definition:is turned into this optparse help text:I like getopt from stdlib, eg:Lately I have been wrapping something similiar to this to make things less verbose (eg; making "-h" implicit).As you can see optparse "The optparse module is deprecated with and will not be developed further; development will continue with the argparse module." Pocoo's click is more intuitive, requires less boilerplate, and is at least as powerful as argparse.The only weakness I've encountered so far is that you can't do much customization to help pages, but that usually isn't a requirement and docopt seems like the clear choice when it is.You may be interested in a little Python module I wrote to make handling of command line arguments even easier (open source and free to use) - CommandoI recommend looking at docopt as a simple alternative to these others.docopt is a new project that works by parsing your --help usage message rather than requiring you to implement everything yourself. You just have to put your usage message in the POSIX format.Yet another option is argh. It builds on argparse, and lets you write things like:It will automatically generate help and so on, and you can use decorators to provide extra guidance on how the arg-parsing should work.My solution is entrypoint2. Example:help text:

How do I convert datetime to date (in Python)?

niklasfi

[How do I convert datetime to date (in Python)?](https://stackoverflow.com/questions/3743222/how-do-i-convert-datetime-to-date-in-python)

How do I convert a datetime.datetime object (e.g., the return value of datetime.datetime.now()) to a datetime.date object in Python?

2010-09-18 19:44:01Z

How do I convert a datetime.datetime object (e.g., the return value of datetime.datetime.now()) to a datetime.date object in Python?Use the date() method:From the documentation:You use the datetime.datetime.date() method:Obviously, the expression above can (and should IMHO :) be written as:You can convert a datetime object to a date with the date() method of the date time object, as follows:you could enter this code form for (today date & Names of the Day & hour) :

datetime.datetime.now().strftime('%y-%m-%d %a %H:%M:%S')'19-09-09 Mon 17:37:56'and enter this code for (today date simply):

datetime.date.today().strftime('%y-%m-%d')

'19-09-10'for object :

datetime.datetime.now().date()

datetime.datetime.today().date()

datetime.datetime.utcnow().date()

datetime.datetime.today().time()

datetime.datetime.utcnow().date()

datetime.datetime.utcnow().time()

Get key by value in dictionary

user998316

[Get key by value in dictionary](https://stackoverflow.com/questions/8023306/get-key-by-value-in-dictionary)

I made a function which will look up ages in a Dictionary and show the matching name:I know how to compare and find the age I just don't know how to show the name of the person. Additionally, I am getting a KeyError because of line 5. I know it's not correct but I can't figure out how to make it search backwards.

2011-11-05 21:09:18Z

I made a function which will look up ages in a Dictionary and show the matching name:I know how to compare and find the age I just don't know how to show the name of the person. Additionally, I am getting a KeyError because of line 5. I know it's not correct but I can't figure out how to make it search backwards.There is none. dict is not intended to be used this way.Or in Python 3.x:Basically, it separates the dictionary's values in a list, finds the position of the value you have, and gets the key at that position.More about keys() and .values() in Python 3: How can I get list of values from dict?If you want both the name and the age, you should be using .items() which gives you key (key, value) tuples:You can unpack the tuple into two separate variables right in the for loop, then match the age.You should also consider reversing the dictionary if you're generally going to be looking up by age, and no two people have the same age:so you can look up the name for an age by just doingI've been calling it mydict instead of list because list is the name of a built-in type, and you shouldn't use that name for anything else.You can even get a list of all people with a given age in one line:or if there is only one person with each age:which will just give you None if there isn't anyone with that age.Finally, if the dict is long and you're on Python 2, you should consider using .iteritems() instead of .items() as Cat Plus Plus did in his answer, since it doesn't need to make a copy of the list.I thought it would be interesting to point out which methods are the quickest, and in what scenario:Here's some tests I ran (on a 2012 MacBook Pro)Results from profile.run() on each method 100000 times:Method 1:Method 2:Method 3:So this shows that for a small dict, method 1 is the quickest. This is most likely because it returns the first match, as opposed to all of the matches like method 2 (see note below).Interestingly, performing the same tests on a dict I have with 2700 entries, I get quite different results (this time run 10000 times):Method 1:Method 2:Method 3:So here, method 3 is much faster. Just goes to show the size of your dict will affect which method you choose.Notes:

Method 2 returns a list of all names, whereas methods 1 and 3 return only the first match.

I have not considered memory usage. I'm not sure if method 3 creates 2 extra lists (keys() and values()) and stores them in memory.one line version: (i is an old dictionary, p is a reversed dictionary)explanation : i.keys() and i.values() returns two lists with keys and values of the dictionary respectively. The zip function has the ability to tie together lists to produce a dictionary.warning : This would work only if the values are hashable and unique.or betterTry this one-liner to reverse a dictionary:I found this answer very effective but not very easy to read for me.To make it more clear you can invert the key and the value of a dictionary. This is make the keys values and the values keys, as seen here.orwhich is essentially the same that this other answer.If you want to find the key by the value, you can use a dictionary comprehension to create a lookup dictionary and then use that to find the key from the value.You can get key by using dict.keys(), dict.values() and list.index() methods, see code samples below:Here is my take on this problem. :)

I have just started learning Python, so I call this:"The Understandable for beginners" solution...Consider using Pandas. As stated in William McKinney's "Python for Data Analysis'To query your series do the following:Which yields:If you need to do anything else with the output transforming the 

answer into a list might be useful:Here, recover_key takes dictionary and value to find in dictionary. We then loop over the keys in dictionary and make a comparison with that of value and return that particular key.we can get the Key of dict by :it's answered, but it could be done with a fancy 'map/reduce' use, e.g.:here is my take on it. This is good for displaying multiple results just in case you need one. So I added the list as well And that's it... The output is as follows:There is no easy way to find a key in a list by 'looking up' the value. However, if you know the value, iterating through the keys, you can look up values in the dictionary by the element. If D[element] where D is a dictionary object, is equal to the key you're trying to look up, you can execute some code.You need to use a dictionary and reverse of that dictionary. It means you need another data structure. If you are in python 3, use enum module but if you are using python 2.7 use enum34 which is back ported for python 2.Example:Cat Plus Plus mentioned that this isn't how a dictionary is intended to be used. Here's why:The definition of a dictionary is analogous to that of a mapping in mathematics. In this case, a dict is a mapping of K (the set of keys) to V (the values) - but not vice versa. If you dereference a dict, you expect to get exactly one value returned. But, it is perfectly legal for different keys to map onto the same value, e.g.:When you look up a key by it's corresponding value, you're essentially inverting the dictionary. But a mapping isn't necessarily invertible! In this example, asking for the key corresponding to v1 could yield k1 or k3. Should you return both? Just the first one found? That's why indexof() is undefined for dictionaries.If you know your data, you could do this. But an API can't assume that an arbitrary dictionary is invertible, hence the lack of such an operation.Just my answer in lambda and filter.already been answered, but since several people mentioned reversing the dictionary, here's how you do it in one line (assuming 1:1 mapping) and some various perf data:python 2.6:2.7+:if you think it's not 1:1, you can still create a reasonable reverse mapping with a couple lines:how slow is this: slower than a simple search, but not nearly as slow as you'd think - on a 'straight' 100000 entry dictionary, a 'fast' search (i.e. looking for a value that should be early in the keys) was about 10x faster than reversing the entire dictionary, and a 'slow' search (towards the end) about 4-5x faster. So after at most about 10 lookups, it's paid for itself. the second version (with lists per item) takes about 2.5x as long as the simple version.Also had some interesting results with ifilter. Theoretically, ifilter should be faster, in that we can use itervalues() and possibly not have to create/go through the entire values list. In practice, the results were... odd...So, for small offsets, it was dramatically faster than any previous version (2.36 *u*S vs. a minimum of 1.48 *m*S for previous cases). However, for large offsets near the end of the list, it was dramatically slower (15.1ms vs. the same 1.48mS). The small savings at the low end is not worth the cost at the high end, imho. Sometimes int() may be needed:Here is a solution which works both in Python 2 and Python 3:The part until [search_age] constructs the reverse dictionary (where values are keys and vice-versa).

You could create a helper method which will cache this reversed dictionary like so:or even more generally a factory which would create a by-age name lookup method for one or more of you listsso you would be able to do:Note that I renamed list to ages_by_name since the former is a predefined type.This is how you access the dictionary to do what you want:of course, your names are so off it looks like it would be printing an age, but it DOES print the name. Since you are accessing by name, it becomes more understandable if you write:Better yet: For multiple occurrences use:

How to deal with SettingWithCopyWarning in Pandas?

bigbug

[How to deal with SettingWithCopyWarning in Pandas?](https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas)

I just upgraded my Pandas from 0.11 to 0.13.0rc1. Now, the application is popping out many new warnings. One of them like this:I want to know what exactly it means?  Do I need to change something?How should I suspend the warning if I insist to use quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE?

2013-12-17 03:48:02Z

I just upgraded my Pandas from 0.11 to 0.13.0rc1. Now, the application is popping out many new warnings. One of them like this:I want to know what exactly it means?  Do I need to change something?How should I suspend the warning if I insist to use quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE?The SettingWithCopyWarning was created to flag potentially confusing "chained" assignments, such as the following, which don't always work as expected, particularly when the first selection returns a copy.  [see GH5390 and GH5597 for background discussion.]The warning offers a suggestion to rewrite as follows:However, this doesn't fit your usage, which is equivalent to:While it's clear that you don't care about writes making it back to the original frame (since you overwrote the reference to it), unfortunately this pattern can not be differentiated from the first chained assignment example, hence the (false positive) warning.  The potential for false positives is addressed in the docs on indexing, if you'd like to read further.  You can safely disable this new warning with the following assignment.This post is meant for readers who, Setup    To know how to deal with this warning, it is important to understand what it means and why it is raised in the first place.When filtering DataFrames, it is possible slice/index a frame to return either a view, or a copy, depending on the internal layout and various implementation details. A "view" is, as the term suggests, a view into the original data, so modifying the view may modify the original object. On the other hand, a "copy" is a replication of data from the original, and modifying the copy has no effect on the original. As mentioned by other answers, the SettingWithCopyWarning was created to flag "chained assignment" operations. Consider df in the setup above. Suppose you would like to select all values in column "B" where values in column "A" is > 5. Pandas allows you to do this in different ways, some more correct than others. For example, And,These return the same result, so if you are only reading these values, it makes no difference. So, what is the issue? The problem with chained assignment, is that it is generally difficult to predict whether a view or a copy is returned, so this largely becomes an issue when you are attempting to assign values back. To build on the earlier example, consider how this code is executed by the interpreter:With a single __setitem__ call to df. OTOH, consider this code:Now, depending on whether __getitem__ returned a view or a copy, the __setitem__ operation may not work.In general, you should use loc for label-based assignment, and iloc for integer/positional based assignment, as the spec guarantees that they always operate on the original. Additionally, for setting a single cell, you should use at and iat.More can be found in the documentation.Consider a simple operation on the "A" column of df. Selecting "A" and dividing by 2 will raise the warning, but the operation will work.There are a couple ways of directly silencing this warning:@Peter Cotton in the comments, came up with a nice way of non-intrusively changing the mode (modified from this gist) using a context manager, to set the mode only as long as it is required, and the reset it back to the original state when finished.The usage is as follows:Or, to raise the exceptionA lot of the time, users attempt to look for ways of suppressing this exception without fully understanding why it was raised in the first place. This is a good example of an XY problem, where users attempt to solve a problem "Y" that is actually a symptom of a deeper rooted problem "X". Questions will be raised based on common problems that encounter this warning, and solutions will then be presented.Wrong way to do this: Right way using loc:You can use any of the following methods to do this.This is actually probably because of code higher up in your pipeline. Did you create df2 from something larger, like? In this case, boolean indexing will return a view, so df2 will reference the original. What you'd need to do is assign df2 to a copy:This is because df2 must have been created as a view from some other slicing operation, such as  The solution here is to either make a copy() of df, or use loc, as before.In general the point of the SettingWithCopyWarning is to show users (and especially new users) that they may be operating on a copy and not the original as they think. There are false positives (IOW if you know what you are doing it could be ok). One possibility is simply to turn off the (by default warn) warning as @Garrett suggest.Here is another option:You can set the is_copy flag to False, which will effectively turn off the check, for that object:If you explicitly copy then no further warning will happen:The code the OP is showing above, while legitimate, and probably something I do as well, is technically a case for this warning, and not a false positive. Another way to not have the warning would be to do the selection operation via reindex, e.g.Or, When you go and do something like this:pandas.ix in this case returns a new, stand alone dataframe.Any values you decide to change in this dataframe, will not change the original dataframe.This is what pandas tries to warn you about.The .ix object tries to do more than one thing, and for anyone who has read anything about clean code, this is a strong smell.Given this dataframe:Two behaviors:Behavior one: dfcopy is now a stand alone dataframe. Changing it will not change dfBehavior two: This changes the original dataframe.The pandas developers recognized that the .ix object was quite smelly[speculatively] and thus created two new objects which helps in the accession and assignment of data. (The other being .iloc).loc is faster, because it does not try to create a copy of the data..loc is meant to modify your existing dataframe inplace, which is more memory efficient..loc is predictable, it has one behavior.What you are doing in your code example is loading a big file with lots of columns, then modifying it to be smaller.The pd.read_csv function can help you out with a lot of this and also make the loading of the file a lot faster.So instead of doing thisDo thisThis will only read the columns you are interested in, and name them properly. No need for using the evil .ix object to do magical stuff.Here I answer the question directly. How to deal with it?Make a .copy(deep=False) after you slice. See pandas.DataFrame.copy.Wait, doesn't a slice return a copy? After all, this is what the warning message is attempting to say? Read the long answer:This gives a warning:This does not:Both df0 and df1 are DataFrame objects, but something about them is different that enables pandas to print the warning. Let's find out what it is.Using your diff tool of choice, you will see that beyond a couple of addresses, the only material difference is this:The method that decides whether to warn is DataFrame._check_setitem_copy which checks _is_copy. So here you go. Make a copy so that your DataFrame is not _is_copy.The warning is suggesting to use .loc, but if you use .loc on a frame that _is_copy, you will still get the same warning. Misleading? Yes. Annoying? You bet. Helpful? Potentially, when chained assignment is used. But it cannot correctly detect chain assignment and prints the warning indiscriminately.This topic is really confusing with Pandas. Luckily, it has a relatively simple solution.The problem is that it is not always clear whether data filtering operations (e.g. loc) return a copy or a view of the DataFrame. Further use of such filtered DataFrame could therefore be confusing.The simple solution is (unless you need to work with very large sets of data):Whenever you need to update any values, always make sure that you implicitely copy the DataFrame before the assignment.To remove any doubt, my solution was to make a deep copy of the slice instead of a regular copy.

This may not be applicable depending on your context (Memory constraints / size of the slice, potential for performance degradation - especially if the copy occurs in a loop like it did for me, etc...)To be clear, here is the warning I received:I had doubts that the warning was thrown because of a column I was dropping on a copy of the slice. While not technically trying to set a value in the copy of the slice, that was still a modification of the copy of the slice.

Below are the (simplified) steps I have taken to confirm the suspicion, I hope it will help those of us who are trying to understand the warning.We knew that already but this is a healthy reminder. This is NOT what the warning is about.It is possible to avoid changes made on df1 to affect df2This actually illustrates the warning.It is possible to avoid changes made on df2 to affect df1Cheers!This should work:Some may want to simply suppress the warning:If you have assigned the slice to a variable and want to set using the variable as in the following:And you do not want to use Jeffs solution because your condition computing df2 is to long or for some other reason, then you can use the following:df2.index.tolist() returns the indices from all entries in df2, which will then be used to set column B in the original dataframe.For me this issue occured in a following >simplified< example. And I was also able to solve it (hopefully with a correct solution):old code with warning:This printed the warning for the line old_row[field] = new_row[field]Since the rows in update_row method are actually type Series, I replaced the line with:i.e. method for accessing/lookups for a Series. Eventhough both works just fine and the result is same, this way I don't have to disable the warnings (=keep them for other chain indexing issues somewhere else).I hope this may help someone.You could avoid the whole problem like this, I believe:Using Assign. From the documentation: Assign new columns to a DataFrame, returning a new object (a copy) with all the original columns in addition to the new ones. See Tom Augspurger's article on method chaining in pandas: https://tomaugspurger.github.io/method-chainingFollowup beginner question / remarkMaybe a clarification for other beginners like me (I come from R which seems to work a bit differently under the hood). The following harmless-looking and functional code kept producing the SettingWithCopy warning, and I couldn't figure out why. I had both read and understood the issued with "chained indexing", but my code doesn't contain any:But then, later, much too late, I looked at where the plot() function is called:So "df" isn't a data frame but an object that somehow remembers that it was created by indexing a data frame (so is that a view?) which would make the line in plot()equivalent towhich is a chained indexing. Did I get that right?Anyway, fixed it.As this question is already fully explained and discussed in existing answers I will just provide a neat pandas approach to the context manager using pandas.option_context (links to docs and example) - there is absolutely no need to create a custom class with all the dunder methods and other bells and whistles.First the context manager code itself:Then an example:Worth noticing is that both approches do not modify a, which is a bit surprising to me, and even a shallow df copy with .copy(deep=False) would prevent this warning to be raised (as far as I understand shallow copy should at least modify a as well, but it doesn't. pandas magic.).

Is there a portable way to get the current username in Python?

Jacob Gabrielson

[Is there a portable way to get the current username in Python?](https://stackoverflow.com/questions/842059/is-there-a-portable-way-to-get-the-current-username-in-python)

Is there a portable way to get the current user's username in Python (i.e., one that works under both Linux and Windows, at least).  It would work like os.getuid:I googled around and was surprised not to find a definitive answer (although perhaps I was just googling poorly).  The pwd module provides a relatively easy way to achieve this under, say, Linux, but it is not present on Windows.  Some of the search results suggested that getting the username under Windows can be complicated in certain circumstances (e.g., running as a Windows service), although I haven't verified that.

2009-05-08 22:20:20Z

Is there a portable way to get the current user's username in Python (i.e., one that works under both Linux and Windows, at least).  It would work like os.getuid:I googled around and was surprised not to find a definitive answer (although perhaps I was just googling poorly).  The pwd module provides a relatively easy way to achieve this under, say, Linux, but it is not present on Windows.  Some of the search results suggested that getting the username under Windows can be complicated in certain circumstances (e.g., running as a Windows service), although I haven't verified that.Look at getpass moduleAvailability: Unix, Windowsp.s. Per comment below "this function looks at the values of various environment variables to determine the user name. Therefore, this function should not be relied on for access control purposes (or possibly any other purpose, since it allows any user to impersonate any other)."You best bet would be to combine os.getuid() with pwd.getpwuid():Refer to the pwd docs for more details:http://docs.python.org/library/pwd.htmlYou can also use:You can probably use:orBut it's not going to be safe because environment variables can be changed.These might work.  I don't know how they behave when running as a service.  They aren't portable, but that's what os.name and ifstatements are for.See:

http://timgolden.me.uk/python/win32_how_do_i/get-the-owner-of-a-file.htmlIf you are needing this to get user's home dir, below could be considered as portable (win32 and linux at least), part of a standard library.Also you could parse such string to get only last path component (ie. user name).See: os.path.expanduserTo me using os module looks the best for portability: Works best on both Linux and Windows.Output:Windows:Linux:No need of installing any modules or extensions. Combined pwd and getpass approach, based on other answers:For UNIX, at least, this works...edit:

I just looked it up and this works on Windows and UNIX:On UNIX it returns your username, but on Windows, it returns your user's group, slash, your username.--I.E.UNIX returns: "username"Windows returns: "domain/username"--It's interesting, but probably not ideal unless you are doing something in the the terminal anyway... in which case you would probably be using os.system to begin with. For example, a while ago I needed to add my user to a group, so I did (this is in Linux, mind you)I feel like that is easier to read and you don't have to import pwd or getpass.I also feel like having "domain/user" could be helpful in certain applications in Windows.I wrote the plx module some time ago to get the user name in a portable way on Unix and Windows (among other things):

http://www.decalage.info/en/python/plxUsage:(it requires win32 extensions on Windows)You can get the current username on Windows by going through the Windows API, although it's a bit cumbersome to invoke via the ctypes FFI (GetCurrentProcess → OpenProcessToken → GetTokenInformation → LookupAccountSid).I wrote a small module that can do this straight from Python, getuser.py. Usage:It works on both Windows and *nix (the latter uses the pwd module as described in the other answers).Using only standard python libs:Works on Windows, Mac or LinuxAlternatively, you could remove one line with an immediate invocation:

How do I append one string to another in Python?

user469652

[How do I append one string to another in Python?](https://stackoverflow.com/questions/4435169/how-do-i-append-one-string-to-another-in-python)

I want an efficient way to append one string to another in Python, other than the following.Is there any good built-in method to use?

2010-12-14 01:41:29Z

I want an efficient way to append one string to another in Python, other than the following.Is there any good built-in method to use?If you only have one reference to a string and you concatenate another string to the end, CPython now special cases this and tries to extend the string in place.The end result is that the operation is amortized O(n).e.g.used to be O(n^2), but now it is O(n).From the source (bytesobject.c):It's easy enough to verify empirically.It's important however to note that this optimisation isn't part of the Python spec. It's only in the cPython implementation as far as I know. The same empirical testing on pypy or jython for example might show the older O(n**2) performance .So far so good, but then,ouch even worse than quadratic. So pypy is doing something that works well with short strings, but performs poorly for larger strings.Don't prematurely optimize. If you have no reason to believe there's a speed bottleneck caused by string concatenations then just stick with + and +=:That said, if you're aiming for something like Java's StringBuilder, the canonical Python idiom is to add items to a list and then use str.join to concatenate them all at the end:That joins str1 and str2 with a space as separators. You can also do "".join(str1, str2, ...). str.join() takes an iterable, so you'd have to put the strings in a list or a tuple.That's about as efficient as it gets for a builtin method.Don't.That is, for most cases you are better off generating the whole string in one go rather then appending to an existing string.For example, don't do: obj1.name + ":" + str(obj1.count)Instead: use "%s:%d" % (obj1.name, obj1.count)That will be easier to read and more efficient.If you need to do many append operations to build a large string, you can use StringIO or cStringIO. The interface is like a file. ie: you write to append text to it.If you're just appending two strings then just use +.it really depends on your application. If you're looping through hundreds of words and want to append them all into a list, .join() is better. But if you're putting together a long sentence, you're better off using +=.Python 3.6 gives us f-strings, which are a delight:You can do most anything inside the curly bracesBasically, no difference. The only consistent trend is that Python seems to be getting slower with every version... :(Python 2.7Python 3.4Python 3.5Python 3.6Python 2.7: Python 3.4Python 3.5Python 3.6append strings with __add__ functionOutput

How do I expand the output display to see more columns of a pandas DataFrame?

beets

[How do I expand the output display to see more columns of a pandas DataFrame?](https://stackoverflow.com/questions/11707586/how-do-i-expand-the-output-display-to-see-more-columns-of-a-pandas-dataframe)

Is there a way to widen the display of output in either interactive or script-execution mode?Specifically, I am using the describe() function on a pandas DataFrame.  When the DataFrame is 5 columns (labels) wide, I get the descriptive statistics that I want.  However, if the DataFrame has any more columns, the statistics are suppressed and something like this is returned:The "8" value is given whether there are 6 or 7 columns.  What does the "8" refer to?I have already tried dragging the IDLE window larger, as well as increasing the "Configure IDLE" width options, to no avail.My purpose in using pandas and describe() is to avoid using a second program like Stata to do basic data manipulation and investigation.

2012-07-29 07:44:51Z

Is there a way to widen the display of output in either interactive or script-execution mode?Specifically, I am using the describe() function on a pandas DataFrame.  When the DataFrame is 5 columns (labels) wide, I get the descriptive statistics that I want.  However, if the DataFrame has any more columns, the statistics are suppressed and something like this is returned:The "8" value is given whether there are 6 or 7 columns.  What does the "8" refer to?I have already tried dragging the IDLE window larger, as well as increasing the "Configure IDLE" width options, to no avail.My purpose in using pandas and describe() is to avoid using a second program like Stata to do basic data manipulation and investigation.Update: Pandas 0.23.4 onwardsThis is not necessary, pandas autodetects the size of your terminal window if you set pd.options.display.width = 0. (For older versions see at bottom.)pandas.set_printoptions(...) is deprecated. Instead, use pandas.set_option(optname, val), or equivalently pd.options.<opt.hierarchical.name> = val. Like:Here is the help for set_option:EDIT: older version information, much of this has been deprecated.As @bmu mentioned, pandas auto detects (by default) the size of the display area, a summary view will be used when an object repr does not fit on the display. You mentioned resizing the IDLE window, to no effect. If you do print df.describe().to_string() does it fit on the IDLE window?The terminal size is determined by pandas.util.terminal.get_terminal_size() (deprecated and removed), this returns a tuple containing the (width, height) of the display. Does the output match the size of your IDLE window? There might be an issue (there was one before when running a terminal in emacs).Note that it is possible to bypass the autodetect, pandas.set_printoptions(max_rows=200, max_columns=10) will never switch to summary view if number of rows, columns does not exceed the given limits.The 'max_colwidth' option helps in seeing untruncated form of each column.Try this:From the documentation:See: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.set_option.htmlIf you want to set options temporarily to display one large DataFrame, you can use option_context:Option values are restored automatically when you exit the with block. Only using these 3 lines worked for me:Anaconda / Python 3.6.5 / pandas: 0.23.0 / Visual Studio Code 1.26Set column max width using:This particular statement sets max width to 800px, per column.You can use print df.describe().to_string() to force it to show the whole table.  (You can use to_string() like this for any DataFrame.  The result of describe is just a DataFrame itself.)The 8 is the number of rows in the DataFrame holding the "description" (because describe computes 8 statistics, min, max, mean, etc.).You can adjust pandas print options with set_printoptions.However this will not work in all cases as pandas detects your console width and it will only use to_string if the output fits in the console (see the docstring of set_printoptions). 

In this case you can explicitly call to_string as answered by BrenBarn.UpdateWith version 0.10 the way wide dataframes are printed changed:Further more the API for setting pandas options changed:You can set the output display to match your current terminal width:According to the docs for v0.18.0, if you're running on a terminal (ie not iPython notebook, qtconsole or IDLE), it's a 2-liner to have Pandas auto-detect your screen width and adapt on the fly with how many columns it shows:It seems like all above answers solve the problem. One more point: instead of pd.set_option('option_name'), you can use the (auto-complete-able)See Pandas doc: Options and Settings:[...]for the max_... params:for the width param:OutPut: I used these settings when scale of data is high.You can refer the documentationhereIf you don't want to mess with your display options and you just want to see this one particular list of columns without expanding out every dataframe you view, you could try: The below line is enough to display all columns from dataframe.

 pd.set_option('display.max_columns', None)You can also try in a loop:You can simply do the following steps,Please kindly refer the doc to change different options/settings for pandas

python open built-in function: difference between modes a, a+, w, w+, and r+?

flybywire

[python open built-in function: difference between modes a, a+, w, w+, and r+?](https://stackoverflow.com/questions/1466000/python-open-built-in-function-difference-between-modes-a-a-w-w-and-r)

In the python built-in open function, what is the exact difference between the modes w, a, w+, a+, and r+?In particular, the documentation implies that all of these will allow writing to the file, and says that it opens the files for "appending", "writing", and "updating" specifically, but does not define what these terms mean.

2009-09-23 13:27:36Z

In the python built-in open function, what is the exact difference between the modes w, a, w+, a+, and r+?In particular, the documentation implies that all of these will allow writing to the file, and says that it opens the files for "appending", "writing", and "updating" specifically, but does not define what these terms mean.The opening modes are exactly the same as those for the C standard library function fopen().The BSD fopen manpage defines them as follows:I noticed that every now and then I need to Google fopen all over again, just to build a mental image of what the primary differences between the modes are. So, I thought a diagram will be faster to read next time. Maybe someone else will find that helpful too.Same info, just in table formwhere meanings are:

(just to avoid any misinterpretation)Note: a and a+ always append to the end of file - ignores any seek movements.

BTW. interesting behavior at least on my win7 / python2.7, for new file opened in a+ mode:

write('aa'); seek(0, 0); read(1); write('b') - second write is ignored

write('aa'); seek(0, 0); read(2); write('b') - second write raises IOErrorThe options are the same as for the fopen function in the C standard library:w truncates the file, overwriting whatever was already therea appends to the file, adding onto whatever was already therew+ opens for reading and writing, truncating the file but also allowing you to read back what's been written to the filea+ opens for appending and reading, allowing you both to append to the file and also read its contentsI think this is important to consider for cross-platform execution, i.e. as a CYA. :)This is directly quoted from Python Software Foundation 2.7.x.I hit upon this trying to figure out why you would use mode 'w+' versus 'w'. In the end, I just did some testing. I don't see much purpose for mode 'w+', as in both cases, the file is truncated to begin with. However, with the 'w+', you could read after writing by seeking back. If you tried any reading with 'w', it would raise an IOError. Reading without using seek with mode 'w+' isn't going to yield anything, since the file pointer will be after where you have written. 

How to split a string into a list?

Thanx

[How to split a string into a list?](https://stackoverflow.com/questions/743806/how-to-split-a-string-into-a-list)

I want my Python function to split a sentence (input) and store each word in a list. My current code splits the sentence, but does not store the words as a list. How do I do that?

2009-04-13 12:48:44Z

I want my Python function to split a sentence (input) and store each word in a list. My current code splits the sentence, but does not store the words as a list. How do I do that?This should be enough to store each word in a list.  words is already a list of the words from the sentence, so there is no need for the loop.Second, it might be a typo, but you have your loop a little messed up. If you really did want to use append, it would be:not Splits the string in text on any consecutive runs of whitespace.Split the string in text on delimiter: ",".The words variable will be a list and contain the words from text split on the delimiter.str.split()Depending on what you plan to do with your sentence-as-a-list, you may want to look at the Natural Language Took Kit.  It deals heavily with text processing and evaluation. You can also use it to solve your problem:This has the added benefit of splitting out punctuation.Example:This allows you to filter out any punctuation you don't want and use only words.  Please note that the other solutions using string.split() are better if you don't plan on doing any complex manipulation of the sentence.[Edited]How about this algorithm? Split text on whitespace, then trim punctuation. This carefully removes punctuation from the edge of words, without harming apostrophes inside words such as we're.The str().split() method does this, it takes a string, splits it into a list:The problem you're having is because of a typo, you wrote print(words) instead of print(word):Renaming the word variable to current_word, this is what you had:..when you should have done:If for some reason you want to manually construct a list in the for loop, you would use the list append() method, perhaps because you want to lower-case all words (for example):Or more a bit neater, using a list-comprehension:shlex has a .split() function. It differs from str.split() in that it does not preserve quotes and treats a quoted phrase as a single word:If you want all the chars of a word/sentence in a list, do this:                I think you are confused because of a typo.  Replace print(words) with print(word) inside your loop to have every word printed on a different line

How do I get the day of week given a date?

Mohit

[How do I get the day of week given a date?](https://stackoverflow.com/questions/9847213/how-do-i-get-the-day-of-week-given-a-date)

I want to find out the following:

given a date (datetime object), what is the corresponding day of the week?For instance, Sunday is the first day, Monday: second day.. and so onAnd then if the input is something like today's date.The output is maybe 6 (since it's Friday)

2012-03-23 22:21:18Z

I want to find out the following:

given a date (datetime object), what is the corresponding day of the week?For instance, Sunday is the first day, Monday: second day.. and so onAnd then if the input is something like today's date.The output is maybe 6 (since it's Friday)Use weekday() (docs):From the documentation:If you'd like to have the date in English:If you'd like to have the date in English:Read more:

https://docs.python.org/2/library/datetime.html#strftime-strptime-behaviorUse date.weekday() or date.isoweekday().I solved this for a codechef question.A solution whithout imports for dates after 1700/1/1 This is a solution if the date is a datetime object.datetime library sometimes gives errors with strptime() so I switched to dateutil library. Here's an example of how you can use it :The output that you get from this is 'Mon'. If you want the output as 'Monday', use the following :This worked for me pretty quickly. I was having problems while using the datetime library because I wanted to store the weekday name instead of weekday number and the format from using the datetime library was causing problems. If you're not having problems with this, great! If you are, you cand efinitely go for this as it has a simpler syntax as well. Hope this helps.Assuming you are given the day, month, and year, you could do:If you have dates as a string, it might be easier to do it using pandas' TimestampOutput:If you have reason to avoid the use of the datetime module, then this function will work.Note: The change from the Julian to the Gregorian calendar is assumed to have occurred in 1582. If this is not true for your calendar of interest then change the line if year > 1582: accordingly.If you're not solely reliant on the datetime module, calendar might be a better alternative. This, for example, will provide you with the day codes:And this will give you the day itself:Or in the style of python, as a one liner:Say you have timeStamp: String variable, YYYY-MM-DD HH:MM:SSstep 1: convert it to dateTime function with blow code...Step 2 : Now you can extract all the required feature as below which will create new Column for each of the fild- hour,month,day of week,year, dateWe can take help of Pandas:As mentioned above in the problem We have:If execute this line in the jupyter notebook we have an output like this:Using weekday() and weekday_name:If you want weekdays in integer number format then use:The output will be:And if you want it as name of the day like Sunday, Monday, Friday, etc you can use:The output will be:'Friday'If having a dates column in Pandas dataframe then:Now suppose if you have a pandas dataframe having a date column like this:

pdExampleDataFrame['Dates'].head(5)Now If we want to know the name of the weekday like Monday, Tuesday, ..etc we can use .weekday_name as follows:the output will be:And if we want the integer number of weekday from this Dates column then we can use:The output will look like this:this should give you your real day number - 1 = sunday, 2 = monday, etc...To get Sunday as 1 through Saturday as 7, this is the simplest solution to your question:All of them:Output:here is how to convert a listof dates to dateUsing Canlendar ModuleHere is my python3 implementation. use this code:import numpy as npdef date(df):

Why use def main()? [duplicate]

Wizzard

[Why use def main()? [duplicate]](https://stackoverflow.com/questions/4041238/why-use-def-main)

I've seen some code samples and tutorials that useBut why? Is there any reason not do define your functions at the top of the file, then just write code under it? ieI just wonder if there is any rhyme to the main?

2010-10-28 08:54:21Z

I've seen some code samples and tutorials that useBut why? Is there any reason not do define your functions at the top of the file, then just write code under it? ieI just wonder if there is any rhyme to the main?Without the main sentinel, the code would be executed even if the script were imported as a module.Everyone else has already answered it, but I think I still have something else to add.Reasons to have that if statement calling main() (in no particular order):But, you are not required to write a main() function and call it inside an if statement.I myself usually start writing small throwaway scripts without any kind of function. If the script grows big enough, or if I feel putting all that code inside a function will benefit me, then I refactor the code and do it. This also happens when I write bash scripts.Even if you put code inside the main function, you are not required to write it exactly as that. A neat variation could be:This means you can call main() from other scripts (or interactive shell) passing custom parameters. This might be useful in unit tests, or when batch-processing. But remember that the code above will require parsing of argv, thus maybe it would be better to use a different call that pass parameters already parsed.In an object-oriented application I've written, the code looked like this:So, feel free to write the code that better suits you. :)if the content of foo.pyA file foo.py can be used in two ways.In this case __name__ is foo, the code section does not get executed and does not print XXXX.When it is executed directly, __name__ is same as __main__ and the code in that section is executed and prints XXXXOne of the use of this functionality to write various kind of unit tests within the same module. "What does if __name__==「__main__」: do?" has already been answered.Having a main() function allows you to call its functionality if you import the module. The main (no pun intended) benefit of this (IMHO) is that you can unit test it.Consider the second script. If you import it in another one, the instructions, as at "global level", will be executed.

How to print the full NumPy array, without truncation?

kame

[How to print the full NumPy array, without truncation?](https://stackoverflow.com/questions/1987694/how-to-print-the-full-numpy-array-without-truncation)

When I print a numpy array, I get a truncated representation, but I want the full array. Is there any way to do this?Examples:

2010-01-01 01:51:46Z

When I print a numpy array, I get a truncated representation, but I want the full array. Is there any way to do this?Examples:Use numpy.set_printoptions:I suggest using np.inf instead of np.nan which is suggested by others. They both work for your purpose, but by setting the threshold to "infinity" it is obvious to everybody reading your code what you mean. Having a threshold of "not a number" seems a little vague to me.The previous answers are the correct ones, but as a weaker alternative you can transform into a list:If you use NumPy 1.15 (released 2018-07-23) or newer, you can use the printoptions context manager:(of course, replace numpy by np if that's how you imported numpy)The use of a context manager (the with-block) ensures that after the context manager is finished, the print options will revert to whatever they were before the block started.  It ensures the setting is temporary, and only applied to code within the block.See numpy.printoptions documentation for details on the context manager and what other arguments it supports.This sounds like you're using numpy.If that's the case, you can add:That will disable the corner printing.  For more information, see this NumPy Tutorial.Here is a one-off way to do this, which is useful if you don't want to change your default settings:Using a context manager as Paul Price sugggestednumpy.savetxtor if you need a string:The default output format is:and it can be configured with further arguments.Note in particular how this also not shows the square brackets, and allows for a lot of customization, as mentioned at: How to print a Numpy array without brackets?Tested on Python 2.7.12, numpy 1.11.1.This is a slight modification (removed the option to pass additional arguments to set_printoptions)of neoks answer. It shows how you can use contextlib.contextmanager to easily create such a contextmanager with fewer lines of code:In your code it can be used like this:Complementary to this answer from the maximum number of columns (fixed with numpy.set_printoptions(threshold=numpy.nan)), there is also a limit of characters to be displayed. In some environments like when calling python from bash (rather than the interactive session), this can be fixed by setting the parameter linewidth as following.In this case, your window should limit the number of characters to wrap the line.For those out there using sublime text and wanting to see results within the output window, you should add the build option "word_wrap": false to the sublime-build file [source] .Since NumPy version 1.16, for more details see GitHub ticket 12251.To turn it off and return to the normal mode Suppose you have a numpy arrayIf you want to print the full array in a one-off way (without toggling np.set_printoptions), but want something simpler (less code) than the context manager, just doYou can use the array2string function - docs.You won't always want all items printed, especially for large arrays. A simple way to show more items:It works fine when sliced array < 1000 by default.If an array is too large to be printed, NumPy automatically skips the central part of the array and only prints the corners:

To disable this behaviour and force NumPy to print the entire array, you can change the printing options using set_printoptions.orYou can also refer to the numpy documentation numpy documentation for "or part" for more help.

How to capitalize the first letter of each word in a string?

TIMEX

[How to capitalize the first letter of each word in a string?](https://stackoverflow.com/questions/1549641/how-to-capitalize-the-first-letter-of-each-word-in-a-string)

...do something here...s should be :What's the easiest way to do this?

2009-10-11 02:03:54Z

...do something here...s should be :What's the easiest way to do this?The .title() method of a string (either ASCII or Unicode is fine) does this:However, look out for strings with embedded apostrophes, as noted in the docs.The .title() method can't work well, Try string.capwords() method, From the python docs on capwords:Just because this sort of thing is fun for me, here are two more solutions.Split into words, initial-cap each word from the split groups, and rejoin.  This will change the white space separating the words into a single white space, no matter what it was.EDIT: I don't remember what I was thinking back when I wrote the above code, but there is no need to build an explicit list; we can use a generator expression to do it in lazy fashion.  So here is a better solution:Use a regular expression to match the beginning of the string, or white space separating words, plus a single non-whitespace character; use parentheses to mark "match groups".  Write a function that takes a match object, and returns the white space match group unchanged and the non-whitespace character match group in upper case.  Then use re.sub() to replace the patterns.  This one does not have the punctuation problems of the first solution, nor does it redo the white space like my first solution.  This one produces the best result.I'm glad I researched this answer.  I had no idea that re.sub() could take a function!  You can do nontrivial processing inside re.sub() to produce the final result!Here's a summary of different ways to do it, they will work for all these inputs:- The simplest solution is to split the sentence into words and capitalize the first letter then join it back together: - If you don't want to split the input string into words first, and using fancy generators:- Or without importing itertools:- Or you can use regular expressions, from steveha's answer: Now, these are some other answers that were posted, and inputs for which they don't work as expected if we are using the definition of a word being the start of the sentence or anything after a blank space:using ' ' for the split will fix the second output, but capwords() still won't work for the firstBe careful with multiple blank spacesCopy-paste-ready version of @jibberia anwser:Why do you complicate your life with joins and for loops when the solution is simple and safe??Just do this:If str.title() doesn't work for you, do the capitalization yourself.One-liner:Clear example:If only you want the first letter:But to capitalize each word:An empty string will raise an Error if you access [1:], therefore I would use:to uppercase the first letter only.As Mark pointed out you should use .title():However, if would like to make the first letter uppercase inside a django template, you could use this:or using a variable:To capitalize words...@Gary02127 comment, below solution work  title with apostrophe The suggested method str.title() does not work in all cases.

For example:instead of "A B 3c". I think, it is better to do something like this:Don't overlook the preservation of white space. If you want to process 'fred   flinstone' and you get 'Fred Flinstone' instead of 'Fred   Flinstone', you've corrupted your white space. Some of the above solutions will lose white space.  Here's a solution that's good for Python 2 and 3 and preserves white space.Although all the answer are already satisfactory but I'll try to cover the 2 extra cases along with the all the previous case.Here you can use thisthis will give you I hope this is not redundant.A quick function worked for Python 3Capitalize string with non-uniform spacesWell, I understand this is an old question and probably answers may have nearly been exhausited, but I would like to add to @Amit Gupta's point of non-uniform spaces.

From the original question, we would like to capitalize every word in the string s = 'the brown fox'. What if the string was s = 'the brown      fox' with non-uniform spaces.**In case you want to downsize **I really like this answer:Copy-paste-ready version of @jibberia anwser:But some of the lines that I was sending split off some blank '' characters that caused errors when trying to do s[1:].  There is probably a better way to do this, but I had to add in a if len(s)>0, as in

What does 'super' do in Python?

user25785

[What does 'super' do in Python?](https://stackoverflow.com/questions/222877/what-does-super-do-in-python)

What's the difference between:and:I've seen super being used quite a lot in classes with only single inheritance. I can see why you'd use it in multiple inheritance but am unclear as to what the advantages are of using it in this kind of situation.

2008-10-21 18:13:15Z

What's the difference between:and:I've seen super being used quite a lot in classes with only single inheritance. I can see why you'd use it in multiple inheritance but am unclear as to what the advantages are of using it in this kind of situation.The benefits of super() in single-inheritance are minimal -- mostly, you don't have to hard-code the name of the base class into every method that uses its parent methods.However, it's almost impossible to use multiple-inheritance without super(). This includes common idioms like mixins, interfaces, abstract classes, etc. This extends to code that later extends yours. If somebody later wanted to write a class that extended Child and a mixin, their code would not work properly.means to call SomeBaseClass's __init__. whilemeans to call a bound __init__ from the parent class that follows Child in the instance's Method Resolution Order (MRO). If the instance is a subclass of Child, there may be a different parent that comes next in the MRO. When you write a class, you want other classes to be able to use it. super() makes it easier for other classes to use the class you're writing.As Bob Martin says, a good architecture allows you to postpone decision making as long as possible.super() can enable that sort of architecture.When another class subclasses the class you wrote, it could also be inheriting from other classes. And those classes could have an __init__ that comes after this __init__ based on the ordering of the classes for method resolution.Without super you would likely hard-code the parent of the class you're writing (like the example does). This would mean that you would not call the next __init__ in the MRO, and you would thus not get to reuse the code in it.If you're writing your own code for personal use, you may not care about this distinction. But if you want others to use your code, using super is one thing that allows greater flexibility for users of the code.This works in Python 2 and 3:This only works in Python 3:It works with no arguments by moving up in the stack frame and getting the first argument to the method (usually self for an instance method or cls for a class method - but could be other names) and finding the class (e.g. Child) in the free variables (it is looked up with the name __class__ as a free closure variable in the method).I prefer to demonstrate the cross-compatible way of using super, but if you are only using Python 3, you can call it with no arguments.What does it give you? For single inheritance, the examples from the question are practically identical from a static analysis point of view. However, using super gives you a layer of indirection with forward compatibility.Forward compatibility is very important to seasoned developers. You want your code to keep working with minimal changes as you change it. When you look at your revision history, you want to see precisely what changed when. You may start off with single inheritance, but if you decide to add another base class, you only have to change the line with the bases - if the bases change in a class you inherit from (say a mixin is added) you'd change nothing in this class. Particularly in Python 2, getting the arguments to super and the correct method arguments right can be difficult. If you know you're using super correctly with single inheritance, that makes debugging less difficult going forward.Other people can use your code and inject parents into the method resolution:Say you add another class to your object, and want to inject a class between Foo and Bar (for testing or some other reason):Using the un-super child fails to inject the dependency because the child you're using has hard-coded the method to be called after its own:However, the class with the child that uses super can correctly inject the dependency:Python linearizes a complicated inheritance tree via the C3 linearization algorithm to create a Method Resolution Order (MRO).We want methods to be looked up in that order.For a method defined in a parent to find the next one in that order without super, it would have to The UnsuperChild does not have access to InjectMe. It is the UnsuperInjector that has access to InjectMe - and yet cannot call that class's method from the method it inherits from UnsuperChild. Both Child classes intend to call a method by the same name that comes next in the MRO, which might be another class it was not aware of when it was created. The one without super hard-codes its parent's method - thus is has restricted the behavior of its method, and subclasses cannot inject functionality in the call chain.The one with super has greater flexibility. The call chain for the methods can be intercepted and functionality injected.You may not need that functionality, but subclassers of your code may.Always use super to reference the parent class instead of hard-coding it.What you intend is to reference the parent class that is next-in-line, not specifically the one you see the child inheriting from.Not using super can put unnecessary constraints on users of your code.Doesn't all of this assume that the base class is a new-style class?Will not work in Python 2. class A must be new-style, i.e: class A(object)I had played a bit with super(), and had recognized that we can change calling order.For example, we have next hierarchy structure:In this case MRO of D will be (only for Python 3):Let's create a class where super() calls after method execution.So we can see that resolution order is same as in MRO. But when we call super() in the beginning of the method:We have a different order it is reversed a order of the MRO tuple.For additional reading I would recommend next answers:When calling super() to resolve to a parent's version of a classmethod, instance method, or staticmethod, we want to pass the current class whose scope we are in as the first argument, to indicate which parent's scope we're trying to resolve to, and as a second argument the object of interest to indicate which object we're trying to apply that scope to.Consider a class hierarchy A, B, and C where each class is the parent of the one following it, and a, b, and c respective instances of each.e.g. using super() from within the __new__() methodExplanation: 1- even though it's usual for __new__() to take as its first param a reference to the calling class, it is not implemented in Python as a classmethod, but rather a staticmethod. That is, a reference to a class has to be passed explicitly as the first argument when calling __new__() directly:2- when calling super() to get to the parent class we pass the child class A as its first argument, then we pass a reference to the object of interest, in this case it's the class reference that was passed when A.__new__(cls) was called. In most cases it also happens to be a reference to the child class. In some situations it might not be, for instance in the case of multiple generation inheritances.3- since as a general rule __new__() is a staticmethod, super(A, cls).__new__ will also return a staticmethod and needs to be supplied all arguments explicitly, including the reference to the object of insterest, in this case cls.4- doing the same thing without supere.g. using super() from within __init__()Explanation:1- __init__ is an instance method, meaning that it takes as its first argument a reference to an instance. When called directly from the instance, the reference is passed implicitly, that is you don't need to specify it:2- when calling super() within __init__() we pass the child class as the first argument and the object of interest as a second argument, which in general is a reference to an instance of the child class.3- The call super(A, self) returns a proxy that will resolve the scope and apply it to self as if it's now an instance of the parent class. Let's call that proxy s. Since __init__() is an instance method the call s.__init__(...) will implicitly pass a reference of self as the first argument to the parent's __init__().4- to do the same without super we need to pass a reference to an instance explicitly to the parent's version of __init__().Explanation:1- A classmethod can be called from the class directly and takes as its first parameter a reference to the class. 2- when calling super() within a classmethod to resolve to its parent's version of it, we want to pass the current child class as the first argument to indicate which parent's scope we're trying to resolve to, and the object of interest as the second argument to indicate which object we want to apply that scope to, which in general is a reference to the child class itself or one of its subclasses.3- The call super(B, cls) resolves to the scope of A and applies it to cls. Since alternate_constructor() is a classmethod the call super(B, cls).alternate_constructor(...) will implicitly pass a reference of cls as the first argument to A's version of alternate_constructor()4- to do the same without using super() you would need to get a reference to the unbound version of A.alternate_constructor() (i.e. the explicit version of the function). Simply doing this would not work:The above would not work because the A.alternate_constructor() method takes an implicit reference to A as its first argument. The cls being passed here would be its second argument.Many great answers, but for visual learners:

Firstly lets explore with arguments to super, and then without.

Imagine theres an instance jack created from the class Jack, who has the inheritance chain as shown in green in the picture. Calling:super(Jack, jack).method(...) will use the MRO (Method Resolution Order) of jack (its inheritance tree in a certain order), and will start searching from Jack. Why can one provide a parent class? Well if we start searching from the instance jack, it would find the instance method, the whole point is to find its parents method.If one does not supply arguments to super, its like the first argument passed in is the class of self, and the second argument passed in is self. These are auto-calculated for you in Python3.However say we dont want to use Jack's method, instead of passing in Jack, we could of passed in Jen to start searching upwards for the method from Jen.It searches one layer at a time (width not depth), e.g. if Adam and Sue both have the required method, the one from Sue will be found first.If Cain and Sue both had the required method, Cain's method would be called first.

This corresponds in code to:MRO is from left to right.some great answers here, but they do not tackle how to use super() in the case where different classes in the hierarchy have different signatures ... especially in the case of __init__ to answer that part and to be able to effectively use super() i'd suggest reading my answer super() and changing the signature of cooperative methods. here's just the solution to this scenario:example usage:output:This is fairly easy to understand.Ok, what happens now if you use super(Child,self)?When a Child instance is created, its MRO(Method Resolution Order) is in the order of (Child, SomeBaseClass, object) based on the inheritance. (assume SomeBaseClass doesn't have other parents except for the default object)By passing Child, self,  super searches in the MRO of the self instance, and return the proxy object next of Child, in this case it's SomeBaseClass, this object then invokes the __init__ method of SomeBaseClass. In other word, if it's super(SomeBaseClass,self), the proxy object that super returns would be object For multi inheritance, the MRO could contain many classes, so basically super lets you decide where you want to start searching in the MRO.

How do I do a case-insensitive string comparison?

Kozyarchuk

[How do I do a case-insensitive string comparison?](https://stackoverflow.com/questions/319426/how-do-i-do-a-case-insensitive-string-comparison)

How can I do case insensitive string comparison in Python?I would like to encapsulate comparison of a regular strings to a repository string using in a very simple and Pythonic way. I also would like to have ability to look up values in a dict hashed by strings using regular python strings.

2008-11-26 01:06:44Z

How can I do case insensitive string comparison in Python?I would like to encapsulate comparison of a regular strings to a repository string using in a very simple and Pythonic way. I also would like to have ability to look up values in a dict hashed by strings using regular python strings.Assuming ASCII strings:Comparing strings in a case insensitive way seems trivial, but it's not. I will be using Python 3, since Python 2 is underdeveloped here.The first thing to note is that case-removing conversions in Unicode aren't trivial. There is text for which text.lower() != text.upper().lower(), such as "ß":But let's say you wanted to caselessly compare "BUSSE" and "Buße". Heck, you probably also want to compare "BUSSE" and "BUẞE" equal - that's the newer capital form. The recommended way is to use casefold:Do not just use lower. If casefold is not available, doing .upper().lower() helps (but only somewhat).Then you should consider accents. If your font renderer is good, you probably think "ê" == "ê" - but it doesn't:This is because the accent on the latter is a combining character.The simplest way to deal with this is unicodedata.normalize. You probably want to use NFKD normalization, but feel free to check the documentation. Then one doesTo finish up, here this is expressed in functions:Using Python 2, calling .lower() on each string or Unicode object......will work most of the time, but indeed doesn't work in the situations @tchrist has described.Assume we have a file called unicode.txt containing the two strings Σίσυφος and ΣΊΣΥΦΟΣ. With Python 2:The Σ character has two lowercase forms, ς and σ, and .lower() won't help compare them case-insensitively.However, as of Python 3, all three forms will resolve to ς, and calling lower() on both strings will work correctly:So if you care about edge-cases like the three sigmas in Greek, use Python 3.(For reference, Python 2.7.3 and Python 3.3.0b1 are shown in the interpreter printouts above.)Section 3.13 of the Unicode standard defines algorithms for caseless

matching.X.casefold() == Y.casefold() in Python 3 implements the "default caseless matching" (D144).Casefolding does not preserve the normalization of strings in all instances and therefore the normalization needs to be done ('å' vs. 'å'). D145 introduces "canonical caseless matching":NFD() is called twice for very infrequent edge cases involving U+0345 character.Example:There are also compatibility caseless matching (D146) for cases such as '㎒' (U+3392) and  "identifier caseless matching" to simplify and optimize caseless matching of identifiers.I saw this solution here using regex.It works well with accentsHowever, it doesn't work with unicode characters case-insensitive. Thank you @Rhymoid for pointing out that as my understanding was that it needs the exact symbol, for the case to be true. The output is as follows:The usual approach is to uppercase the strings or lower case them for the lookups and comparisons.  For example:How about converting to lowercase first? you can use string.lower().This is another regex which I have learned to love/hate over the last week so usually import as (in this case yes) something that reflects how im feeling!

make a normal function.... ask for input, then use ....something = re.compile(r'foo*|spam*',  yes.I)...... re.I (yes.I below) is the same as IGNORECASE but you cant make as many mistakes writing it!You then search your message using regex's but honestly that should be a few pages in its own , but the point is that foo or spam are piped together and case is ignored.

Then if either are found then lost_n_found would display one of them. if neither then lost_n_found is equal to None. If its not equal to none return the user_input in lower case using "return lost_n_found.lower()"This allows you to much more easily match up anything thats going to be case sensitive. Lastly (NCS) stands for "no one cares seriously...!" or not case sensitive....whicheverif anyone has any questions get me on this..

How do I use raw_input in Python 3

Lonnie Price

[How do I use raw_input in Python 3](https://stackoverflow.com/questions/954834/how-do-i-use-raw-input-in-python-3)

I am using Python 3.1 and can't get the raw_input to "freeze" the dos pop-up. The book I'm reading is for Python 2.5 and I'm using Python 3.1What should I do to fix this?

2009-06-05 08:32:23Z

I am using Python 3.1 and can't get the raw_input to "freeze" the dos pop-up. The book I'm reading is for Python 2.5 and I'm using Python 3.1What should I do to fix this?Starting with Python 3, raw_input() was renamed to input().From What’s New In Python 3.0, Builtins section second item.This works in Python 3.x and 2.x:A reliable way to address this issix is a module which patches over many of the 2/3 common code base pain points.As others have indicated, the raw_input function has been renamed to input in Python 3.0, and you really would be better served by a more up-to-date book, but I want to point out that there are better ways to see the output of your script.From your description, I think you're using Windows, you've saved a .py file and then you're double-clicking on it to run it. The terminal window that pops up closes as soon as your program ends, so you can't see what the result of your program was. To solve this, your book recommends adding a raw_input / input statement to wait until the user presses enter. However, as you've seen, if something goes wrong, such as an error in your program, that statement won't be executed and the window will close without you being able to see what went wrong. You might find it easier to use a command-prompt or IDLE.When you're looking at the folder window that contains your Python program, hold down shift and right-click anywhere in the white background area of the window. The menu that pops up should contain an entry "Open command window here". (I think this works on Windows Vista and Windows 7.) This will open a command-prompt window that looks something like this:To run your program, type the following (substituting your script name):...and press enter. (If you get an error that "python" is not a recognized command, see http://showmedo.com/videotutorials/video?name=960000&fromSeriesID=96 ) When your program finishes running, whether it completes successfully or not, the window will remain open and the command-prompt will appear again for you to type another command. If you want to run your program again, you can press the up arrow to recall the previous command you entered and press enter to run it again, rather than having to type out the file name every time.IDLE is a simple program editor that comes installed with Python. Among other features it can run your programs in a window. Right-click on your .py file and choose "Edit in IDLE". When your program appears in the editor, press F5 or choose "Run module" from the "Run" menu. Your program will run in a window that stays open after your program ends, and in which you can enter Python commands to run immediately.Timmerman's solution works great when running the code, but if you don't want to get Undefined name errors when using pyflakes or a similar linter you could use the following instead:Here's a piece of code I put in my scripts that I wan't to run in py2/3-agnostic environment:Now you can use real_raw_input. It's quite expensive but short and readable. Using raw input is usually time expensive (waiting for input), so it's not important.In theory, you can even assign raw_input instead of real_raw_input but there might be modules that check existence of raw_input and behave accordingly. It's better stay on the safe side.Probably not the best solution, but before I came here I just made this on the fly to keep working without having a quick break from study.Then when I run raw_input('Enter your first name: ') on the script I was working on, it captures it as does input() would.There may be a reason not to do this, that I haven't come across yet!

Is it possible to break a long line to multiple lines in Python [duplicate]

Bin Chen

[Is it possible to break a long line to multiple lines in Python [duplicate]](https://stackoverflow.com/questions/4172448/is-it-possible-to-break-a-long-line-to-multiple-lines-in-python)

Just like C, you can break a long line into multiple short lines. But in Python, if I do this, there will be an indent error... Is it possible?

2010-11-13 12:17:04Z

Just like C, you can break a long line into multiple short lines. But in Python, if I do this, there will be an indent error... Is it possible?From PEP 8 - Style Guide for Python Code:Example of implicit line continuation:On the topic of line-breaks around a binary operator, it goes on to say:-Example of explicit line continuation:There is more than one way to do it.1). A long statement:2). Using parenthesis:3). Using \ again:Quoting PEP8:If you want to assign a long str to variable, you can do it as below:Do not add any comma, or you will get a tuple which contains many strs!It works in Python too:When trying to enter continuous text (say, a query) do not put commas at the end of the line or you will get a list of strings instead of one long string:kinda like that.There is a comment like this from acgtyrant, sorry, didn't see that. :/As far as I know, it can be done. Python has implicit line continuation (inside parentheses, brackets, and strings) for triple-quoted strings ("""like this""")and the indentation of continuation lines is not important. For more info, you may want to read this article on lexical analysis, from python.org.DB related code looks easier on the eyes in multiple lines, enclosed by a pair of triple quotes:than the following one giant long line:

TensorFlow not found using pip

Yash Kumar Verma

[TensorFlow not found using pip](https://stackoverflow.com/questions/38896424/tensorflow-not-found-using-pip)

I'm trying to intstall TensorFlow using pip:What am I doing wrong? So far I've used Python and pip with no issues.

2016-08-11 12:28:24Z

I'm trying to intstall TensorFlow using pip:What am I doing wrong? So far I've used Python and pip with no issues.I found this to finally work.Edit 1: This was tested on Windows (8, 8.1, 10), Mac and Linux. Change python3 to python according to your configuration. Change py3 to py2 in the url if you are using Python 2.x.Edit 2: A list of different versions if someone needs: https://storage.googleapis.com/tensorflowEdit 3: A list of urls for the available wheel packages is available here:

https://www.tensorflow.org/install/pip#package-locationYou need a 64-bit version of Python and in your case are using a 32-bit version. As of now Tensorflow only supports 64-bit versions of Python 3.5.x and 3.6.x on Windows. See the install docs to see what is currently supportedTo check which version of Python you are running, type python or python3 to start the interpreter, and then type import struct;print(struct.calcsize("P") * 8) and that will print either 32 or 64 to tell you which bit version of Python you are running.From comments:To download a different version of Python for Windows, go to python.org/downloads/windows and scroll down until you see the version you want that ends in a "64". That will be the 64 bit version that should work with tensorflowYou need to use right version of Python and pipOn Windows 10, with Python 3.6.X version I was facing same then after checking deliberately , I noticed I had Python-32 bit installation on my 64 bit machine. Remember TensorFlow is only compatible with 64bit installation of python. Not 32 bit of PythonIf we download Python from python.org , the default installation would be 32 bit. So we have to download 64 bit installer manually to install Python 64 bit. And then add below to PATH environment.Then run gpupdate /Force on command prompt. If python command doesnt work for 64 bit restart your machine.Then run python on command prompt. It should show 64 bitThen run below command to install tensorflow CPU version(recommended)Update - Python 3.7Currently only Python 3.5 and Python 3.6 are supported officially. Tensorflow has not released binaries for Python 3.7 still officially, we might need to wait a little for it to be released. You can use Python 3.6.x alongside or Anaconda with Python<3.7 virtual environment for time being.If you are trying to install it on a windows machine you need to have a 64-bit version of python 3.5. This is the only way to actually install it. From the website:You can download the proper version of python from here (make sure you grab one of the ones that says "Windows x86-64")You should now be able to install with pip install tensorflow or python -m pip install tensorflow (make sure that you are using the right pip, from python3, if you have both python2 and python3 installed)Remember to install Anaconda 3-5.2.0 as the latest version which is 3-5.3.0 have python version 3.7 which is not supported by Tensorflow.From tensorflow website: "You will need pip version 8.1 or later for the following commands to work". Run this command to upgrade your pip, then try install tensorflow again:I figured out that TensorFlow 1.12.0 only works with Python version 3.5.2. I had Python 3.7 but that didn't work. So, I had to downgrade Python and then I could install TensorFlow to make it work.To downgrade your python version from 3.7 to 3.6Updated 11/28/2016: TensorFlow is now available in PyPI, starting with release 0.12. You can type...or......to install the CPU-only or GPU-accelerated version of TensorFlow respectively.Previous answer: TensorFlow is not yet in the PyPI repository, so you have to specify the URL to the appropriate "wheel file" for your operating system and Python version.The full list of supported configurations is listed on the TensorFlow website, but for example, to install version 0.10 for Python 2.7 on Linux, using CPU only, you would type the following command:Install Python 3.5.x 64 bit amd version here. Make sure you add Python to your PATH variable. Then open a command prompt and type should give you the following result :Now type I had the same problem and solved with this:Plus:Found on Docs.UPDATE!There are new links for new versionsFor example, for installing tensorflow v1.0.0 in OSX you need to use:instead ofI had the same error when trying to install on my Mac (using Python 2.7). A similar solution to the one I'm giving here also seemed to work for Python 3 on Windows 8.1 according to a different answer on this page by Yash Kumar VermaSolution   Step 1: go to The URL of the TensorFlow Python package section of the TensorFlow installation page and copy the URL of the relevant link for your Python installation.Step 2: open a terminal/command prompt and run the following command:

pip install --upgrade [paste copied url link here]   So for me it was the following:

pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.2.0-py2-none-any.whlUpdate (July 21 2017): I tried this with some others who were running on Windows machines with Python 3.6 and they had to change the line in Step 2 to:

python -m pip install [paste copied url link here]Update (26 July 2018): For Python 3.6.2 (not 3.7 because it's in 3.6.2 in TF Documentation), you can also use pip3 install --upgrade [paste copied URL here] in Step 2.Try this, it should work:Try this:Source: https://www.tensorflow.org/get_started/os_setup (page no longer exists)Update 2/23/17

Documentation moved to: https://www.tensorflow.org/installThis works for windows 10.0If you run into this issue recently (say, after Python 3.7 release in 2018), most likely this is caused by the lack of Python 3.7 support (yet) from the tensorflow side. Try using Python 3.6 instead if you don't mind. There are some tricks you can find from https://github.com/tensorflow/tensorflow/issues/20444, but use them at your own risk. I used the one harpone suggested - first downloaded the tensorflow wheel for Python 3.6 and then renamed it manually...The good news is that there is a pull request for 3.7 support already. Hope it will be released soon.I had the same problem. After uninstalling the 32-bit version of python and reinstalling the 64-bit version I tried reinstalling TensorFlow and it worked.Link to TensorFlow guide: https://www.tensorflow.org/install/install_windowsIf you're trying to install tensorflow in anaconda and it isn't working, then you may need to downgrade python version because only 3.6.x is currently supported while anaconda has the latest version.If you are using the Anaconda Python installation, pip install tensorflow will give the error stated above, shown below:According to the TensorFlow installation page, you will need to use the --ignore-installed flag when running pip install. However, before this can be done see this link

to ensure the TF_BINARY_URL variable is set correctly in relation to the desired version of TensorFlow that you wish to install.Unfortunately my reputation is to low to command underneath @Sujoy answer.In their docs they claim to support python 3.6.

The link provided by @mayur shows that their is indeed only a python3.5 wheel package. This is my try to install tensorflow:while python 3.5 seems to install successfully. I would love to see a python3.6 version since they claim it should also work on python3.6.Quoted :"TensorFlow supports Python 3.5.x and 3.6.x on Windows. Note that Python 3 comes with the pip3 package manager, which is the program you'll use to install TensorFlow."Source : https://www.tensorflow.org/install/install_windowsPython3.5 install :I hope i am terrible wrong here but if not ring a alarm bell 😛 Edit:

A couple of posts below someone pointed out that the following command would work and it did.Strange pip is not working 🤔This worked for me with Python 2.7 on Mac OS X Yosemite 10.10.5:There are multiple groups of answers to this question. This answer aims to generalize one group of answers:There may not be a version of TensorFlow that is compatible with your version of Python. This is particularly true if you're using a new release of Python. For example, there may be a delay between the release of a new version of Python and the release of TensorFlow for that version of Python.In this case, I believe your options are to:

1) Downgrade to the previous version of Python.

2) Compile TensorFlow from the source code.

3) Wait for a matching version of TensorFlow.For windows this worked for me,Download the wheel from this link. Then from command line navigate to your download folder where the wheel is present and simply type in the following command - pip install tensorflow-1.0.0-cp36-cp36m-win_amd64.whlI was facing the same issue. I tried the following and it worked.

installing for Mac OS X, anaconda python 2.7pip uninstall tensorflow

export TF_BINARY_URL=<get the correct url from http://tflearn.org/installation/>

pip install --upgrade $TF_BINARY_URL

Installed tensorflow-1.0.0The URL to install TensorFlow in Windows, below is the URL. It worked fine for me.For pyCharm users:https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0-py3-none-any.whl**Following steps allows you to install tensorflow and kerasupdate 2019:

for install the preview version of TensorFlow 2 in Google Colab you can use: !wget https://developer.nvidia.com/compute/cuda/10.0/Prod/local_installers/cuda-repo-ubuntu1604-10-0-local-10.0.130-410.48_1.0-1_amd64 -O cuda-repo-ubuntu1604-10-0-local-10.0.130-410.48_1.0-1_amd64.deb

!dpkg -i cuda-repo-ubuntu1604-10-0-local-10.0.130-410.48_1.0-1_amd64.deb

!apt-key add /var/cuda-repo-10-0-local-10.0.130-410.48/7fa2af80.pub

!apt-get update

!apt-get install cuda

!pip install tf-nightly-gpu-2.0-previewand for install the TensorFlow 2 bye pip you can use :

 pip install tf-nightly-gpu-2.0-preview for GPU and

 pip install tf-nightly-2.0-preview 

for CPU. I had this problem on OSX Sierra 10.12.2. It turns out I had the wrong version of Python installed (I had Python 3.4 but tensorflow pypi packages for OSX are only for python 3.5 and up). The solution was to install Python 3.6. Here's what I did to get it working. Note: I used Homebrew to install Python 3.6, you could do the same by using the Python 3.6 installer on python.orgExcerpt from tensorflow website

https://www.tensorflow.org/install/install_windowsIf your command pip install --upgrade tensorflowcompiles, then your version of tensorflow should be the newest. I personally prefer to use anaconda. You can easily install and upgrade tensorflow as follows:Also if you want to use it with your GPU you have an easy install:I've been using it for a while now and I have never had any problem.

String comparison in Python: is vs. == [duplicate]

Coquelicot

[String comparison in Python: is vs. == [duplicate]](https://stackoverflow.com/questions/2988017/string-comparison-in-python-is-vs)

I noticed a Python script I was writing was acting squirrelly, and traced it to an infinite loop, where the loop condition was while line is not ''. Running through it in the debugger, it turned out that line was in fact ''. When I changed it to !='' rather than is not '', it worked fine. Also, is it generally considered better to just use '==' by default, even when comparing int or Boolean values? I've always liked to use 'is' because I find it more aesthetically pleasing and pythonic (which is how I fell into this trap...), but I wonder if it's intended to just be reserved for when you care about finding two objects with the same id.

2010-06-07 08:31:49Z

I noticed a Python script I was writing was acting squirrelly, and traced it to an infinite loop, where the loop condition was while line is not ''. Running through it in the debugger, it turned out that line was in fact ''. When I changed it to !='' rather than is not '', it worked fine. Also, is it generally considered better to just use '==' by default, even when comparing int or Boolean values? I've always liked to use 'is' because I find it more aesthetically pleasing and pythonic (which is how I fell into this trap...), but I wonder if it's intended to just be reserved for when you care about finding two objects with the same id.Not always.  NaN is a counterexample.  But usually, identity (is) implies equality (==).  The converse is not true: Two distinct objects can have the same value.You use == when comparing values and is when comparing identities.When comparing ints (or immutable types in general), you pretty much always want the former.  There's an optimization that allows small integers to be compared with is, but don't rely on it.For boolean values, you shouldn't be doing comparisons at all.   Instead of:write:For comparing against None, is None is preferred over == None.Yes, that's exactly what it's for.I would like to show a little example on how is and == are involved in immutable types. Try that:is compares two objects in memory, == compares their values. For example, you can see that small integers are cached by Python:You should use == when comparing values and is when comparing identities. (Also, from an English point of view, "equals" is different from "is".)The logic is not flawed.  The statementshould never be read to mean It is a logical error on the part of the reader to assume that the converse of a logic statement is true.  See http://en.wikipedia.org/wiki/Converse_(logic)See This questionYour logic in reading is slightly flawed.If is applies then == will be True, but it does NOT apply in reverse. == may yield True while is yields False.

How to install pip with Python 3?

deamon

[How to install pip with Python 3?](https://stackoverflow.com/questions/6587507/how-to-install-pip-with-python-3)

I want to install pip. It should support Python 3, but it requires setuptools, which is available only for Python 2.How can I install pip with Python 3?

2011-07-05 18:58:49Z

I want to install pip. It should support Python 3, but it requires setuptools, which is available only for Python 2.How can I install pip with Python 3?edit: Manual installation and use of setuptools is not the standard process anymore.Congrats, you should already have pip installed. If you do not, read onward.You can usually install the package for pip through your package manager if your version of Python is older than 2.7.9 or 3.4, or if your system did not include it for whatever reason.Instructions for some of the more common distros follow.Run the following command from a terminal:Run the following command from a terminal:On a fresh Debian/Ubuntu install, the package may not be found until you do:On CentOS 7, you have to install setup tools first, and then use that to install pip, as there is no direct package for it.Assuming you installed Python 3.4 from EPEL, you can install Python 3's setup tools and use it to install pip.Install using the manual way detailed below.If you want to do it the manual way, the now-recommended method is to install using the get-pip.py script from pip's installation instructions.I was able to install pip for python 3 on Ubuntu just by running sudo apt-get install python3-pip. Good news! Python 3.4 (released March 2014) ships with Pip. This is the best feature of any Python release. It makes the community's wealth of libraries accessible to everyone. Newbies are no longer excluded by the prohibitive difficulty of setup. In shipping with a package manager, Python joins Ruby, Nodejs, Haskell, Perl, Go--almost every other contemporary language with a majority open-source community. Thank you Python.Of course, that doesn't mean Python packaging is problem solved. The experience remains frustrating. I discuss this at Does Python have a package/module management system?Alas for everyone using an earlier Python. Manual instructions follow.Follow my detailed instructions at  https://stackoverflow.com/a/12476379/284795 . EssentiallyPer https://pip.pypa.io/en/stable/installing.htmlDownload get-pip.py, being careful to save it as a .py file rather than .txt. Then, run it from the command prompt.You possibly need an administrator command prompt to do this. Follow http://technet.microsoft.com/en-us/library/cc947813(v=ws.10).aspxFor me, this installed Pip at C:\Python27\Scripts\pip.exe. Find pip.exe on your computer, then add its folder (eg.  C:\Python27\Scripts) to your path (Start / Edit environment variables). Now you should be able to run pip from the command line. Try installing a package:There you go (hopefully)! For Ubuntu 12.04 or older, won't work. Instead, use:if you're using python 3.4+just type:As per https://pip.pypa.io/en/latest/installing.html the current way is:I think that should work for any versionSingle Python in systemTo install packages in Python always follow these steps:Note: This is assuming no alias is set for pythonThrough this method, there will be no confusion regarding which python version is receiving the package.Multiple PythonsSay you have python3 ↔ python3.6 and python3.7 ↔ python3.7This is essentially the same method as shown previously.Note 1How to find which python, your python3 command spawns:Notice python 3.6.6 in the second line. Note 2Change what python3 or python points to:  https://askubuntu.com/questions/320996/how-to-make-python-program-command-execute-python-3I'm not sure when exactly this was introduced, but it's installed pip3 for me when it didn't already exist.If you are on macOS, use homebrew.Also note that you should check the console if the install finished successfully. Sometimes it doesn't (e.g. an error due to ownership), but people simply overlook the log.According to the official Homebrew page:So to install Python 3, run the following command:Then, the pip is installed automatically, and you can install any package by pip install <package>.If you use several different versions of python try using virtualenv http://www.virtualenv.org/en/latest/virtualenv.html#installationWith the advantage of pip for each local environment.Then install a local environment in the current directory by:Note that you specify the path to a python binary you have installed on your system.Then there are now an local pythonenvironment in that folder. ./ENVNow there should be ./ENV/pip-3.3use 

./ENV/pip-3.3 freeze to list the local installed libraries.use ./ENV/pip-3.3 install packagename to install at the local environment.use ./ENV/python3.3 pythonfile.py to run your python script.Here is my way to solve this problem at ubuntu 12.04:Then install the python3 from source code:When you finished installing all of them, pip3 will get installed automatically.This is what I did on OS X Mavericks to get this to work.Firstly, have brew installedInstall python 3.4Then I get the latest version of distribute:I hope this helps.For python3 try this:The good thing is that It will also detect what version of python you have (even if it's an environment of python in your custom location).

After this you can proceed normally with (for example)source:

https://pypi.python.org/pypi/setuptools/1.1.6#upgrading-from-setuptools-0-6Assuming you are in a highly restricted computer env (such as myself) without root access or ability to install packages...  I had never setup a fresh/standalone/raw/non-root instance of Python+virtualenv before this post.  I had do quite a bit of Googling to make this work.Then... pip, pip, pip!Final tip to newbie Pythoneers: You don't think you need virtualenv when you start, but you will be happy to have it later.  Helps with "what if" installation / upgrade scenarios for open source / shared packages.Ref: https://virtualenv.pypa.io/en/latest/installation.htmlpip is installed together when you install Python. You can use

sudo pip install (module)

or

python3 -m pip install (module).https://docs.python.org/3/whatsnew/3.4.html#whatsnew-pep-453so if you have python 3.4 installed, you can just: sudo pip3 install xxxTo install pip, securely download get-pip.py.Then run the following:Refer: PIP InstallationAnd for Windows 8.1/10 OS Users just open cmd (command prompt) write this : C:\Users\%USERNAME%\AppData\Local\Programs\Python\Python36-32\Scriptsthen just write this : pip3 install {name of package}Hint: the location of folder Python36-32 may get different for new python 3.x versions If your Linux distro came with Python already installed, you should be able to install PIP using your system’s package manager. This is preferable since system-installed versions of Python do not play nicely with the get-pip.py script used on Windows and Mac.Advanced Package Tool (Python 2.x)Advanced Package Tool (Python 3.x)pacman Package Manager (Python 2.x)pacman Package Manager (Python 3.x)Yum Package Manager (Python 2.x)Yum Package Manager (Python 3.x)Dandified Yum (Python 2.x)Dandified Yum (Python 3.x)Zypper Package Manager (Python 2.x)Zypper Package Manager (Python 3.x)Please follow below steps to install python 3 with pip:Step 1 : Install Python from download hereStep 2 : you’ll need to download get-pip.pyStep 3 : After download get-pip.py , open your commant prompt and go to directory where your get-pip.py file saved .Step 4 : Enter command python get-pip.py in cmd.Step 5 : Pip installed successfully , Verify pip installation by type command in cmd pip --version

How to set the current working directory? [duplicate]

ricardo

[How to set the current working directory? [duplicate]](https://stackoverflow.com/questions/1810743/how-to-set-the-current-working-directory)

How to set the current working directory in Python?

2009-11-27 21:53:48Z

How to set the current working directory in Python?Try os.chdirPerhaps this is what you are looking for:To set the working directory:It work for Mac also To check working directorypeople using pandas packagethe following syntax to be used to import the file in python CLI

How to read a large file line by line

384X21

[How to read a large file line by line](https://stackoverflow.com/questions/8009882/how-to-read-a-large-file-line-by-line)

I want to iterate over each line of an entire file. One way to do this is by reading the entire file, saving it to a list, then going over the line of interest. This method uses a lot of memory, so I am looking for an alternative.My code so far:Executing this code gives an error message: device active.Any suggestions?The purpose is to calculate pair-wise string similarity, meaning for each line in file, I want to calculate the Levenshtein distance with every other line.

2011-11-04 13:26:29Z

I want to iterate over each line of an entire file. One way to do this is by reading the entire file, saving it to a list, then going over the line of interest. This method uses a lot of memory, so I am looking for an alternative.My code so far:Executing this code gives an error message: device active.Any suggestions?The purpose is to calculate pair-wise string similarity, meaning for each line in file, I want to calculate the Levenshtein distance with every other line.The correct, fully Pythonic way to read a file is the following:The with statement handles opening and closing the file, including if an exception is raised in the inner block. The for line in f treats the file object f as an iterable, which automatically uses buffered I/O and memory management so you don't have to worry about large files.Two memory efficient ways in ranked order (first is best) -with is the nice and efficient pythonic way to read large files. advantages - 1) file object is automatically closed after exiting from with execution block. 2) exception handling inside the with block. 3) memory for loop iterates through the f file object line by line. internally it does buffered IO (to optimized on costly IO operations) and memory management. Sometimes one might want more fine-grained control over how much to read in each iteration. In that case use iter & yield. Note with this method one explicitly needs close the file at the end.Pitfalls and for the sake of completeness - below methods are not as good or not as elegant for reading large files but please read to get rounded understanding.In Python, the most common way to read lines from a file is to do the following: When this is done, however, the readlines() function (same applies for read() function) loads the entire file into memory, then iterates over it. A slightly better approach (the first mentioned two methods are the best) for large files is to use the fileinput module, as follows:the fileinput.input() call reads lines sequentially, but doesn't keep them in memory after they've been read or even simply so this, since file in python is iterable. With universal newline support all text file lines will seem to be terminated with '\n', whatever the terminators in the file, '\r', '\n', or '\r\n'. EDIT - To specify universal newline support:The newline parameter is only supported in Python 3 and defaults to None. The mode parameter defaults to 'r' in all cases. The U is deprecated in Python 3. In Python 2 on Windows some other mechanism appears to translate \r\n to \n.Docs:Binary mode can still parse the file into lines with in.  Each line will have whatever terminators it has in the file.Thanks to @katrielalex's answer, Python's open() doc, and iPython experiments.this is a possible way of reading a file in python:it does not allocate a full list. It iterates over the lines.Some context up front as to where I am coming from. Code snippets are at the end.When I can, I prefer to use an open source tool like H2O to do super high performance parallel CSV file reads, but this tool is limited in feature set. I end up writing a lot of code to create data science pipelines before feeding to H2O cluster for the supervised learning proper.I have been reading files like 8GB HIGGS dataset from UCI repo and even 40GB CSV files for data science purposes significantly faster by adding lots of parallelism with the multiprocessing library's pool object and map function. For example clustering with nearest neighbor searches and also DBSCAN and Markov clustering algorithms requires some parallel programming finesse to bypass some seriously challenging memory and wall clock time problems. I usually like to break the file row-wise into parts using gnu tools first and then glob-filemask them all to find and read them in parallel in the python program. I use something like 1000+ partial files commonly. Doing these tricks helps immensely with processing speed and memory limits.  The pandas dataframe.read_csv is single threaded so you can do these tricks to make pandas quite faster by running a map() for parallel execution.  You can use htop to see that with plain old sequential pandas dataframe.read_csv, 100% cpu on just one core is the actual bottleneck in pd.read_csv, not the disk at all.I should add I'm using an SSD on fast video card bus, not a spinning HD on SATA6 bus, plus 16 CPU cores.Also, another technique that I discovered works great in some applications is parallel CSV file reads all within one giant file, starting each worker at different offset into the file, rather than pre-splitting one big file into many part files. Use python's file seek() and tell() in each parallel worker to read the big text file in strips, at different byte offset start-byte and end-byte locations in the big file, all at the same time concurrently. You can do a regex findall on the bytes, and return the count of linefeeds. This is a partial sum.  Finally sum up the partial sums to get the global sum when the map function returns after the workers finished.Following is some example benchmarks using the parallel byte offset trick:I use 2 files: HIGGS.csv is 8 GB. It is from the UCI machine learning repository.  all_bin .csv is 40.4 GB and is from my current project.

I use 2 programs: GNU wc program which comes with Linux, and the pure python fastread.py program which I developed.That’s some 4.5 GB/s, or 45 Gb/s, file slurping speed.  That ain’t no spinning hard disk, my friend. That’s actually a Samsung Pro 950 SSD. Below is the speed benchmark for the same file being line-counted by gnu wc, a pure C compiled program.What is cool is you can see my pure python program essentially matched the speed of the gnu wc compiled C program in this case.  Python is interpreted but C is compiled, so this is a pretty interesting feat of speed, I think you would agree.  Of course, wc really needs to be changed to a parallel program, and then it would really beat the socks off my python program. But as it stands today, gnu wc is just a sequential program.  You do what you can, and python can do parallel today. Cython compiling might be able to help me (for some other time). Also memory mapped files was not explored yet.Conclusion: The speed is good for a pure python program compared to a C program. However, it’s not good enough to use the pure python program over the C program, at least for linecounting purpose. Generally the technique can be used for other file processing, so this python code is still good.Question: Does compiling the regex just one time and passing it to all workers will improve speed? Answer: Regex pre-compiling does NOT help in this application. I suppose the reason is that the overhead of process serialization and creation for all the workers is dominating.One more thing. 

Does parallel CSV file reading even help?  Is the disk the bottleneck, or is it the CPU?  Many so-called top-rated answers on stackoverflow contain the common dev wisdom that you only need one thread to read a file, best you can do, they say. Are they sure, though?Let’s find out:Oh yes, yes it does. Parallel file reading works quite well.  Well there you go!Ps. In case some of you wanted to know, what if the balanceFactor was 2 when using a single worker process? Well, it’s horrible:Key parts of the fastread.py python program:The def for PartitionDataToWorkers is just ordinary sequential code. I left it out in case someone else wants to get some practice on what parallel programming is like. I gave away for free the harder parts: the tested and working parallel code, for your learning benefit.Thanks to:  The open-source H2O project, by Arno and Cliff and the H2O staff for their great software and instructional videos, which have provided me the inspiration for this pure python high performance parallel byte offset reader as shown above.  H2O does parallel file reading using java, is callable by python and R programs, and is crazy fast, faster than anything on the planet at reading big CSV files.Katrielalex provided the way to open & read one file.However the way your algorithm goes it reads the whole file for each line of the file. That means the overall amount of reading a file - and computing the Levenshtein distance - will be done N*N if N is the amount of lines in the file. Since you're concerned about file size and don't want to keep it in memory, I am concerned about the resulting quadratic runtime. Your algorithm is in the O(n^2) class of algorithms which often can be improved with specialization.I suspect that you already know the tradeoff of memory versus runtime here, but maybe you would want to investigate if there's an efficient way to compute multiple Levenshtein distances in parallel. If so it would be interesting to share your solution here.How many lines do your files have, and on what kind of machine (mem & cpu power) does your algorithm have to run, and what's the tolerated runtime?Code would look like:But the questions are how do you store the distances (matrix?) and can you gain an advantage of preparing e.g. the outer_line for processing, or caching some intermediate results for reuse.If you want, for example, to check a specific line for a length greater than 10, work with what you already have available.From the python documentation for fileinput.input():further, the definition of the function is:reading between the lines, this tells me that files can be a list so you could have something like:See here for more informationI would strongly recommend not using the default file loading as it is horrendously slow. You should look into the numpy functions and the IOpro functions (e.g. numpy.loadtxt()).http://docs.scipy.org/doc/numpy/user/basics.io.genfromtxt.htmlhttps://store.continuum.io/cshop/iopro/Then you can break your pairwise operation into chunks:It's almost always much faster to load data in chunks and then do matrix operations on it than to do it element by element!!I have created a script used to cut an Apache access.log file several times a day.

So I needed to set a position cursor on last line parsed during last execution.

To this end, I used file.seek() and file.seek() methods which allows the storage of the cursor in file.My code :Best way to read large file, line by line is to use python enumerate function

Writing unit tests in Python: How do I start? [closed]

user225312

[Writing unit tests in Python: How do I start? [closed]](https://stackoverflow.com/questions/3371255/writing-unit-tests-in-python-how-do-i-start)

I completed my first proper project in Python and now my task is to write tests for it.Since this is the first time I did a project, this is the first time I would be writing tests for it.The question is, how do I start? I have absolutely no idea. Can anyone point me to some documentation/ tutorial/ link/ book that I can use to start with writing tests (and I guess unit testing in particular)Any advice will be welcomed on this topic. 

2010-07-30 12:10:06Z

I completed my first proper project in Python and now my task is to write tests for it.Since this is the first time I did a project, this is the first time I would be writing tests for it.The question is, how do I start? I have absolutely no idea. Can anyone point me to some documentation/ tutorial/ link/ book that I can use to start with writing tests (and I guess unit testing in particular)Any advice will be welcomed on this topic. If you're brand new to using unittests, the simplest approach to learn is often the best. On that basis along I recommend using py.test rather than the default unittest module.Consider these two examples, which do the same thing:Example 1 (unittest):Example 2 (pytest):Assuming that both files are named test_unittesting.py, how do we run the tests?Example 1 (unittest):Example 2 (pytest):The free Python book Dive Into Python has a chapter on unit testing that you might find useful.If you follow modern practices you should probably write the tests while you are writing your project, and not wait until your project is nearly finished.Bit late now, but now you know for next time. :)There are, in my opinion, three great python testing frameworks that are good to check out.

unittest - module comes standard with all python distributions

nose - can run unittest tests, and has less boilerplate.

pytest - also runs unittest tests, has less boilerplate, better reporting, lots of cool extra featuresTo get a good comparison of all of these, read through the introductions to each at http://pythontesting.net/start-here.

There's also extended articles on fixtures, and more there.The docs for unittest would be a good place to start.Also, it is a bit late now, but in the future please consider writing unit tests before or during the project itself. That way you can use them to test as you go along, and (in theory) you can use them as regression tests, to verify that your code changes have not broken any existing code. This would give you the full benefit of writing test cases :)unittest comes with the standard library, but I would recomend you nosetests."nose extends unittest to make testing easier."I would also recomend you pylint"analyzes Python source code looking for bugs and signs of poor quality."As others already replied, it's late to write unit tests, but not too late. The question is whether your code is testable or not. Indeed, it's not easy to put existing code under test, there is even a book about this: Working Effectively with Legacy Code (see key points or precursor PDF). Now writing the unit tests or not is your call. You just need to be aware that it could be a tedious task. You might tackle this to learn unit-testing or consider writing acceptance (end-to-end) tests first, and start writing unit tests when you'll change the code or add new feature to the project. nosetests is brilliant solution for unit-testing in python. It supports both unittest based testcases and doctests, and gets you started with it with just simple config file.

What is monkey patching?

Sergei Basharov

[What is monkey patching?](https://stackoverflow.com/questions/5626193/what-is-monkey-patching)

I am trying to understand, what is monkey patching or a monkey patch? Is that something like methods/operators overloading or delegating? Does it have anything common with these things?

2011-04-11 19:05:41Z

I am trying to understand, what is monkey patching or a monkey patch? Is that something like methods/operators overloading or delegating? Does it have anything common with these things?No, it's not like any of those things. It's simply the dynamic replacement of attributes at runtime.For instance, consider a class that has a method get_data. This method does an external lookup (on a database or web API, for example), and various other methods in the class call it. However, in a unit test, you don't want to depend on the external data source - so you dynamically replace the get_data method with a stub that returns some fixed data.Because Python classes are mutable, and methods are just attributes of the class, you can do this as much as you like - and, in fact, you can even replace classes and functions in a module in exactly the same way.But, as a commenter pointed out, use caution when monkeypatching: A simple example looks like this:Source: MonkeyPatch page on Zope wiki.Simply put, monkey patching is making changes to a module or class while the program is running. There's an example of monkey-patching in the Pandas documentation:To break this down, first we import our module:Next we create a method definition, which exists unbound and free outside the scope of any class definitions (since the distinction is fairly meaningless between a function and an unbound method, Python 3 does away with the unbound method):Next we simply attach that method to the class we want to use it on:And then we can use the method on an instance of the class, and delete the method when we're done:If you're using name-mangling (prefixing attributes with a double-underscore, which alters the name, and which I don't recommend) you'll have to name-mangle manually if you do this. Since I don't recommend name-mangling, I will not demonstrate it here.How can we use this knowledge, for example, in testing?Say we need to simulate a data retrieval call to an outside data source that results in an error, because we want to ensure correct behavior in such a case.  We can monkey patch the data structure to ensure this behavior. (So using a similar method name as suggested by Daniel Roseman:)And when we test it for behavior that relies on this method raising an error, if correctly implemented, we'll get that behavior in the test results.Just doing the above will alter the Structure object for the life of the process, so you'll want to use setups and teardowns in your unittests to avoid doing that, e.g.:(While the above is fine, it would probably be a better idea to use the mock library to patch the code. mock's patch decorator would be less error prone than doing the above, which would require more lines of code and thus more opportunities to introduce errors. I have yet to review the code in mock but I imagine it uses monkey-patching in a similar way.)According to Wikipedia:First: monkey patching is an evil hack (in my opinion).It is often used to replace a method on the module or class level with a custom implementation.The most common usecase is adding a workaround for a bug in a module or class when you can't replace the original code. In this case you replace the "wrong" code through monkey patching with an implementation inside your own module/package.Monkey patching can only be done in dynamic languages, of which python is a good example.  Changing a method at runtime instead of updating the object definition is one example;similarly, adding attributes (whether methods or variables) at runtime is considered monkey patching.  These are often done when working with modules you don't have the source for, such that the object definitions can't be easily changed.This is considered bad because it means that an object's definition does not completely or accurately describe how it actually behaves.Monkey patching is reopening the existing classes or methods in class at runtime and changing the behavior, which should be used cautiously, or you should use it only when you really need to.As Python is a dynamic programming language, Classes are mutable so you can reopen them and modify or even replace them.For more info Please refer [1]: https://medium.com/@nagillavenkatesh1234/monkey-patching-in-python-explained-with-examples-25eed0aea505

How to import a module given its name as string?

Kamil Kisiel

[How to import a module given its name as string?](https://stackoverflow.com/questions/301134/how-to-import-a-module-given-its-name-as-string)

I'm writing a Python application that takes as a command as an argument, for example:I want the application to be extensible, that is, to be able to add new modules that implement new commands without having to change the main application source. The tree looks something like:So I want the application to find the available command modules at runtime and execute the appropriate one.Python defines an __import__ function, which takes a string for a module name:So currently I have something like:This works just fine, I'm just wondering if there is possibly a more idiomatic way to accomplish what we are doing with this code.Note that I specifically don't want to get in to using eggs or extension points. This is not an open-source project and I don't expect there to be "plugins". The point is to simplify the main application code and remove the need to modify it each time a new command module is added.

2008-11-19 06:09:57Z

I'm writing a Python application that takes as a command as an argument, for example:I want the application to be extensible, that is, to be able to add new modules that implement new commands without having to change the main application source. The tree looks something like:So I want the application to find the available command modules at runtime and execute the appropriate one.Python defines an __import__ function, which takes a string for a module name:So currently I have something like:This works just fine, I'm just wondering if there is possibly a more idiomatic way to accomplish what we are doing with this code.Note that I specifically don't want to get in to using eggs or extension points. This is not an open-source project and I don't expect there to be "plugins". The point is to simplify the main application code and remove the need to modify it each time a new command module is added.With Python older than 2.7/3.1, that's pretty much how you do it. For newer versions, see importlib.import_module for Python 2 and and Python 3.You can use exec if you want to as well.Or using __import__ you can import a list of modules by doing this:Ripped straight from Dive Into Python.The recommended way for Python 2.7 and 3.1 and later is to use importlib module:e.g.As mentioned the imp module provides you loading functions:I've used these before to perform something similar.  In my case I defined a specific class with defined methods that were required.

Once I loaded the module I would check if the class was in the module, and then create an instance of that class, something like this:Use the imp module, or the more direct __import__() function.Nowadays you should use importlib. The docs actually provide a recipe for that, and it goes like:Importing a package (e.g., pluginX/__init__.py) under your current dir is actually straightforward:If you want it in your locals:same would work with globals()You can use exec:Similar as @monkut 's solution but reusable and error tolerant described here http://stamat.wordpress.com/dynamic-module-import-in-python/:The below piece worked for me:if you want to import in shell-script:For example, my module names are like jan_module/feb_module/mar_module.The following worked for me:It loads modules from the folder 'modus'. The modules have a single class with the same name as the module name. E.g. the file modus/modu1.py contains:The result is a list of dynamically loaded classes "adapters".

User input and command line arguments [closed]

Teifion

[User input and command line arguments [closed]](https://stackoverflow.com/questions/70797/user-input-and-command-line-arguments)

How do I have a Python script that a) can accept user input and how do I make it b) read in arguments if run from the command line?

2008-09-16 09:44:59Z

How do I have a Python script that a) can accept user input and how do I make it b) read in arguments if run from the command line?To read user input you can try the cmd module for easily creating a mini-command line interpreter (with help texts and autocompletion) and raw_input (input for Python 3+) for reading a line of text from the user.Command line inputs are in sys.argv. Try this in your script:There are two modules for parsing command line options: optparse (deprecated since Python 2.7, use argparse instead) and getopt. If you just want to input files to your script, behold the power of fileinput.The Python library reference is your friend.Or for Python 3:raw_input is no longer available in Python 3.x.  But raw_input was renamed input, so the same functionality exists.Documentation of the changeThe best way to process command line arguments is the argparse module.Use raw_input() to get user input.  If you import the readline module your users will have line editing and history.Careful not to use the input function, unless you know what you're doing. Unlike raw_input, input will accept any python expression, so it's kinda like evalThis simple program helps you in understanding how to feed the user input from command line and to show help on passing invalid argument.1) To find the square root of 52) Passing invalid argument other than numberUse 'raw_input' for input from a console/terminal.if you just want a command line argument like a file name or something e.g. then you can use sys.argv...sys.argv is a list where 0 is the program name, so in the above example sys.argv[1] would be "file_name.txt"If you want to have full on command line options use the optparse module.PevIf you are running Python <2.7, you need optparse, which as the doc explains will create an interface to the command line arguments that are called when your application is run.However, in Python ≥2.7, optparse has been deprecated, and was replaced with the argparse as shown above. A quick example from the docs...As of Python 3.2 2.7, there is now argparse for processing command line arguments.If it's a 3.x version then just simply use:For example, you want to input 8:x will equal 8 but it's going to be a string except if you define it otherwise.So you can use the convert command, like:In Python 2:In Python 3:

Remove specific characters from a string in Python

Matt Phillips

[Remove specific characters from a string in Python](https://stackoverflow.com/questions/3939361/remove-specific-characters-from-a-string-in-python)

I'm trying to remove specific characters from a string using Python. This is the code I'm using right now. Unfortunately it appears to do nothing to the string.How do I do this properly?

2010-10-15 03:46:21Z

I'm trying to remove specific characters from a string using Python. This is the code I'm using right now. Unfortunately it appears to do nothing to the string.How do I do this properly?Strings in Python are immutable (can't be changed).  Because of this, the effect of line.replace(...) is just to create a new string, rather than changing the old one.  You need to rebind (assign) it to line in order to have that variable take the new value, with those characters removed.Also, the way you are doing it is going to be kind of slow, relatively.  It's also likely to be a bit confusing to experienced pythonators, who will see a doubly-nested structure and think for a moment that something more complicated is going on.Starting in Python 2.6 and newer Python 2.x versions *, you can instead use str.translate, (but read on for Python 3 differences):or regular expression replacement with re.subThe characters enclosed in brackets constitute a character class.  Any characters in line which are in that class are replaced with the second parameter to sub: an empty string.In Python 3, strings are Unicode. You'll have to translate a little differently. kevpie mentions this in a comment on one of the answers, and it's noted in the documentation for str.translate.When calling the translate method of a Unicode string, you cannot pass the second parameter that we used above. You also can't pass None as the first parameter. Instead, you pass a translation table (usually a dictionary) as the only parameter. This table maps the ordinal values of characters (i.e. the result of calling ord on them) to the ordinal values of the characters which should replace them, or—usefully to us—None to indicate that they should be deleted.So to do the above dance with a Unicode string you would call something likeHere dict.fromkeys and map are used to succinctly generate a dictionary containingEven simpler, as another answer puts it, create the translation table in place:Or create the same translation table with str.maketrans:*   for compatibility with earlier Pythons, you can create a "null" translation table to pass in place of None:Here string.maketrans is used to create a translation table, which is just a string containing the characters with ordinal values 0 to 255.Am I missing the point here, or is it just the following:Put it in a loop:In regular expressions (regex), | is a logical OR and \ escapes spaces and special characters that might be actual regex commands. Whereas sub stands for substitution, in this case with the empty string ''.For the inverse requirement of only allowing certain characters in a string, you can use regular expressions with a set complement operator [^ABCabc]. For example, to remove everything except ascii letters, digits, and the hyphen:From the python regular expression documentation:The asker almost had it. Like most things in Python, the answer is simpler than you think.You don't have to do the nested if/for loop thing, but you DO need to check each character individually.Strings are immutable in Python. The replace method returns a new string after the replacement. Try:I was surprised that no one had yet recommended using the builtin filter function.Say we want to filter out everything that isn't a number. Using the filter builtin method "...is equivalent to the generator expression (item for item in iterable if function(item))" [Python 3 Builtins: Filter]In Python 3 this returns To get a printed string,I am not sure how filter ranks in terms of efficiency but it is a good thing to know how to use when doing list comprehensions and such.UPDATELogically, since filter works you could also use list comprehension and from what I have read it is supposed to be more efficient because lambdas are the wall street hedge fund managers of the programming function world. Another plus is that it is a one-liner that doesnt require any imports. For example, using the same string 's' defined above,That's it. The return will be a string of all the characters that are digits in the original string.If you have a specific list of acceptable/unacceptable characters you need only adjust the 'if' part of the list comprehension.or alternatively,Using filter, you'd just need one line

This treats the string as an iterable and checks every character if the lambda returns True:Here's some possible ways to achieve this task:PS: Instead using " ?.!/;:" the examples use the vowels... and yeah, "murcielago" is the Spanish word to say bat... funny word as it contains all the vowels :)PS2: If you're interested on performance you could measure these attempts with a simple code like:In my box you'd get:So it seems attempt4 is the fastest one for this particular input.Here's my Python 2/3 compatible version. Since the translate api has changed. How about this:You can also use a function in order to substitute different kind of regular expression or other pattern with the use of a list. With that, you can mixed regular expression, character class, and really basic text pattern. It's really useful when you need to substitute a lot of elements like HTML ones.*NB: works with Python 3.xIn the function string_cleanup, it takes your string x and your list notwanted as arguments. For each item in that list of elements or pattern, if a substitute is needed it will be done.The output:My method I'd use probably wouldn't work as efficiently, but it is massively simple. I can remove multiple characters at different positions all at once, using slicing and formatting.

Here's an example:This will result in 'removed' holding the word 'this'.Formatting can be very helpful for printing variables midway through a print string. It can insert any data type using a % followed by the variable's data type; all data types can use %s, and floats (aka decimals) and integers can use %d. Slicing can be used for intricate control over strings. When I put words[:3],  it allows me to select all the characters in the string from the beginning (the colon is before the number, this will mean 'from the beginning to') to the 4th character (it includes the 4th character). The reason 3 equals till the 4th position is because Python starts at 0. Then, when I put word[-1:], it means the 2nd last character to the end (the colon is behind the number). Putting -1 will make Python count from the last character, rather than the first. Again, Python will start at 0. So, word[-1:] basically means 'from the second last character to the end of the string.So, by cutting off the characters before the character I want to remove and the characters after and sandwiching them together, I can remove the unwanted character. Think of it like a sausage. In the middle it's dirty, so I want to get rid of it. I simply cut off the two ends I want then put them together without the unwanted part in the middle. If I want to remove multiple consecutive characters, I simply shift the numbers around in the [] (slicing part). Or if I want to remove multiple characters from different positions, I can simply sandwich together multiple slices at once.Examples:removed equals 'cool'.removed equals 'macs'.In this case, [3:5] means character at position 3 through character at position 5 (excluding the character at the final position). Remember, Python starts counting at 0, so you will need to as well.Try this one:This method works well in python 3.5.2You could use the re module's regular expression replacement. Using the ^ expression allows you to pick exactly what you want  from your string.Output to this would be "Thisisabsurd". Only things specified after the ^ symbol will appear.The string method replace does not modify the original string. It leaves the original alone and returns a modified copy.What you want is something like: line = line.replace(char,'')However, creating a new string each and every time that a character is removed is very inefficient. I recommend the following instead:Below one.. with out using regular expression concept.. e.g.,To remove all the number from the string  you can use setRecursive split:

s=string ; chars=chars to removeexample: # for each file on a directory, rename filenameEven the below approach worksoutput: abcde

How do I update pip itself from inside my virtual environment?

zakdances

[How do I update pip itself from inside my virtual environment?](https://stackoverflow.com/questions/15221473/how-do-i-update-pip-itself-from-inside-my-virtual-environment)

I'm able to update pip-managed packages, but how do I update pip itself? According to pip --version, I currently have pip 1.1 installed in my virtualenv and I want to update to the latest version. What's the command for that? Do I need to use distribute or is there a native pip or virtualenv command? I've already tried pip update and pip update pip with no success.

2013-03-05 10:29:18Z

I'm able to update pip-managed packages, but how do I update pip itself? According to pip --version, I currently have pip 1.1 installed in my virtualenv and I want to update to the latest version. What's the command for that? Do I need to use distribute or is there a native pip or virtualenv command? I've already tried pip update and pip update pip with no success.pip is just a PyPI package like any other; you could use it to upgrade itself the same way you would upgrade any package:On Windows the recommended command is:The more safe method is to run pip though a python module:On windows there seem to be a problem with binaries that try to replace themselves, this method works around that limitation.In my case my pip version was broken so the update by itself would not work.Fix:I tried all of these solutions mentioned above under Debian Jessie. They don't work, because it just takes the latest version compile by the debian package manager which is 1.5.6 which equates to version 6.0.x. Some packages that use pip as prerequisites will not work as a results, such as spaCy (which needs the option --no-cache-dir to function correctly). So the actual best way to solve these problems is to run get-pip.py downloaded using wget, from the website or using curl as follows: This will install the current version which at the time of writing this solution is 9.0.1 which is way beyond what Debian provides.Upgrading pip using 'pip install --upgrade pip' does not always work because of the dreaded cert issue: There was a problem confirming the ssl certificate: [SSL: TLSV1_ALERT_PROTOCOL_VERSION] tlsv1 alert protocol versionI like to use the one line command for virtual envs:Or if you want to install it box wide you will need you can give curl a -s flag if you want to silence the output when running in an automation script.In my case this worked from the terminal command line in Debian StableTo get this to work for me I had to drill down in the Python directory using the Python command prompt (on WIN10 from VS CODE). In my case it was in my "AppData\Local\Programs\Python\python35-32" directory. From there now I ran the command...This worked and I'm good to go.Open Command Prompt with Administrator Permissions, and repeat the command:In case you are using venv any update to pip install will result in upgrading the system pip instead of the venv pip. You need to upgrade the pip bootstrapping packages as well. I had installed Python in C:\Python\Python36 so I went to the Windows command prompt and typed "cd C:\Python\Python36 to get to the right directory. Then entered the "python -m install --upgrade pip" all good!On my lap-top with Windows 7 the right way to install latest version of pip is:pip version 10 has an issue. It will manifest as the error:The solution is to be in the venv you want to upgrade and then run:rather than justSingle Line Python Program

The best way I have found is to write a single line program that downloads and runs the official get-pip script.  See below for the code.The official docs recommend using curl to download the get-pip script, but since I work on windows and don't have curl installed I prefer using python itself to download and run the script.  Here is the single line program that can be run via the command line using Python 3:This line gets the official "get-pip.py" script as per the installation notes and executes the script with the "exec" command.  For Python2 you would replace "urllib.request" with "urllib2":Precautions

It's worth noting that running any python script blindly is inherently dangerous.  For this reason, the official instructions recommend downloading the script and inspecting it before running.  That said, many people don't actually inspect the code and just run it.  This one-line program makes that easier.I had a similar problem on a raspberry pi.The problem was that http requires SSL and so I needed to force it to use https to get around this requirement.or I was in a similar situation and wanted to update urllib3 package.

What worked for me was:Very Simple. Just download pip from https://bootstrap.pypa.io/get-pip.py . Save the file in some forlder or dekstop. I saved the file in my D drive.Then from your command prompt navigate to the folder where you have downloaded pip. Then type there

What is the difference between re.search and re.match?

Daryl Spitzer

[What is the difference between re.search and re.match?](https://stackoverflow.com/questions/180986/what-is-the-difference-between-re-search-and-re-match)

What is the difference between the search() and match() functions in the Python re module?I've read the documentation (current documentation), but I never seem to remember it.  I keep having to look it up and re-learn it.  I'm hoping that someone will answer it clearly with examples so that (perhaps) it will stick in my head.  Or at least I'll have a better place to return with my question and it will take less time to re-learn it.

2008-10-08 00:51:36Z

What is the difference between the search() and match() functions in the Python re module?I've read the documentation (current documentation), but I never seem to remember it.  I keep having to look it up and re-learn it.  I'm hoping that someone will answer it clearly with examples so that (perhaps) it will stick in my head.  Or at least I'll have a better place to return with my question and it will take less time to re-learn it.re.match is anchored at the beginning of the string. That has nothing to do with newlines, so it is not the same as using ^ in the pattern.As the re.match documentation says:re.search searches the entire string, as the documentation says:So if you need to match at the beginning of the string, or to match the entire string use match. It is faster. Otherwise use search.The documentation has a specific section for match vs. search that also covers multiline strings:Now, enough talk. Time to see some example code:search ⇒ find something anywhere in the string and return a match object.match ⇒ find something at the beginning of the string and return a match object.re.search searches for the pattern throughout the string, whereas re.match does not search the pattern; if it does not, it has no other choice than to match it at start of the string.This comment from @ivan_bilan under the accepted answer above got me thinking if such hack is actually speeding anything up, so let's find out how many tons of performance you will really gain.I prepared the following test suite:I made 10 measurements (1M, 2M, ..., 10M words) which gave me the following plot:The resulting lines are surprisingly (actually not that surprisingly) straight. And the search function is (slightly) faster given this specific pattern combination. The moral of this test: Avoid overoptimizing your code.You can refer the below example to understand the working of re.match and re.searchre.match will return none, but re.search will return abc.The difference is, re.match() misleads anyone accustomed to Perl, grep, or sed regular expression matching, and re.search() does not. :-)More soberly, As John D. Cook remarks, re.match() "behaves as if every pattern has ^ prepended."  In other words, re.match('pattern') equals re.search('^pattern').  So it anchors a pattern's left side.  But it also doesn't anchor a pattern's right side: that still requires a terminating $.Frankly given the above, I think re.match() should be deprecated.  I would be interested to know reasons it should be retained.re.match attempts to match a pattern at the beginning of the string. re.search attempts to match the pattern throughout the string until it finds a match.Much shorter:Following Ex says it:

How to change the font size on a matplotlib plot

Herman Schaaf

[How to change the font size on a matplotlib plot](https://stackoverflow.com/questions/3899980/how-to-change-the-font-size-on-a-matplotlib-plot)

How does one change the font size for all elements (ticks, labels, title) on a matplotlib plot?I know how to change the tick label sizes, this is done with:But how does one change the rest?

2010-10-10 10:43:27Z

How does one change the font size for all elements (ticks, labels, title) on a matplotlib plot?I know how to change the tick label sizes, this is done with:But how does one change the rest?From the matplotlib documentation, This sets the font of all items to the font specified by the kwargs object, font.Alternatively, you could also use the rcParams update method as suggested in this answer:orYou can find a full list of available properties on the Customizing matplotlib page.If you are a control freak like me, you may want to explicitly set all your font sizes:Note that you can also set the sizes calling the rc method on matplotlib:If you want to change the fontsize for just a specific plot that has already been created, try this:Update: See the bottom of the answer for a slightly better way of doing it.

Update #2: I've figured out changing legend title fonts too.

Update #3: There is a bug in Matplotlib 2.0.0 that's causing tick labels for logarithmic axes to revert to the default font. Should be fixed in 2.0.1 but I've included the workaround in the 2nd part of the answer.This answer is for anyone trying to change all the fonts, including for the legend, and for anyone trying to use different fonts and sizes for each thing. It does not use rc (which doesn't seem to work for me). It is rather cumbersome but I could not get to grips with any other method personally. It basically combines ryggyr's answer here with other answers on SO.The benefit of this method is that, by having several font dictionaries, you can choose different fonts/sizes/weights/colours for the various titles, choose the font for the tick labels, and choose the font for the legend, all independently.UPDATE:I have worked out a slightly different, less cluttered approach that does away with font dictionaries, and allows any font on your system, even .otf fonts. To have separate fonts for each thing, just write more font_path and font_prop like variables.Hopefully this is a comprehensive answerHere is a totally different approach that works surprisingly well to change the font sizes:Change the figure size!I usually use code like this:The smaller you make the figure size, the larger the font is relative to the plot. This also upscales the markers. Note I also set the dpi or dot per inch. I learned this from a posting the AMTA (American Modeling Teacher of America) forum.

Example from above code: Use plt.tick_params(labelsize=14)You can use plt.rcParams["font.size"] for setting font_size in matplotlib and also you can use plt.rcParams["font.family"] for setting font_family in matplotlib. Try this example:Based on the above stuff:I totally agree with Prof Huster that the simplest way to proceed is to change the size of the figure, which allows keeping the default fonts. I just had to complement this with a bbox_inches option when saving the figure as a pdf because the axis labels were cut.This is an extension to Marius Retegan answer. You can make a separate JSON file with all your modifications and than load it with rcParams.update. The changes will only apply to the current script. Soand save this 'example_file.json' in the same folder.Here is what I generally use in Jupyter Notebook:

What is the difference between __init__ and __call__?

sam

[What is the difference between __init__ and __call__?](https://stackoverflow.com/questions/9663562/what-is-the-difference-between-init-and-call)

I want to know the difference between __init__ and __call__ methods.  For example:

2012-03-12 08:09:45Z

I want to know the difference between __init__ and __call__ methods.  For example:The first is used to initialise newly created object, and receives arguments used to do that:The second implements function call operator.Defining a custom __call__() method in the meta-class allows the class's instance to be called as a function, not always modifying the instance itself.In Python, functions are first-class objects, this means: function references can be passed in inputs to other functions and/or methods, and executed from inside them.Instances of Classes (aka Objects), can be treated as if they were functions: pass them to other methods/functions and call them. In order to achieve this, the __call__ class function has to be specialized. def __call__(self, [args ...])

    It takes as an input a variable number of arguments. Assuming x being an instance of the Class X, x.__call__(1, 2) is analogous to calling x(1,2) or the instance itself as a function.In Python, __init__() is properly defined as Class Constructor (as well as __del__() is the Class Destructor). Therefore, there is a net distinction between __init__() and __call__(): the first builds an instance of Class up, the second makes such instance callable as a function would be without impacting the lifecycle of the object itself (i.e. __call__ does not impact the construction/destruction lifecycle) but it can modify its internal state (as shown below).Example.__call__ makes the instance of a class callable. 

Why would it be required?  Technically __init__ is called once by __new__ when object is created, so that it can be initialized. But there are many scenarios where you might want to redefine your object, say you are done with your object, and may find a need for a new object. With __call__ you can redefine the same object as if it were new. This is just one case, there can be many more. __init__ would be treated as Constructor where as __call__ methods can be called with objects any number of times. Both __init__ and __call__ functions do take default arguments.I will try to explain this using an example, suppose you wanted to print a fixed number of terms from fibonacci series. Remember that the first 2 terms of fibonacci series are 1s. Eg: 1, 1, 2, 3, 5, 8, 13....You want the list containing the fibonacci numbers to be initialized only once and after that it should update. Now we can use the __call__ functionality. Read @mudit verma's answer. It's like you want the object to be callable as a function but not re-initialized every time you call it.Eg:The output is:If you observe the output __init__ was called only one time that's when the class was instantiated for the first time, later on the object was being called without re-initializing.You can also use __call__ method in favor of implementing decorators.This example taken from Python 3 Patterns, Recipes and IdiomsOutput:__init__ is a special method in Python classes, it is the constructor method for a class. It is called whenever an object of the class is constructed or we can say it initialises a new object.

Example:If we use A(), it will give an error 

TypeError: __init__() missing 1 required positional argument: 'a' as it requires 1 argument a because of __init__ ......... __call__ when implemented in the Class helps us invoke the Class instance as a function call.Example:Here if we use B(), it runs just fine because it doesn't have an __init__ function here. Short and sweet answers are already provided above. I wanna provide some practical implementation as compared with Java.Note: scenario 1 and scenario 2 seems same in terms of result output. 

But in scenario1, we again create another new instance instance1. In scenario2, 

we simply modify already created instance1. __call__ is beneficial here as the system doesn't need to create new instance.Equivalent in JavaSo, __init__ is called when you are creating an instance of any class and initializing the instance variable also.Example: And __call__ is called when you call the object like any other function. Example:We can use call method to use other class methods as static methods.__call__ allows to return arbitrary values, while __init__ being an constructor returns the instance of class implicitly. As other answers properly pointed out, __init__ is called just once, while it's possible to call __call__ multiple times, in case the initialized instance is assigned to intermediate variable.

Convert string representation of list to list

harijay

[Convert string representation of list to list](https://stackoverflow.com/questions/1894269/convert-string-representation-of-list-to-list)

I was wondering what the simplest way is to convert a string list like the following to a list:Even in case user puts spaces in between the commas, and spaces inside of the quotes. I need to handle that as well to:in Python.I know I can strip spaces with strip() and split() using the split operator and check for non alphabets. But the code was getting very kludgy. Is there a quick function that I'm not aware of?

2009-12-12 18:19:03Z

I was wondering what the simplest way is to convert a string list like the following to a list:Even in case user puts spaces in between the commas, and spaces inside of the quotes. I need to handle that as well to:in Python.I know I can strip spaces with strip() and split() using the split operator and check for non alphabets. But the code was getting very kludgy. Is there a quick function that I'm not aware of?ast.literal_eval:The eval is dangerous - you shouldn't execute user input.If you have 2.6 or newer, use ast instead of eval:Once you have that, strip the strings.If you're on an older version of Python, you can get very close to what you want with a simple regular expression:This isn't as good as the ast solution, for example it doesn't correctly handle escaped quotes in strings. But it's simple, doesn't involve a dangerous eval, and might be good enough for your purpose if you're on an older Python without ast.The json module is a better solution whenever there is a stringified list of dictionaries. The json.loads(your_data) function can be used to convert it to a list.Similarly There is a quick solution:Unwanted whitespaces in the list elements may be removed in this way:Inspired from some of the answers above that work with base python packages I compared the performance of a few (using Python 3.7.3):Method 1: astMethod 2: jsonMethod 3: no importI was disappointed to see what I considered the method with the worst readability was the method with the best performance... there are tradeoffs to consider when going with the most readable option... for the type of workloads I use python for I usually value readability over a slightly more performant option, but as usual it depends.Without importing anything:Assuming that all your inputs are lists and that the double quotes in the input actually don't matter, this can be done with a simple regexp replace.  It is a bit perl-y but works like a charm.  Note also that the output is now a list of unicode strings, you didn't specify that you needed that, but it seems to make sense given unicode input.The junkers variable contains a compiled regexp (for speed) of all characters we don't want, using ] as a character required some backslash trickery.

The re.sub replaces all these characters with nothing, and we split the resulting string at the commas.   Note that this also removes spaces from inside entries u'["oh no"]' ---> [u'ohno'].  If this is not what you wanted, the regexp needs to be souped up a bit.  If you know that your lists only contain quoted strings, this pyparsing example will give you your list of stripped strings (even preserving the original Unicode-ness).If your lists can have more datatypes, or even contain lists within lists, then you will need a more complete grammar - like this one on the pyparsing wiki, which will handle tuples, lists, ints, floats, and quoted strings.  Will work with Python versions back to 2.4.To further complete @Ryan 's answer using json, one very convenient function to convert unicode is the one posted here: https://stackoverflow.com/a/13105359/7599285ex with double or single quotes:I would like to provide a more intuitive patterning solution with regex. 

The below function takes as input a stringified list containing arbitrary strings. Stepwise explanation:

You remove all whitespacing,bracketing and value_separators (provided they are not part of the values you want to extract, else make the regex more complex). Then you split the cleaned string on single or double quotes and take the non-empty values (or odd indexed values, whatever the preference). testsample: "['21',"foo" '6', '0', " A"]"and with pure python - not importing any librariesSo, following all the answers I decided to time the most common methods:So in the end regex wins!you can save yourself the .strip() fcn by just slicing off the first and last characters from the string representation of the list (see third line below)

How would you make a comma-separated string from a list of strings?

mweerden

[How would you make a comma-separated string from a list of strings?](https://stackoverflow.com/questions/44778/how-would-you-make-a-comma-separated-string-from-a-list-of-strings)

What would be your preferred way to concatenate strings from a sequence such that between every two consecutive pairs a comma is added. That is, how do you map, for instance, ['a', 'b', 'c'] to 'a,b,c'? (The cases ['s'] and [] should be mapped to 's' and '', respectively.)I usually end up using something like ''.join(map(lambda x: x+',',l))[:-1], but also feeling somewhat unsatisfied.

2008-09-04 21:04:04Z

What would be your preferred way to concatenate strings from a sequence such that between every two consecutive pairs a comma is added. That is, how do you map, for instance, ['a', 'b', 'c'] to 'a,b,c'? (The cases ['s'] and [] should be mapped to 's' and '', respectively.)I usually end up using something like ''.join(map(lambda x: x+',',l))[:-1], but also feeling somewhat unsatisfied.This won't work if the list contains numbers.And if the list contains non-string types (such as integers, floats, bools, None) then do:Why the map/lambda magic? Doesn't this work?In case if there are numbers in the list, you could use list comprehension:or a generator expression:",".join(l) will not work for all cases. I'd suggest using the csv module with StringIOHere is a alternative solution in Python 3.0 which allows non-string list items:NOTE: The space after comma is intentional.Don't you just want:Obviously it gets more complicated if you need to quote/escape commas etc in the values. In that case I would suggest looking at the csv module in the standard library:https://docs.python.org/library/csv.html@Peter HoffmannUsing generator expressions has the benefit of also producing an iterator but saves importing itertools. Furthermore, list comprehensions are generally preferred to map, thus, I'd expect generator expressions to be preferred to imap.my_list may contain any type of variables. This avoid the result 'A,,,D,E'.@jmanning2k using a list comprehension has the downside of creating a new temporary list. The better solution would be using itertools.imap which returns an iteratorHere is an example with listMore Accurate:-Example 2:-I would say the csv library is the only sensible option here, as it was built to cope with all csv use cases such as commas in a string, etc.To output a list l to a .csv file:It is also possible to use writer.writerows(iterable) to output multiple rows to csv.This example is compatible with Python 3, as the other answer here used StringIO which is Python 2.Unless I'm missing something, ','.join(foo) should do what you're asking for.(edit:  and as jmanning2k points out, is safer and quite Pythonic, though the resulting string will be difficult to parse if the elements can contain commas -- at that point, you need the full power of the csv module, as Douglas points out in his answer.)My two cents. I like simpler an one-line code in python:It's pythonic, works for strings, numbers, None and empty string. It's short and satisfies the requirements. If the list is not going to contain numbers, we can use this simpler variation:Also this solution doesn't create a new list, but uses an iterator, like @Peter Hoffmann pointed (thanks).

What is the python keyword「with」used for? [duplicate]

MikeN

[What is the python keyword「with」used for? [duplicate]](https://stackoverflow.com/questions/1369526/what-is-the-python-keyword-with-used-for)

What is the python keyword "with" used for?Example from: http://docs.python.org/tutorial/inputoutput.html

2009-09-02 18:57:51Z

What is the python keyword "with" used for?Example from: http://docs.python.org/tutorial/inputoutput.htmlIn python the with keyword is used when working with unmanaged resources (like file streams). It is similar to the using statement in VB.NET and C#. It allows you to ensure that a resource is "cleaned up" when the code that uses it finishes running, even if exceptions are thrown. It provides 'syntactic sugar' for try/finally blocks. From Python Docs: Update fixed VB callout per Scott Wisniewski's comment. I was indeed confusing with with using.Explanation from the Preshing on Programming blog:

What is the common header format of Python files?

Ashwin Nanjappa

[What is the common header format of Python files?](https://stackoverflow.com/questions/1523427/what-is-the-common-header-format-of-python-files)

I came across the following header format for Python source files in a document about Python coding guidelines:Is this the standard format of headers in the Python world?

What other fields/information can I put in the header?

Python gurus share your guidelines for good Python source headers :-)

2009-10-06 03:23:14Z

I came across the following header format for Python source files in a document about Python coding guidelines:Is this the standard format of headers in the Python world?

What other fields/information can I put in the header?

Python gurus share your guidelines for good Python source headers :-)Its all metadata for the Foobar module.The first one is the docstring of the module, that is already explained in Peter's answer.Here you have more information, listing __author__, __authors__, __contact__, __copyright__, __license__, __deprecated__, __date__ and __version__ as recognized metadata.I strongly favour minimal file headers, by which I mean just:ie. three groups of imports, with a single blank line between them. Within each group, imports are sorted. The final group, imports from local source, can either be absolute imports as shown, or explicit relative imports.Everything else is a waste of time, visual space, and is actively misleading.If you have legal disclaimers or licencing info, it goes into a separate file. It does not need to infect every source code file. Your copyright should be part of this.  People should be able to find it in your LICENSE file, not random source code.Metadata such as authorship and dates is already maintained by your source control. There is no need to add a less-detailed, erroneous, and out-of-date version of the same info in the file itself.I don't believe there is any other data that everyone needs to put into all their source files. You may have some particular requirement to do so, but such things apply, by definition, only to you. They have no place in「general headers recommended for everyone」.The answers above are really complete, but if you want a quick and dirty header to copy'n paste, use this:Why this is a good one:See also: https://www.python.org/dev/peps/pep-0263/If you just write a class in each file, you don't even need the documentation (it would go inside the class doc).Also see PEP 263 if you are using a non-ascii characterset

How do I get my Python program to sleep for 50 milliseconds?

TK.

[How do I get my Python program to sleep for 50 milliseconds?](https://stackoverflow.com/questions/377454/how-do-i-get-my-python-program-to-sleep-for-50-milliseconds)

How do I get my Python program to sleep for 50 milliseconds?

2008-12-18 10:20:24Z

How do I get my Python program to sleep for 50 milliseconds?ReferenceNote that if you rely on sleep taking exactly 50 ms, you won't get that. It will just be about it.can also using pyautogui asif first is not None, then it will pause for first arg second, in this example:0.05 secif first is None, and second arg is True, then it will sleep for global pause setting which is set withif you are wondering the reason, see the source code:

How to calculate number of days between two given dates?

Ray Vega

[How to calculate number of days between two given dates?](https://stackoverflow.com/questions/151199/how-to-calculate-number-of-days-between-two-given-dates)

If I have two dates (ex. '8/18/2008' and '9/26/2008'), what is the best way to get the number of days between these two dates?

2008-09-29 23:36:25Z

If I have two dates (ex. '8/18/2008' and '9/26/2008'), what is the best way to get the number of days between these two dates?If you have two date objects, you can just subtract them, which computes a timedelta object.The relevant section of the docs:

https://docs.python.org/library/datetime.html.See this answer for another example.Using the power of datetime:Days until Christmas:More arithmetic here.You want the datetime module. Another example:As pointed out hereIt also can be easily done with arrow:For reference: http://arrow.readthedocs.io/en/latest/without using Lib just pure code:For calculating dates and times there are several options but I will write the simple way:Hope it helpseveryone has answered excellently using the date,

let me try to answer it using pandasThis will give the answer.

In case one of the input is dataframe column. simply use dt.days in place of daysThis assumes, of course, that you've already verified that your dates are in the format r'\d+/\d+/\d+'.Here are three ways to go with this problem :There is also a datetime.toordinal() method that was not mentioned yet:https://docs.python.org/3/library/datetime.html#datetime.date.toordinalSeems well suited for calculating days difference, though not as readable as timedelta.days.

python exception message capturing

Hellnar

[python exception message capturing](https://stackoverflow.com/questions/4690600/python-exception-message-capturing)

This doesn't seem to work, I get syntax error, what is the proper way of doing this for logging all kind of exceptions to a file

2011-01-14 11:33:50Z

This doesn't seem to work, I get syntax error, what is the proper way of doing this for logging all kind of exceptions to a fileYou have to define which type of exception you want to catch. So write except Exception, e: instead of except, e: for a general exception (that will be logged anyway).Other possibility is to write your whole try/except code this way:in Python 3.x and modern versions of Python 2.x use except Exception as e instead of except Exception, e:The syntax is no longer supported in python 3. Use the following instead.Updating this to something simpler for logger (works for both python 2 and 3). You do not need traceback module.This is now the old way (though still works):exc_value is the error message.There are some cases where you can use the e.message or e.messages.. But it does not work in all cases. Anyway the more safe is to use the str(e) If you want the error class, error message and stack trace (or some of those), use sys.exec_info().Minimal working code with some formatting:Which gives the following output:The function sys.exc_info() gives you details about the most recent exception. It returns a tuple of (type, value, traceback).traceback is an instance of traceback object. You can format the trace with the methods provided. More can be found in the traceback documentation .You can use logger.exception("msg") for logging exception with traceback:After python 3.6, you can use formatted string literal. It's neat! (https://docs.python.org/3/whatsnew/3.6.html#whatsnew36-pep498)You can try specifying the BaseException type explicitly. However, this will only catch derivatives of BaseException. While this includes all implementation-provided exceptions, it is also possibly to raise arbitrary old-style classes.Use str(ex) to print execption

How do I use itertools.groupby()?

James Sulak

[How do I use itertools.groupby()?](https://stackoverflow.com/questions/773/how-do-i-use-itertools-groupby)

I haven't been able to find an understandable explanation of how to actually use Python's itertools.groupby() function.  What I'm trying to do is this:I've reviewed the documentation, and the examples, but I've had trouble trying to apply them beyond a simple list of numbers. So, how do I use of itertools.groupby()?  Is there another technique I should be using?  Pointers to good "prerequisite" reading would also be appreciated.

2008-08-03 18:27:09Z

I haven't been able to find an understandable explanation of how to actually use Python's itertools.groupby() function.  What I'm trying to do is this:I've reviewed the documentation, and the examples, but I've had trouble trying to apply them beyond a simple list of numbers. So, how do I use of itertools.groupby()?  Is there another technique I should be using?  Pointers to good "prerequisite" reading would also be appreciated.IMPORTANT NOTE: You have to sort your data first.The part I didn't get is that in the example constructionk is the current grouping key, and g is an iterator that you can use to iterate over the group defined by that grouping key. In other words, the groupby iterator itself returns iterators.Here's an example of that, using clearer variable names:This will give you the output:In this example, things is a list of tuples where the first item in each tuple is the group the second item belongs to. The groupby() function takes two arguments: (1) the data to group and (2) the function to group it with. Here, lambda x: x[0] tells groupby() to use the first item in each tuple as the grouping key.In the above for statement, groupby returns three (key, group iterator) pairs - once for each unique key. You can use the returned iterator to iterate over each individual item in that group.Here's a slightly different example with the same data, using a list comprehension:This will give you the output:The example on the Python docs is quite straightforward:So in your case, data is a list of nodes, keyfunc is where the logic of your criteria function goes and then groupby() groups the data.You must be careful to sort the data by the criteria before you call groupby or it won't work. groupby method actually just iterates through a list and whenever the key changes it creates a new group.itertools.groupby is a tool for grouping items.From the docs, we glean further what it might do:groupby objects yield key-group pairs where the group is a generator.FeaturesComparisons    UsesNote: Several of the latter examples derive from Víctor Terrón's PyCon (talk) (Spanish), "Kung Fu at Dawn with Itertools". See also the groupbysource code written in C.* A function where all items are passed through and compared, influencing the result.  Other objects with key functions include  sorted(), max() and min().ResponseA neato trick with groupby is to run length encoding in one line:will give you a list of 2-tuples where the first element is the char and the 2nd is the number of repetitions.Edit: Note that this is what separates itertools.groupby from the SQL GROUP BY semantics: itertools doesn't (and in general can't) sort the iterator in advance, so groups with the same "key" aren't merged.Another example:results inNote that igroup is an iterator (a sub-iterator as the documentation calls it).This is useful for chunking a generator:Another example of groupby - when the keys are not sorted.  In the following example, items in xx are grouped by values in yy.  In this case, one set of zeros is output first, followed by a set of ones, followed again by a set of zeros.Produces:WARNING:The syntax list(groupby(...)) won't work the way that you intend. It seems to destroy the internal iterator objects, so usingwill produce:Instead, of list(groupby(...)), try [(k, list(g)) for k,g in groupby(...)], or if you use that syntax often,and get access to the groupby functionality while avoiding those pesky (for small data) iterators all together.I would like to give another example where groupby without sort is not working. Adapted from example by James Sulakoutput isthere are two groups with vehicule, whereas one could expect only one group@CaptSolo, I tried your example, but it didn't work.Output:As you can see, there are two o's and two e's, but they got into separate groups. That's when I realized you need to sort the list passed to the groupby function. So, the correct usage would be:Output:Just remembering, if the list is not sorted, the groupby function will not work!You can use groupby to group things to iterate over. You give groupby an iterable, and a optional key function/callable by which to check the items as they come out of the iterable, and it returns an iterator that gives a two-tuple of the result of the key callable and the actual items in another iterable. From the help:Here's an example of groupby using a coroutine to group by a count, it uses a key callable (in this case, coroutine.send) to just spit out the count for however many iterations and a grouped sub-iterator of elements:printsOne useful example that I came across may be helpful:Sample input: 14445221Sample output: (1,1) (3,4) (1,5) (2,2) (1,1)You can write own groupby function:

Why does「not(True) in [False, True]」return False?

Texom512

[Why does「not(True) in [False, True]」return False?](https://stackoverflow.com/questions/31421379/why-does-nottrue-in-false-true-return-false)

If I do this:That returns True. Simply because False is in the list.But if I do:That returns False. Whereas not(True) is equal to False:Why?

2015-07-15 04:12:58Z

If I do this:That returns True. Simply because False is in the list.But if I do:That returns False. Whereas not(True) is equal to False:Why?Operator precedence 2.x, 3.x. The precedence of not is lower than that of in. So it is equivalent to:This is what you want:As @Ben points out: It's recommended to never write not(True), prefer not True. The former makes it look like a function call, while not is an operator, not a function.not x in y is evaluated as x not in yYou can see exactly what's happening by disassembling the code.  The first case works as you expect:The second case, evaluates to True not in [False, True], which is False clearly:What you wanted to express instead was (not(True)) in [False, True], which as expected is True, and you can see why:Operator precedence. in binds more tightly than not, so your expression is equivalent to not((True) in [False, True]).It's all about operator precedence (in is stronger than not). But it can be easily corrected by adding parentheses at the right place:writing:is the same like:which looks if True is in the list and returns the "not" of the result.It is evaluating as not True in [False, True], which returns False because True is in [False, True] If you try You get the expected result.Alongside the other answers that mentioned the precedence of not is lower than in, actually your statement is equivalent to :But note that if you don't separate your condition from the other ones, python will use 2 roles (precedence or chaining) in order to separate that, and in this case python used precedence. Also, note that if you want to separate a condition you need to put all the condition in parenthesis not just the object or value :But as mentioned, there is another modification by python on operators that is chaining:Based on python documentation :For example the result of following statement is False:Because python will chain the statements like following :Which exactly is False and True that is False. You can assume that the central object will be shared between 2 operations and other objects (False in this case).And note that its also true for all Comparisons, including membership tests and identity tests operations which are following operands :Example :Another famous example is number range :which is equal to :Let's see it as a collection containment checking operation: [False, True] is a list containing some elements.The expression True in [False, True] returns True, as True is an element contained in the list.Therefore, not True in [False, True] gives the "boolean opposite", not result of the above expression (without any parentheses to preserve precedence, as in has greater precedence than not operator).

Therefore, not True will result False.On the other hand, (not True) in [False, True], is equal to False in [False, True], which is True (False is contained in the list).To clarify on some of the other answers, adding parentheses after a unary operator does not change its precedence. not(True) does not make not bind more tightly to True. It's just a redundant set of parentheses around True. It's much the same as (True) in [True, False]. The parentheses don't do anything.  If you want the binding to be more tight, you have to put the parentheses around the whole expression, meaning both the operator and the operand, i.e., (not True) in [True, False].To see this another way, consider ** binds more tightly than -, which is why you get the negative of two squared, not the square of negative two (which would be positive four). What if you did want the square of negative two? Obviously, you'd add parentheses:However, it's not reasonable to expect the following to give 4because -(2) is the same as -2. The parentheses do absolutely nothing. not(True) is exactly the same. 

Filter dict to contain only certain keys?

mpen

[Filter dict to contain only certain keys?](https://stackoverflow.com/questions/3420122/filter-dict-to-contain-only-certain-keys)

I've got a dict that has a whole bunch of entries. I'm only interested in a select few of them. Is there an easy way to prune all the other ones out?

2010-08-06 00:08:36Z

I've got a dict that has a whole bunch of entries. I'm only interested in a select few of them. Is there an easy way to prune all the other ones out?Constructing a new dict: Uses dictionary comprehension. If you use a version which lacks them (ie Python 2.6 and earlier), make it dict((your_key, old_dict[your_key]) for ...). It's the same, though uglier.Note that this, unlike jnnnnn's version, has stable performance (depends only on number of your_keys) for old_dicts of any size. Both in terms of speed and memory. Since this is a generator expression, it processes one item at a time, and it doesn't looks through all items of old_dict.Removing everything in-place:Slightly more elegant dict comprehension:Here's an example in python 2.6:The filtering part is the if statement.This method is slower than delnan's answer if you only want to select a few of very many keys.You can do that with project function from my funcy library:Also take a look at select_keys.Code 1:Code 2:Code 3:All pieced of code performance are measured with timeit using number=1000, and collected 1000 times for each piece of code. For python 3.6 the performance of three ways of filter dict keys almost the same. For python 2.7 code 3 is slightly faster.This one liner lambda should work:Here's an example:It's a basic list comprehension iterating over your dict keys (i in x) and outputs a list of tuple (key,value) pairs if the key lives in your desired key list (y). A dict() wraps the whole thing to output as a dict object.Given your original dictionary orig and the set of entries that you're interested in keys:which isn't as nice as delnan's answer, but should work in every Python version of interest. It is, however, fragile to each element of keys existing in your original dictionary.Based on the accepted answer by delnan.What if one of your wanted keys aren't in the old_dict? The delnan solution will throw a KeyError exception that you can catch. If that's not what you need maybe you want to:This function will do the trick:Just like delnan's version, this one uses dictionary comprehension and has stable performance for large dictionaries (dependent only on the number of keys you permit, and not the total number of keys in the dictionary).And just like MyGGan's version, this one allows your list of keys to include keys that may not exist in the dictionary.And as a bonus, here's the inverse, where you can create a dictionary by excluding certain keys in the original:Note that unlike delnan's version, the operation is not done in place, so the performance is related to the number of keys in the dictionary. However, the advantage of this is that the function will not modify the dictionary provided.Edit: Added a separate function for excluding certain keys from a dict.If we want to make a new dictionary with selected keys removed, we can make use of dictionary comprehension

For example:Another option:But you get a list (Python 2) or an iterator (Python 3) returned by filter(), not a dict.    Short form:As most of the answers suggest in order to maintain the conciseness we have to create a duplicate object be it a list or dict. This one creates a throw-away list but deletes the keys in original dict. Here is another simple method using del in one liner:e_keys is the list of the keys to be excluded. It will update your dict rather than giving you a new one.If you want a new output dict, then make a copy of the dict before deleting:You could use python-benedict, it's a dict subclass.Installation: pip install python-benedictIt's open-source on GitHub: https://github.com/fabiocaccamo/python-benedict Disclaimer: I'm the author of this library.

Find intersection of two nested lists?

elfuego1

[Find intersection of two nested lists?](https://stackoverflow.com/questions/642763/find-intersection-of-two-nested-lists)

I know how to get an intersection of two flat lists:or But when I have to find intersection for nested lists then my problems starts:In the end I would like to receive:Can you guys give me a hand with this?

2009-03-13 13:42:44Z

I know how to get an intersection of two flat lists:or But when I have to find intersection for nested lists then my problems starts:In the end I would like to receive:Can you guys give me a hand with this?If you want:Then here is your solution for Python 2:In Python 3 filter returns an iterable instead of list, so you need to wrap filter calls with list():Explanation: The filter part takes each sublist's item and checks to see if it is in the source list c1. 

The list comprehension is executed for each sublist in c2. You don't need to define intersection.  It's already a first-class part of set.For people just looking to find the intersection of two lists, the Asker provided two methods:But there is a hybrid method that is more efficient, because you only have to do one conversion between list/set, as opposed to three:This will run in O(n), whereas his original method involving list comprehension will run in O(n^2)Flatten variant:Nested variant:The functional approach:and it can be applied to the more general case of 1+ listsThe & operator takes the intersection of two sets.A pythonic way of taking the intersection of 2 lists is:You should flatten using this code ( taken from http://kogs-www.informatik.uni-hamburg.de/~meine/python_tricks ), the code is untested, but I'm pretty sure it works:After you had flattened the list, you perform the intersection in the usual way:Since intersect was defined, a basic list comprehension is enough:Improvement thanks to S. Lott's remark and TM.'s associated remark:Given:I find the following code works well and maybe more concise if using set operation:It got:If order needed:we got:By the way, for a more python style, this one is fine too:I don't know if I am late in answering your question. After reading your question I came up with a function intersect() that can work on both list and nested list. I used recursion to define this function, it is very intuitive. Hope it is what you are looking for:Example:Do you consider [1,2] to intersect with [1, [2]]? That is, is it only the numbers you care about, or the list structure as well?If only the numbers, investigate how to "flatten" the lists, then use the set() method.I was also looking for a way to do it, and eventually it ended up like this:We can use set methods for this:To define intersection that correctly takes into account the cardinality of the elements use Counter:Here's one way to set c3 that doesn't involve sets:But if you prefer to use just one line, you can do this:It's a list comprehension inside a list comprehension, which is a little unusual, but I think you shouldn't have too much trouble following it.For me this is very elegant and quick way to to it :)All you need to use initializer - third argument in the reduce function.Above code works for both python2 and python3, but you need to import reduce module as from functools import reduce. Refer below link for details.

Is there any pythonic way to combine two dicts (adding values for keys that appear in both)?

Derrick Zhang

[Is there any pythonic way to combine two dicts (adding values for keys that appear in both)?](https://stackoverflow.com/questions/11011756/is-there-any-pythonic-way-to-combine-two-dicts-adding-values-for-keys-that-appe)

For example I have two dicts:I need a pythonic way of 'combining' two dicts such that the result is:That is to say: if a key appears in both dicts, add their values, if it appears in only one dict, keep its value.

2012-06-13 09:17:28Z

For example I have two dicts:I need a pythonic way of 'combining' two dicts such that the result is:That is to say: if a key appears in both dicts, add their values, if it appears in only one dict, keep its value.Use collections.Counter:Counters are basically a subclass of dict, so you can still do everything else with them you'd normally do with that type, such as iterate over their keys and values.A more generic solution, which works for non-numeric values as well:or even more generic:For example:Intro:

There are the (probably) best solutions. But you have to know it and remember it and sometimes you have to hope that your Python version isn't too old or whatever the issue could be.Then there are the most 'hacky' solutions. They are great and short but sometimes are hard to understand, to read and to remember.There is, though, an alternative which is to to try to reinvent the wheel.

- Why reinventing the wheel?

- Generally because it's a really good way to learn (and sometimes just because the already-existing tool doesn't do exactly what you would like and/or the way you would like it) and the easiest way if you don't know or don't remember the perfect tool for your problem.So, I propose to reinvent the wheel of the Counter class from the collections module (partially at least):There would probably others way to implement that and there are already tools to do that but it's always nice to visualize how things would basically works.The one with no extra imports!Their is a pythonic standard called EAFP(Easier to Ask for Forgiveness than Permission). Below code is based on that python standard.EDIT: thanks to jerzyk for his improvement suggestions.Definitely summing the Counter()s is the most pythonic way to go in such cases but only if it results in a positive value. Here is an example and as you can see there is no c in result after negating the c's value in B dictionary.That's because Counters were primarily designed to work with positive integers to represent running counts (negative count is meaningless). But to help with those use cases,python documents the minimum range and type restrictions as follows:So for getting around that problem after summing your Counter you can use Counter.update in order to get the desire output. It works like dict.update() but adds counts instead of replacing them.OR Alternative you can use Counter as @Martijn has mentioned above.For a more generic and extensible way check mergedict. It uses singledispatch and can merge values based on its types.Example:Thanks to @tokeinizer_fsj that told me in a comment that I didn't get completely the meaning of the question (I thought that add meant just adding keys that eventually where different in the two dictinaries and, instead, i meant that the common key values should be summed). So I added that loop before the merging, so that the second dictionary contains the sum of the common keys. The last dictionary will be the one whose values will last in the new dictionary that is the result of the merging of the two, so I thing the problem is solved. The solution is valid from python 3.5 and following versions.Additionally, please note a.update( b ) is 2x faster than a + bThis is a simple solution for merging two dictionaries where += can be applied to the values, it has to iterate over a dictionary only onceYou could easily generalize this:Then it can take any number of dicts.This solution is easy to use, it is used as a normal dictionary, but you can use the sum function.The above solutions are great for the scenario where you have a small number of Counters. If you have a big list of them though, something like this is much nicer:The above solution is essentially summing the Counters by:This does the same thing but I think it always helps to see what it is effectively doing underneath.Merging three dicts a,b,c in a single line without any other modules or libsIf we have the three dictsMerge all with a single line and return a dict object usingReturningWhat about:Output:The best to use is dict():

Best practice for Python assert

meade

[Best practice for Python assert](https://stackoverflow.com/questions/944592/best-practice-for-python-assert)

2009-06-03 12:57:16Z

To be able to automatically throw an error when x become less than zero throughout the function. You can use class descriptors. Here is an example:Asserts should be used to test conditions that should never happen.  The purpose is to crash early in the case of a corrupt program state.Exceptions should be used for errors that can conceivably happen, and you should almost always create your own Exception classes.For example, if you're writing a function to read from a configuration file into a dict, improper formatting in the file should raise a ConfigurationSyntaxError, while you can assert that you're not about to return None.In your example, if x is a value set via a user interface or from an external source, an exception is best.If x is only set by your own code in the same program, go with an assertion."assert" statements are removed when the compilation is optimized.  So, yes, there are both performance and functional differences.If you use assert to implement application functionality, then optimize the deployment to production, you will be plagued by "but-it-works-in-dev" defects.See PYTHONOPTIMIZE and -O -OOAssume you work on 200,000 lines of code with four colleagues Alice, Bernd, Carl, and Daphne.

They call your code, you call their code.Then assert has four roles:In my mind, assert's two purposes of documentation (1 and 3) and 

safeguard (2 and 4) are equally valuable.

Informing the people may even be more valuable than informing the computer

because it can prevent the very mistakes the assert aims to catch (in case 1)

and plenty of subsequent mistakes in any case.In addition to the other answers, asserts themselves throw exceptions, but only AssertionErrors. From a utilitarian standpoint, assertions aren't suitable for when you need fine grain control over which exceptions you catch.The only thing that's really wrong with this approach is that it's hard to make a very descriptive exception using assert statements.  If you're looking for the simpler syntax, remember you can also do something like this:Another problem is that using assert for normal condition-checking is that it makes it difficult to disable the debugging asserts using the -O flag.The English language word assert here is used in the sense of swear, affirm, avow. It doesn't mean "check" or "should be". It means that you as a coder are making a sworn statement here:If the code is correct, barring Single-event upsets, hardware failures and such, no assert will ever fail. That is why the behaviour of the program to an end user must not be affected. Especially, an assert cannot fail even under exceptional programmatic conditions. It just doesn't ever happen. If it happens, the programmer should be zapped for it.As has been said previously, assertions should be used when your code SHOULD NOT ever reach a point, meaning there is a bug there. Probably the most useful reason I can see to use an assertion is an invariant/pre/postcondition. These are something that must be true at the start or end of each iteration of a loop or a function.For example, a recursive function (2 seperate functions so 1 handles bad input and the other handles bad code, cause it's hard to distinguish with recursion). This would make it obvious if I forgot to write the if statement, what had gone wrong.These loop invariants often can be represented with an assertion.Well, this is an open question, and I have two aspects that I want to touch on: when to add assertions and how to write the error messages.To explain it to a beginner - assertions are statements which can raise errors, but you won't be catching them. And they normally should not be raised, but in real life they sometimes do get raised anyway. And this is a serious situation, which the code cannot recover from, what we call a 'fatal error'.Next, it's for 'debugging purposes', which, while correct, sounds very dismissive. I like the 'declaring invariants, which should never be violated' formulation better, although it works differently on different beginners... Some 'just get it', and others either don't find any use for it, or replace normal exceptions, or even control flow with it.In Python, assert is a statement, not a function! (remember assert(False, 'is true') will not raise. But, having that out of the way:When, and how, to write the optional 'error message'?This acually applies to unit testing frameworks, which often have many dedicated methods to do assertions (assertTrue(condition), assertFalse(condition), assertEqual(actual, expected) etc.). They often also provide a way to comment on the assertion.In throw-away code you could do without the error messages.In some cases, there is nothing to add to the assertion:def dump(something):

       assert isinstance(something, Dumpable)

       # ...But apart from that, a message is useful for communication with other programmers (which are sometimes interactive users of your code, e.g. in Ipython/Jupyter etc.).Give them information, not just leak internal implementation details.instead of:write:or maybe even:I know, I know - this is not a case for a static assertion, but I want to point to the informational value of the message.This may be conroversial, but it hurts me to read things like:Then, getting AssertionError: a must be equal to b is also readable, and the statement looks logical in code. Also, you can get something out of it without reading the traceback (which can sometimes not even be available).An Assert is to check -

1. the valid condition, 

2. the valid statement, 

3. true logic;

of  source code. Instead of failing the whole project it gives an alarm that something is not appropriate in your source file.In example 1, since variable 'str' is not null. So no any assert or exception get raised.Example 1:In example 2, var 'str' is null. So we are saving the user from going ahead of faulty program by assert statement. Example 2:The moment we don't want debug and realized the assertion issue in the source code. Disable the optimization flagpython -O assertStatement.py

nothing will get print In IDE's such as PTVS, PyCharm, Wing assert isinstance() statements can be used to enable code completion for some unclear objects.Both the use of assert and the raising of exceptions are about communication.Therefore, if you consider the occurence of a specific situation at run-time as a bug that you would like to inform the developers about ("Hi developer, this condition indicates that there is a bug somewhere, please fix the code.") then go for an assertion.  If the assertion checks input arguments of your code, you should typically add to the documentation that your code has "undefined behaviour" when the input arguments violate that conditions.If instead the occurrence of that very situation is not an indication of a bug in your eyes, but instead a (maybe rare but) possible situation that you think should rather be handled by the client code, raise an exception.  The situations when which exception is raised should be part of the documentation of the respective code.The evaluation of assertions takes some time.  They can be eliminated at compile time, though.  This has some consequences, however, see below.Normally assertions improve the maintainability of the code, since they improve readability by making assumptions explicit and during run-time regularly verifying these assumptions.  This will also help catching regressions.  There is one issue, however, that needs to be kept in mind: Expressions used in assertions should have no side-effects.  As mentioned above, assertions can be eliminated at compile time - which means that also the potential side-effects would disappear.  This can - unintendedly - change the behaviour of the code.For what it's worth, if you're dealing with code which relies on assert to function properly, then adding the following code will ensure that asserts are enabled:

Converting a Pandas GroupBy output from Series to DataFrame

saveenr

[Converting a Pandas GroupBy output from Series to DataFrame](https://stackoverflow.com/questions/10373660/converting-a-pandas-groupby-output-from-series-to-dataframe)

I'm starting with input data like thisWhich when printed appears as this:Grouping is simple enough:and printing yields a GroupBy object:But what I want eventually is another DataFrame object that contains all the rows in the GroupBy object. In other words I want to get the following result:I can't quite see how to accomplish this in the pandas documentation. Any hints would be welcome.

2012-04-29 16:10:35Z

I'm starting with input data like thisWhich when printed appears as this:Grouping is simple enough:and printing yields a GroupBy object:But what I want eventually is another DataFrame object that contains all the rows in the GroupBy object. In other words I want to get the following result:I can't quite see how to accomplish this in the pandas documentation. Any hints would be welcome.g1 here is a DataFrame. It has a hierarchical index, though:Perhaps you want something like this?Or something like:I want to slightly change the answer given by Wes, because version 0.16.2 requires as_index=False. If you don't set it, you get an empty dataframe.Source:EDIT:In version 0.17.1 and later you can use subset in count and reset_index with parameter name in size:The difference between count and size is that size counts NaN values while count does not.Simply, this should do the task:Here, grouped_df.size() pulls up the unique groupby count, and reset_index() method resets the name of the column you want it to be.

Finally, the pandas Dataframe() function is called upon to create DataFrame object. Maybe I misunderstand the question but if you want to convert the groupby back to a dataframe you can use .to_frame(). I wanted to reset the index when I did this so I included that part as well. example code unrelated to questionThe key is to use the reset_index() method.Use:Now you have your new dataframe in g1:I found this worked for me.I have aggregated with Qty wise data and store to dataframeBelow solution may be simpler:These solutions only partially worked for me because I was doing multiple aggregations. Here is a sample output of my grouped by that I wanted to convert to a dataframe: Because I wanted more than the count provided by reset_index(), I wrote a manual method for converting the image above into a dataframe. I understand this is not the most pythonic/pandas way of doing this as it is quite verbose and explicit, but it was all I needed. Basically, use the reset_index() method explained above to start a "scaffolding" dataframe, then loop through the group pairings in the grouped dataframe, retrieve the indices, perform your calculations against the ungrouped dataframe, and set the value in your new aggregated dataframe.If a dictionary isn't your thing, the calculations could be applied inline in the for loop:

How do I get indices of N maximum values in a NumPy array?

Alexis Métaireau

[How do I get indices of N maximum values in a NumPy array?](https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array)

NumPy proposes a way to get the index of the maximum value of an array via np.argmax.I would like a similar thing, but returning the indexes of the N maximum values.For instance, if I have an array, [1, 3, 2, 4, 5], function(array, n=3) would return the indices [4, 3, 1] which correspond to the elements [5, 4, 3].

2011-08-02 10:29:25Z

NumPy proposes a way to get the index of the maximum value of an array via np.argmax.I would like a similar thing, but returning the indexes of the N maximum values.For instance, if I have an array, [1, 3, 2, 4, 5], function(array, n=3) would return the indices [4, 3, 1] which correspond to the elements [5, 4, 3].The simplest I've been able to come up with is:This involves a complete sort of the array. I wonder if numpy provides a built-in way to do a partial sort; so far I haven't been able to find one.If this solution turns out to be too slow (especially for small n), it may be worth looking at coding something up in Cython.Newer NumPy versions (1.8 and up) have a function called argpartition for this. To get the indices of the four largest elements, doUnlike argsort, this function runs in linear time in the worst case, but the returned indices are not sorted, as can be seen from the result of evaluating a[ind]. If you need that too, sort them afterwards:To get the top-k elements in sorted order in this way takes O(n + k log k) time.Simpler yet:where n is the number of maximum values.Use:For regular Python lists:If you use Python 2, use xrange instead of range.Source: heapq — Heap queue algorithmIf you happen to be working with a multidimensional array then you'll need to flatten and unravel the indices:For example:If you don't care about the order of the K-th largest elements you can use argpartition, which should perform better than a full sort through argsort.Credits go to this question.I ran a few tests and it looks like argpartition outperforms argsort as the size of the array and the value of K increase.For multidimensional arrays you can use the axis keyword in order to apply the partitioning along the expected axis.And for grabbing the items:But note that this won't return a sorted result. In that case you can use np.argsort() along the intended axis:Here is an example:This will be faster than a full sort depending on the size of your original array and the size of your selection:It, of course, involves tampering with your original array.  Which you could fix (if needed) by making a copy or replacing back the original values.  ...whichever is cheaper for your use case.Method np.argpartition only returns the k largest indices, performs a local sort, and is faster than np.argsort(performing a full sort) when array is quite large. But the returned indices are NOT in ascending/descending order. Let's say with an example:We can see that if you want a strict ascending order top k indices, np.argpartition won't return what you want.Apart from doing a sort manually after np.argpartition, my solution is to use PyTorch, torch.topk, a tool for neural network construction, providing NumPy-like APIs with both CPU and GPU support. It's as fast as NumPy with MKL, and offers a GPU boost if you need large matrix/vector calculations.Strict ascend/descend top k indices code will be:Note that torch.topk accepts a torch tensor, and returns both top k values and top k indices in type torch.Tensor. Similar with np, torch.topk also accepts an axis argument so that you can handle multi-dimensional arrays/tensors.Use:Now the result list would contain N tuples (index, value) where value is maximized.Use:It also works with 2D arrays. For example,bottleneck has a partial sort function, if the expense of sorting the entire array just to get the N largest values is too great.I know nothing about this module; I just googled numpy partial sort.The following is a very easy way to see the maximum elements and its positions. Here axis is the domain; axis = 0 means column wise maximum number and axis = 1 means row wise max number for the 2D case. And for higher dimensions it depends upon you.I found it most intuitive to use np.unique. The idea is, that the unique method returns the indices of the input values. Then from the max unique value and the indicies, the position of the original values can be recreated.I think the most time efficiency way is manually iterate through the array and keep a k-size min-heap, as other people have mentioned.And I also come up with a brute force approach:Set the largest element to a large negative value after you use argmax to get its index. And then the next call of argmax will return the second largest element.

And you can log the original value of these elements and recover them if you want.This code works for a numpy matrix array:This produces a true-false n_largest matrix indexing that also works to extract n_largest elements from a matrix array

Printing Python version in output

Thomas Owens

[Printing Python version in output](https://stackoverflow.com/questions/1252163/printing-python-version-in-output)

How can I print the version number of the current Python installation from my script?

2009-08-09 20:15:39Z

How can I print the version number of the current Python installation from my script?TryThis prints the full version information string. If you only want the python version number, then Bastien Léonard's solution is the best. You might want to examine the full string and see if you need it or portions of it.This prints something likeTry    orThis will return a current python version in terminal.expanded version  specificor or

How do I remove/delete a virtualenv?

sudostack

[How do I remove/delete a virtualenv?](https://stackoverflow.com/questions/11005457/how-do-i-remove-delete-a-virtualenv)

I created an environment with the following command: virtualenv venv --distributeI cannot remove it with the following command: rmvirtualenv venv -

This is part of virtualenvwrapper as mentioned in answer below for virtualenvwrapperI do an lson my current directory and I still see venvThe only way I can remove it seems to be: sudo rm -rf venvNote that the environment is not active. I'm running Ubuntu 11.10. Any ideas? I've tried rebooting my system to no avail.

2012-06-12 21:54:54Z

I created an environment with the following command: virtualenv venv --distributeI cannot remove it with the following command: rmvirtualenv venv -

This is part of virtualenvwrapper as mentioned in answer below for virtualenvwrapperI do an lson my current directory and I still see venvThe only way I can remove it seems to be: sudo rm -rf venvNote that the environment is not active. I'm running Ubuntu 11.10. Any ideas? I've tried rebooting my system to no avail.That's it! There is no command for deleting your virtual environment. Simply deactivate it and rid your application of its artifacts by recursively removing it.Note that this is the same regardless of what kind of virtual environment you are using. virtualenv, venv, Anaconda environment, pyenv, pipenv are all based the same principle here.Just to echo what @skytreader had previously commented, rmvirtualenv is a command provided by virtualenvwrapper, not virtualenv. Maybe you didn't have virtualenvwrapper installed?See VirtualEnvWrapper Command Reference for more details.Use rmvirtualenvRemove an environment, in the $WORKON_HOME.Syntax:You must use deactivate before removing the current environment.Reference: http://virtualenvwrapper.readthedocs.io/en/latest/command_ref.htmlYou can remove all the dependencies by recursively uninstalling all of them and then delete the venv.Edit including Isaac Turner commentarySimply remove the virtual environment from the system.There's no special command for itfrom virtualenv's official document https://virtualenv.pypa.io/en/stable/userguide/If you are using pyenv, it is possible to delete your virtual environment:I used pyenv uninstall my_virt_env_name to delete the virual environment.Note: I'm using pyenv-virtualenv installed through the install script.The following command works for me.If you are a Windows user and you are using conda to manage the environment in Anaconda prompt, you can do the following:Make sure you deactivate the virtual environment or restart Anaconda Prompt. Use the following command to remove virtual environment:Alternatively,  you can go to the (that's the default file path) and delete the folder manually.if you are windows user, then it's in C:\Users\your_user_name\Envs. You can delete it from there. Also try in command prompt rmvirtualenv environment name. I tried with command prompt so it said deleted but it was still existed. So i manually delete it. deactivate is the command you are looking for. Like what has already been said, there is no command for deleting your virtual environment. Simply deactivate it!If you're a windows user, you can also delete the environment by going to: C:/Users/username/Anaconda3/envs  Here you can see a list of virtual environment and delete the one that you no longer need.step 1: delete virtualenv virtualenvwrapper by copy and paste the following command below:step 2: go to .bashrc and delete all virtualenv and virtualenvwrapperopen terminal:scroll down and you will see the code bellow then delete it.next, source the .bashrc:FINAL steps: without terminal/shell go to /home and find .virtualenv (I forgot the name so if your find similar to .virtualenv or .venv just delete it. That will work.

How do I correctly clean up a Python object?

wilhelmtell

[How do I correctly clean up a Python object?](https://stackoverflow.com/questions/865115/how-do-i-correctly-clean-up-a-python-object)

__del__(self) above fails with an AttributeError exception.  I understand Python doesn't guarantee the existence of "global variables" (member data in this context?) when __del__() is invoked.  If that is the case and this is the reason for the exception, how do I make sure the object destructs properly?

2009-05-14 19:04:12Z

__del__(self) above fails with an AttributeError exception.  I understand Python doesn't guarantee the existence of "global variables" (member data in this context?) when __del__() is invoked.  If that is the case and this is the reason for the exception, how do I make sure the object destructs properly?I'd recommend using Python's with statement for managing resources that need to be cleaned up.  The problem with using an explicit close() statement is that you have to worry about people forgetting to call it at all or forgetting to place it in a finally block to prevent a resource leak when an exception occurs.To use the with statement, create a class with the following methods:In your example above, you'd use Then, when someone wanted to use your class, they'd do the following:The variable package_obj will be an instance of type Package (it's the value returned by the __enter__ method).  Its __exit__ method will automatically be called, regardless of whether or not an exception occurs.You could even take this approach a step further.  In the example above, someone could still instantiate Package using its constructor without using the with clause.  You don't want that to happen.  You can fix this by creating a PackageResource class that defines the __enter__ and __exit__ methods.  Then, the Package class would be defined strictly inside the __enter__ method and returned.  That way, the caller never could instantiate the Package class without using a with statement:You'd use this as follows:The standard way is to use atexit.register:But you should keep in mind that this will persist all created instances of Package until Python is terminated.Demo using the code above saved as package.py:As an appendix to Clint's answer, you can simplify PackageResource using contextlib.contextmanager:Alternatively, though probably not as Pythonic, you can override Package.__new__:and simply use with Package(...) as package.To get things shorter, name your cleanup function close and use contextlib.closing, in which case you can either use the unmodified Package class via with contextlib.closing(Package(...)) or override its __new__ to the simplerAnd this constructor is inherited, so you can simply inherit, e.g.I don't think that it's possible for instance members to be removed before __del__ is called. My guess would be that the reason for your particular AttributeError is somewhere else (maybe you mistakenly remove self.file elsewhere).However, as the others pointed out, you should avoid using __del__. The main reason for this is that instances with __del__ will not be garbage collected (they will only be freed when their refcount reaches 0). Therefore, if your instances are involved in circular references, they will live in memory for as long as the application run. (I may be mistaken about all this though, I'd have to read the gc docs again, but I'm rather sure it works like this).I think the problem could be in __init__ if there is more code than shown?__del__ will be called even when __init__ has not been executed properly or threw an exception.SourceA better alternative is to use weakref.finalize. See the examples at Finalizer Objects and Comparing finalizers with __del__() methods.Here is a minimal working skeleton:Important: return selfIf you're like me, and overlook the return self part (of Clint Miller's correct answer), you will be staring at this nonsense:I spent half a day on this. Hope it helps the next person.Just wrap your destructor with a try/except statement and it will not throw an exception if your globals are already disposed of.EditTry this:It will stuff the file list in the del function that is guaranteed to exist at the time of call. The weakref proxy is to prevent Python, or yourself from deleting the self.files variable somehow (if it is deleted, then it will not affect the original file list). If it is not the case that this is being deleted even though there are more references to the variable, then you can remove the proxy encapsulation.It seems that the idiomatic way to do this is to provide a close() method (or similar), and call it explicitely.

What are the advantages of NumPy over regular Python lists?

Thomas Browne

[What are the advantages of NumPy over regular Python lists?](https://stackoverflow.com/questions/993984/what-are-the-advantages-of-numpy-over-regular-python-lists)

What are the advantages of NumPy over regular Python lists?I have approximately 100 financial markets series, and I am going to create a cube array of 100x100x100 = 1 million cells. I will be regressing (3-variable) each x with each y and z, to fill the array with standard errors.I have heard that for "large matrices" I should use NumPy as opposed to Python lists, for performance and scalability reasons. Thing is, I know Python lists and they seem to work for me. What will the benefits be if I move to NumPy?What if I had 1000 series (that is, 1 billion floating point cells in the cube)? 

2009-06-14 23:02:14Z

What are the advantages of NumPy over regular Python lists?I have approximately 100 financial markets series, and I am going to create a cube array of 100x100x100 = 1 million cells. I will be regressing (3-variable) each x with each y and z, to fill the array with standard errors.I have heard that for "large matrices" I should use NumPy as opposed to Python lists, for performance and scalability reasons. Thing is, I know Python lists and they seem to work for me. What will the benefits be if I move to NumPy?What if I had 1000 series (that is, 1 billion floating point cells in the cube)? NumPy's arrays are more compact than Python lists -- a list of lists as you describe, in Python, would take at least 20 MB or so, while a NumPy 3D array with single-precision floats in the cells would fit in 4 MB. Access in reading and writing items is also faster with NumPy.Maybe you don't care that much for just a million cells, but you definitely would for a billion cells -- neither approach would fit in a 32-bit architecture, but with 64-bit builds NumPy would get away with 4 GB or so, Python alone would need at least about 12 GB (lots of pointers which double in size) -- a much costlier piece of hardware!The difference is mostly due to "indirectness" -- a Python list is an array of pointers to Python objects, at least 4 bytes per pointer plus 16 bytes for even the smallest Python object (4 for type pointer, 4 for reference count, 4 for value -- and the memory allocators rounds up to 16). A NumPy array is an array of uniform values -- single-precision numbers takes 4 bytes each, double-precision ones, 8 bytes. Less flexible, but you pay substantially for the flexibility of standard Python lists!NumPy is not just more efficient; it is also more convenient. You get a lot of vector and matrix operations for free, which sometimes allow one to avoid unnecessary work. And they are also efficiently implemented.For example, you could read your cube directly from a file into an array:Sum along the second dimension:Find which cells are above a threshold:Remove every even-indexed slice along the third dimension:Also, many useful libraries work with NumPy arrays.  For example, statistical analysis and visualization libraries.Even if you don't have performance problems, learning NumPy is worth the effort.Alex mentioned memory efficiency, and Roberto mentions convenience, and these are both good points. For a few more ideas, I'll mention speed and functionality.Functionality: You get a lot built in with NumPy, FFTs, convolutions, fast searching, basic statistics, linear algebra, histograms, etc. And really, who can live without FFTs?Speed: Here's a test on doing a sum over a list and a NumPy array, showing that the sum on the NumPy array is 10x faster (in this test -- mileage may vary).which on my systems (while I'm running a backup) gives:Here's a nice answer from the FAQ on the scipy.org website:What advantages do NumPy arrays offer over (nested) Python lists?All have highlighted almost all major differences between numpy array and python list, I will just brief them out here:

Does Python have an ordered set?

Casebash

[Does Python have an ordered set?](https://stackoverflow.com/questions/1653970/does-python-have-an-ordered-set)

Python has an ordered dictionary. What about an ordered set?

2009-10-31 10:12:07Z

Python has an ordered dictionary. What about an ordered set?There is an ordered set (possible new link) recipe for this which is referred to from the Python 2 Documentation. This runs on Py2.6 or later and 3.0 or later without any modifications. The interface is almost exactly the same as a normal set, except that initialisation should be done with a list.This is a MutableSet, so the signature for .union doesn't match that of set, but since it includes __or__ something similar can easily be added:The keys of a dictionary are unique. Thus, if one disregards the values in an ordered dictionary (e.g. by assigning them None), then one has essentially an ordered set.As of Python 3.1 there is collections.OrderedDict. The following is an example implementation of an OrderedSet. (Note that only few methods need to be defined or overridden: collections.OrderedDict and collections.MutableSet do the heavy lifting.)The answer is no, but you can use collections.OrderedDict from the Python standard library with just keys (and values as None) for the same purpose.Update: As of Python 3.7 (and CPython 3.6), standard dict is guaranteed to preserve order and is more performant than OrderedDict. (For backward compatibility and especially readability, however, you may wish to continue using OrderedDict.)Here's an example of how to use dict as an ordered set to filter out duplicate items while preserving order, thereby emulating an ordered set. Use the dict class method fromkeys() to create a dict, then simply ask for the keys() back.I can do you one better than an OrderedSet: boltons has a pure-Python, 2/3-compatible IndexedSet type that is not only an ordered set, but also supports indexing (as with lists).Simply pip install boltons (or copy setutils.py into your codebase), import the IndexedSet and:Everything is unique and retained in order. Full disclosure: I wrote the IndexedSet, but that also means you can bug me if there are any issues. :)While others have pointed out that there is no built-in implementation of an insertion-order preserving set in Python (yet), I am feeling that this question is missing an answer which states what there is to be found on PyPI.To the best of my knowledge there currently is:Both implementations are based on the recipe posted by Raymond Hettinger to ActiveState which is also mentioned in other answers here. I have checked out both and identified the following Both implementations have O(1) for add(item) and __contains__(item) (item in my_set).Unfortunately neither implementation has method-based set operations like set1.union(set2) -> You have to use the operator-based form like set1 | set2 instead. See the Python documentation on Set Objects for a full list of set operation methods and their operator-based equivalents.I first went with ordered-set until I used remove(item) for the first time which crashed my script with a NotImplementedError. As I have never used lookup by index so far, I meanwhile switched to oset. If you know about other implementations on PyPI, let me know in the comments.If you're using the ordered set to maintain a sorted order, consider using a sorted set implementation from PyPI. The sortedcontainers module provides a SortedSet for just this purpose. Some benefits: pure-Python, fast-as-C implementations, 100% unit test coverage, hours of stress testing.Installing from PyPI is easy with pip:Note that if you can't pip install, simply pull down the sortedlist.py and sortedset.py files from the open-source repository.Once installed you can simply:The sortedcontainers module also maintains a performance comparison with several alternative implementations.For the comment that asked about Python's bag data type, there's alternatively a SortedList data type which can be used to efficiently implement a bag.In case you're already using pandas in your code, its Index object behaves pretty like an ordered set, as shown in this article.There's no OrderedSet in official library.

I make an exhaustive cheatsheet of all the data structure for your reference.A little late to the game, but I've written a class setlist as part of collections-extended that fully implements both Sequence and SetGitHub: https://github.com/mlenzen/collections-extendedDocumentation: http://collections-extended.lenzm.net/en/latest/PyPI: https://pypi.python.org/pypi/collections-extendedThe ParallelRegression package provides a setList( ) ordered set class that is more method-complete than the options based on the ActiveState recipe.  It supports all methods available for lists and most if not all methods available for sets.For many purposes simply calling sorted will suffice.  For exampleIf you are going to use this repeatedly, there will be overhead incurred by calling the sorted function so you might want to save the resulting list, as long as you're done changing the set.  If you need to maintain unique elements and sorted, I agree with the suggestion of using OrderedDict from collections with an arbitrary value such as None.So i also had a small list where i clearly had the possibility of introducing non-unique values.I searched for the existence of a unique list of some sort, but then realized that testing the existence of the element before adding it works just fine.I don't know if there are caveats to this simple approach, but it solves my problem.

Empty set literal?

Johan Råde

[Empty set literal?](https://stackoverflow.com/questions/6130374/empty-set-literal)

[] = empty list() = empty tuple{} = empty dictIs there a similar notation for an empty set?

Or do I have to write set()?

2011-05-25 20:18:55Z

[] = empty list() = empty tuple{} = empty dictIs there a similar notation for an empty set?

Or do I have to write set()?No, there's no literal syntax for the empty set. You have to write set().By all means, please use set() to create an empty set. But, if you want to impress people, tell them that you can create an empty set using literals and * with Python >= 3.5 (see PEP 448) by doing:this is basically a more condensed way of doing {_ for _ in ()}, but, don't do this.Just to extend the accepted answer:From version 2.7 and 3.1 python has got set literal {} in form of usage {1,2,3}, but {} itself still used for empty dict.Python 2.7 (first line is invalid in Python <2.7)Python 3.xMore here: https://docs.python.org/3/whatsnew/2.7.html#other-language-changesIt depends on if you want the literal for a comparison, or for assignment.If you want to make an existing set empty, you can use the .clear() metod, especially if you want to avoid creating a new object.  If you want to do a comparison, use set() or check if the length is 0.example:Adding to the crazy ideas: with Python 3 accepting unicode identifiers, you could declare a variable ϕ = frozenset() (ϕ is U+03D5) and use it instead.Yes. The same notation that works for non-empty dict/set works for empty ones.Notice the difference between non-empty dict and set literals:{1: 'a', 2: 'b', 3: 'c'} -- a number of key-value pairs inside makes a dict

{'aaa', 'bbb', 'ccc'} -- a tuple of values inside makes a set  So:{} == zero number of key-value pairs == empty dict

{*()} == empty tuple of values == empty setHowever the fact, that you can do it, doesn't mean you should. Unless you have some strong reasons, it's better to construct an empty set explicitly, like:NB: As ctrueden noticed in comments, {()} is not an empty set. It's a set with 1 element: empty tuple.

Difference between map, applymap and apply methods in Pandas

marillion

[Difference between map, applymap and apply methods in Pandas](https://stackoverflow.com/questions/19798153/difference-between-map-applymap-and-apply-methods-in-pandas)

Can you tell me when to use these vectorization methods with basic examples? I see that map is a Series method whereas the rest are DataFrame methods. I got confused about apply and applymap methods though. Why do we have two methods for applying a function to a DataFrame? Again, simple examples which illustrate the usage would be great!

2013-11-05 20:20:14Z

Can you tell me when to use these vectorization methods with basic examples? I see that map is a Series method whereas the rest are DataFrame methods. I got confused about apply and applymap methods though. Why do we have two methods for applying a function to a DataFrame? Again, simple examples which illustrate the usage would be great!Straight from Wes McKinney's Python for Data Analysis book, pg. 132 (I highly recommended this book):Summing up, apply works on a row / column basis of a DataFrame, applymap works element-wise on a DataFrame, and map works element-wise on a Series.There's great information in these answers, but I'm adding my own to clearly summarize which methods work array-wise versus element-wise. jeremiahbuddha mostly did this but did not mention Series.apply.  I don't have the rep to comment.There is a lot of overlap between the capabilities of Series.apply and Series.map, meaning that either one will work in most cases.  They do have some slight differences though, some of which were discussed in osa's answer.First major difference: DEFINITION Second major difference: INPUT ARGUMENT Third major difference: BEHAVIORFourth major difference (the most important one): USE CASEAdding to the other answers, in a Series there are also map and apply. Apply can make a DataFrame out of a series; however, map will just put a series in every cell of another series, which is probably not what you want.Also if I had a function with side effects, such as "connect to a web server", I'd probably use apply just for the sake of clarity.Map can use not only a function, but also a dictionary or another series. Let's say you want to manipulate permutations.TakeThe square of this permutation isYou can compute it using map. Not sure if self-application is documented, but it works in 0.15.1. @jeremiahbuddha mentioned that apply works on row/columns, while applymap works element-wise. But it seems you can still use apply for element-wise computation.... Probably simplest explanation the difference between apply and applymap:apply takes the whole column as a parameter and then assign the result to this columnapplymap takes the separate cell value as a parameter and assign the result back to this cell.NB If apply returns the single value you will have this value instead of the column after assigning and eventually will have just a row instead of matrix.Just wanted to point out, as I struggled with this for a bitMy understanding:From the function point of view:If the function has variables that need to compare within a column/ row, use 

apply. e.g.: lambda x: x.max()-x.mean().If the function is to be applied to each element:1> If a column/row is located, use apply2> If apply to entire dataframe, use applymapFOMO:The following example shows apply and applymap applied to a DataFrame.map function is something you do apply on Series only. You cannot apply map  on DataFrame.The thing to remember is that  apply can do anything applymap can, but apply has eXtra options.The X factor options are: axis and result_type where result_type only works when axis=1 (for columns).As a sidenote, Series map function, should not be confused with the Python map function.The first one is applied on Series, to map the values, and the second one to every item of an iterable.Lastly don't confuse the dataframe apply method with groupby apply method.

Proper indentation for Python multiline strings

ensnare

[Proper indentation for Python multiline strings](https://stackoverflow.com/questions/2504411/proper-indentation-for-python-multiline-strings)

What is the proper indentation for Python multiline strings within a function?oror something else?It looks kind of weird to have the string hanging outside the function in the first example.

2010-03-23 23:35:28Z

What is the proper indentation for Python multiline strings within a function?oror something else?It looks kind of weird to have the string hanging outside the function in the first example.You probably want to line up with the """Since the newlines and spaces are included in the string itself, you will have to postprocess it. If you don't want to do that and you have a whole lot of text, you might want to store it separately in a text file. If a text file does not work well for your application and you don't want to postprocess, I'd probably go withIf you want to postprocess a multiline string to trim out the parts you don't need, you should consider the textwrap module or the technique for postprocessing docstrings presented in PEP 257:The textwrap.dedent function allows one to start with correct indentation in the source, and then strip it from the text before use.The trade-off, as noted by some others, is that this is an extra function call on the literal; take this into account when deciding where to place these literals in your code.The trailing \ in the log message literal is to ensure that line break isn't in the literal; that way, the literal doesn't start with a blank line, and instead starts with the next full line.The return value from textwrap.dedent is the input string with all common leading whitespace indentation removed on each line of the string. So the above log_message value will be:Use inspect.cleandoc like so:Relative indentation will be maintained as expected. As commented below, if you want to keep preceding empty lines, use textwrap.dedent. However that also keeps the first line break.One option which seems to missing from the other answers (only mentioned deep down in a comment by naxa) is the following:This will allow proper aligning, join the lines implicitly, and still keep the line shift which, for me, is one of the reasons why I would like to use multiline strings anyway.It doesn't require any postprocessing, but you need to manually add the \n at any given place that you want the line to end. Either inline or as a separate string after. The latter is easier to copy-paste in.Some more options.  In Ipython with pylab enabled, dedent is already in the namespace.  I checked and it is from matplotlib.  Or it can be imported with:In documentation it states that it is faster than the textwrap equivalent one and in my tests in ipython it is indeed 3 times faster on average with my quick tests.   It also has the benefit that it discards any leading blank lines this allows you to be flexible in how you construct the string:Using the matplotlib dedent on these three examples will give the same sensible result.  The textwrap dedent function will have a leading blank line with 1st example.Obvious disadvantage is that textwrap is in standard library while matplotlib is external module. Some tradeoffs here... the dedent functions make your code more readable where the strings get defined, but require processing later to get the string in usable format.  In docstrings it is obvious that you should use correct indentation as most uses of the docstring will do the required processing.When I need a non long string in my code I find the following admittedly ugly code where I let the long string drop out of the enclosing indentation.  Definitely fails on "Beautiful is better than ugly.", but one could argue that it is simpler and more explicit than the dedent alternative.If you want a quick&easy solution and save yourself from typing newlines, you could opt for a list instead, e.g.:I preferorMy two cents, escape the end of line to get the indents:I came here looking for a simple 1-liner to remove/correct the identation level of the docstring for printing, without making it look untidy,  for example by making it "hang outside the function" within the script.Here's what I ended up doing: Obviously, if you're indenting with spaces (e.g. 4) rather than the tab key use something like this instead: And you don't need to remove the first character if you like your docstrings to look like this instead:The first option is the good one - with indentation included. 

It is in python style - provides readability for the code.To display it properly:It depends on how you want the text to display. If you want it all to be left-aligned then either format it as in the first snippet or iterate through the lines left-trimming all the space.For strings you can just after process the string. For docstrings you need to after process the function instead. Here is a solution for both that is still readable.I'm having a similar issue, code got really unreadable using multilines, I came out with something likeyes, at beginning could look terrible but the embedded syntax was quite complex and adding something at the end (like '\n"') was not a solution

Import a file from a subdirectory?

Adam Matan

[Import a file from a subdirectory?](https://stackoverflow.com/questions/1260792/import-a-file-from-a-subdirectory)

I have a file called tester.py, located on /project./project has a subdirectory called lib, with a file called BoxTime.py:I want to import BoxTime from tester. I have tried this:Which resulted:Any ideas how to import BoxTime from the subdirectory?EDITThe __init__.py was the problem, but don't forget to refer to BoxTime as lib.BoxTime, or use:

2009-08-11 14:39:05Z

I have a file called tester.py, located on /project./project has a subdirectory called lib, with a file called BoxTime.py:I want to import BoxTime from tester. I have tried this:Which resulted:Any ideas how to import BoxTime from the subdirectory?EDITThe __init__.py was the problem, but don't forget to refer to BoxTime as lib.BoxTime, or use:Take a look at the Packages documentation (Section 6.4) here: http://docs.python.org/tutorial/modules.htmlIn short, you need to put a blank file named in the "lib" directory.Much later -- in linux, it would look like this:You can try inserting it in sys.path:I am writing this down because everyone seems to suggest that you have to create a lib directory. You don't need to name your sub-directory lib. You can name it anything provided you put an __init__.py into it. You can do that by entering the following command in a linux shell:So now you have this structure:Then you can import mylib into main.py like this:You can also import functions and classes like this:Any variable function or class you place inside __init__.py can also be accessed:Or like this:Does your lib directory contain a __init__.py file?Python uses __init__.py to determine if a directory is a module.Try import .lib.BoxTime. For more information read about relative import in PEP 328.I do this which basically covers all cases (make sure you have __init__.py in relative/path/to/your/lib/folder):

Example: 

You have in your project folder: You have in another project folder: You want to use /root/anotherproject/utils.py and call foo function which is in it.So you write in app.py:Create an empty file  __init__.py in subdirectory /lib.

And add at the begin of main codethenor better/project/tester.py/project/lib/BoxTime.pycreate blank file __init__.py down the line till you reach the file/project/lib/somefolder/BoxTime.py#lib -- needs has two items one __init__.py and a directory named somefolder

#somefolder has two items boxtime.py and __init__.pytry this:from lib import BoxTime

How to delete a character from a string using Python

Lazer

[How to delete a character from a string using Python](https://stackoverflow.com/questions/3559559/how-to-delete-a-character-from-a-string-using-python)

There is a string, for example. EXAMPLE.How can I remove the middle character, i.e., M from it? I don't need the code. I want to know:

2010-08-24 18:06:31Z

There is a string, for example. EXAMPLE.How can I remove the middle character, i.e., M from it? I don't need the code. I want to know:In Python, strings are immutable, so you have to create a new string.  You have a few options of how to create the new string.  If you want to remove the 'M' wherever it appears:If you want to remove the central character:You asked if strings end with a special character.  No, you are thinking like a C programmer.  In Python, strings are stored with their length, so any byte value, including \0, can appear in a string.This is probably the best way:Don't worry about shifting characters and such. Most Python code takes place on a much higher level of abstraction.To replace a specific position:To replace a specific character:Strings are immutable. But you can convert them to a list, which is mutable, and then convert the list back to a string after you've changed it.You can also create a new string, as others have shown, by taking everything except the character you want from the existing string.You can't, because strings in Python are immutable.No. They are similar to lists of characters; the length of the list defines the length of the string, and no character acts as a terminator.You cannot modify the existing string, so you must create a new one containing everything except the middle character.Use the translate() method:UserString.MutableStringMutable way:How to remove one character from a string:

Here is an example where there is a stack of cards represented as characters in a string.

One of them is drawn (import random module for the random.choice() function, that picks a random character in the string).

A new string, cardsLeft, is created to hold the remaining cards given by the string function replace() where the last parameter indicates that only one "card" is to be replaced by the empty string...I have seen this somewhere here.Here's what I did to slice out the "M":If you want to delete/ignore characters in a string, and, for instance, you have this string,"[11:L:0]"from a web API response or something like that, like a CSV file, let's say you are using requestsloop and get rid of unwanted chars:Optional split, and you will be able to read values individually:Now accessing each value is easier:This will printTo delete a char or a sub-string once (only the first occurrence):       NOTE: Here 1 can be replaced with any int for the number of occurrence you want to replace. You can simply use list comprehension.Assume that you have the string: my name is and you want to remove character m. use the following code:Strings are immutable in Python so both your options mean the same thing basically.

Convert pandas dataframe to NumPy array

mister.nobody.nz

[Convert pandas dataframe to NumPy array](https://stackoverflow.com/questions/13187778/convert-pandas-dataframe-to-numpy-array)

I am interested in knowing how to convert a pandas dataframe into a NumPy array.dataframe:givesI would like to convert this to a NumPy array, as so:How can I do this? As a bonus, is it possible to preserve the dtypes, like this?or similar?

2012-11-02 00:57:33Z

I am interested in knowing how to convert a pandas dataframe into a NumPy array.dataframe:givesI would like to convert this to a NumPy array, as so:How can I do this? As a bonus, is it possible to preserve the dtypes, like this?or similar?To convert a pandas dataframe (df) to a numpy ndarray, use this code:pandas v0.24.0 introduced two new methods for obtaining NumPy arrays from pandas objects: If you visit the v0.24 docs for .values, you will see a big red warning that says:See this section of the v0.24.0 release notes, and this answer for more information. In the spirit of better consistency throughout the API, a new method to_numpy has been introduced to extract the underlying NumPy array from DataFrames.As mentioned above, this method is also defined on Index and Series objects (see here).By default, a view is returned, so any modifications made will affect the original.If you need a copy instead, use to_numpy(copy=True). If you're using pandas 1.x, chances are you'll be dealing with extension types a lot more. You'll have to be a little more careful that these extension types are correctly converted.This is called out in the docs.As shown in another answer, DataFrame.to_records is a good way to do this.This cannot be done with to_numpy, unfortunately. However, as an alternative, you can use np.rec.fromrecords:Performance wise, it's nearly the same (actually, using rec.fromrecords is a bit faster).to_numpy() (in addition to array) was added as a result of discussions under two GitHub issues GH19954 and GH23623. Specifically, the docs mention the rationale:to_numpy aim to improve the consistency of the API, which is a major step in the right direction. .values will not be deprecated in the current version, but I expect this may happen at some point in the future, so I would urge users to migrate towards the newer API, as soon as you can.DataFrame.values has inconsistent behaviour, as already noted.DataFrame.get_values() is simply a wrapper around DataFrame.values, so everything said above applies.DataFrame.as_matrix() is deprecated now, do NOT use!Note: The .as_matrix() method used in this answer is deprecated. Pandas 0.23.4 warns:Pandas has something built in...givesI would just chain the DataFrame.reset_index() and DataFrame.values functions to get the Numpy representation of the dataframe, including the index:To get the dtypes we'd need to transform this ndarray into a structured array using view:You can use the to_records method, but have to play around a bit with the dtypes if they are not what you want from the get go. In my case, having copied your DF from a string, the index type is string (represented by an object dtype in pandas):Converting the recarray dtype does not work for me, but one can do this in Pandas already:Note that Pandas does not set the name of the index properly (to ID) in the exported record array (a bug?), so we profit from the type conversion to also correct for that. At the moment Pandas has only 8-byte integers, i8, and floats, f8 (see this issue).It seems like df.to_records() will work for you. The exact feature you're looking for was requested and to_records pointed to as an alternative.I tried this out locally using your example, and that call yields something very similar to the output you were looking for:Note that this is a recarray rather than an array. You could move the result in to regular numpy array by calling its constructor as np.array(df.to_records()).Try this:Here is my approach to making a structure array from a pandas DataFrame.Create the data frameDefine function to make a numpy structure array (not a record array) from a pandas DataFrame.Use reset_index to make a new data frame that includes the index as part of its data. Convert that data frame to a structure array.EDIT: Updated df_to_sarray to avoid error calling .encode() with python 3. Thanks to Joseph Garvin and halcyon  for their comment and solution.Two ways to convert the data-frame to its Numpy-array representation.Doc: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.as_matrix.htmlA Simpler Way for Example DataFrame: USE:GET:Just had a similar problem when exporting from dataframe to arcgis table and stumbled on a solution from usgs (https://my.usgs.gov/confluence/display/cdi/pandas.DataFrame+to+ArcGIS+Table).

In short your problem has a similar solution:I went through the answers above. The "as_matrix()" method works but its obsolete now. For me, What worked was ".to_numpy()". This returns a multidimensional array. I'll prefer using this method if you're reading data from excel sheet and you need to access data from any index. Hope this helps :)Further to meteore's answer, I found the codedoesn't work for me. So I put my code here for the convenience of others stuck with this issue.A simple way to convert dataframe to numpy array:Use of to_numpy is encouraged to preserve consistency.Reference:

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.htmlTry this:Some more information at: [https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html]

Valid for numpy 1.16.5 and pandas 0.25.2.

How to access the ith column of a NumPy multidimensional array?

lpl

[How to access the ith column of a NumPy multidimensional array?](https://stackoverflow.com/questions/4455076/how-to-access-the-ith-column-of-a-numpy-multidimensional-array)

Suppose I have:test[i] gets me ith line of the array (eg [1, 2]). How can I access the ith column? (eg [1, 3, 5]). Also, would this be an expensive operation?

2010-12-15 21:27:41Z

Suppose I have:test[i] gets me ith line of the array (eg [1, 2]). How can I access the ith column? (eg [1, 3, 5]). Also, would this be an expensive operation?Similarly, lets you access rows.  This is covered in Section 1.4 (Indexing) of the NumPy reference.  This is quick, at least in my experience.  It's certainly much quicker than accessing each element in a loop.And if you want to access more than one column at a time you could do:this command gives you a row vector, if you just want to loop over it, it's fine, but if you want to hstack with some other array with dimension 3xN, you will havewhilegives you a column vector, so that you can do concatenate or hstack operation.e.g.You could also transpose and return a row:To get several and indepent columns, just:you will get colums 0 and 2 Although the question has been answered, let me mention some nuances.Let's say you are interested in the first column of the arrayAs you already know from other answers, to get it in the form of "row vector" (array of shape (3,)), you use slicing:To check if an array is a view or a copy of another array you can do the following:see ndarray.base.Besides the obvious difference between the two (modifying arr_c1_ref will affect arr), the number of byte-steps for traversing each of them is different:see strides.

Why is this important? Imagine that you have a very big array A instead of the arr:and you want to compute the sum of all the elements of the first column, i.e. A_c1_ref.sum() or A_c1_copy.sum(). Using the copied version is much faster:This is due to the different number of strides mentioned before:Although it might seem that using column copies is better, it is not always true for the reason that making a copy takes time and uses more memory (in this case it took me approx. 200 µs to create the A_c1_copy). However if we need the copy in the first place, or we need to do many different operations on a specific column of the array and we are ok with sacrificing memory for speed, then making a copy is the way to go.In the case that we are interested in working mostly with columns, it could be a good idea to create our array in column-major ('F') order instead of the row-major ('C') order (which is the default), and then do the slicing as before to get a column without copying it:Now, performing the sum operation (or any other) on a column-view is much faster.Finally let me note that transposing an array and using row-slicing is the same as using the column-slicing on the original array, because transposing is done by just swapping the shape and the strides of the original array.Then you can select the 2nd - 4th column this way:

How to split a string into array of characters?

Adrian

[How to split a string into array of characters?](https://stackoverflow.com/questions/4978787/how-to-split-a-string-into-array-of-characters)

I've tried to look around the web for answers to splitting a string into an array of characters but I can't seem to find a simple methodstr.split(//) does not seem to work like Ruby does. Is there a simple way of doing this without looping?

2011-02-12 15:14:38Z

I've tried to look around the web for answers to splitting a string into an array of characters but I can't seem to find a simple methodstr.split(//) does not seem to work like Ruby does. Is there a simple way of doing this without looping?You need listYou take the string and pass it to list()You can also do it in this very simple way without list():If you want to process your String one character at a time. you have various options.  Output:Output:Output:  Output:  I explored another two ways to accomplish this task. It may be helpful for someone.The first one is easy:And the second one use map and lambda function. It may be appropriate for more complex tasks:For exampleSee python docs for more methodsThe task boils down to iterating over characters of the string and collecting them into a list. The most naïve solution would look likeOf course, it can be shortened to justbut there still are shorter solutions that do the same thing.list constructor can be used to convert any iterable (iterators, lists, tuples, string etc.) to list. The big plus is that it works the same in both Python 2 and Python 3.Also, starting from Python 3.5 (thanks to the awesome PEP 448) it's now possible to build a list from any iterable by unpacking it to an empty list literal:This is neater, and in some cases more efficient than calling list constructor directly.I'd advise against using map-based approaches, because map does not return a list in Python 3. See How to use filter, map, and reduce in Python 3.I you just need an array of chars:If you want to split the str by a particular str:split() inbuilt function will only separate the value on the basis of certain condition but in the single word, it cannot fulfill the condition. So, it can be solved with the help of list(). It internally calls the Array and it will store the value on the basis of an array.Suppose,If you wish to read only access to the string you can use array notation directly.Could be useful for testing without using regexp.

Does the string contain an ending newline?Well, much as I like the list(s) version, here's another more verbose way I found (but it's cool so I thought I'd add it to the fray):similar to list(string) but returns a generator that is lazily evaluated at point of use, so memory efficient.where a is the string that you want to separate out. The values "a[i]" are the individual character of the the string these could be appended to a list.Unpack them:

Is there a simple, elegant way to define singletons? [duplicate]

Jamie

[Is there a simple, elegant way to define singletons? [duplicate]](https://stackoverflow.com/questions/31875/is-there-a-simple-elegant-way-to-define-singletons)

There seem to be many ways to define singletons in Python. Is there a consensus opinion on Stack Overflow?

2008-08-28 09:03:09Z

There seem to be many ways to define singletons in Python. Is there a consensus opinion on Stack Overflow?I don't really see the need, as a module with functions (and not a class) would serve well as a singleton. All its variables would be bound to the module, which could not be instantiated repeatedly anyway. If you do wish to use a class, there is no way of creating private classes or private constructors in Python, so you can't protect against multiple instantiations, other than just via convention in use of your API. I would still just put methods in a module, and consider the module as the singleton.Here's my own implementation of singletons. All you have to do is decorate the class; to get the singleton, you then have to use the Instance method. Here's an example:And here's the code:You can override the __new__ method like this: A slightly different approach to implement the singleton in Python is the borg pattern by Alex Martelli (Google employee and Python genius).So instead of forcing all instances to have the same identity, they share state.The module approach works well. If I absolutely need a singleton I prefer the Metaclass approach.See this implementation from PEP318, implementing the singleton pattern with a decorator:As the accepted answer says, the most idiomatic way is to just use a module.With that in mind, here's a proof of concept:See the Python data model for more details on __new__.Example:Notes:The Python documentation does cover this:I would probably rewrite it to look more like this:It should be relatively clean to extend this:I'm very unsure about this, but my project uses 'convention singletons' (not enforced singletons), that is, if I have a class called DataController, I define this in the same module:It is not elegant, since it's a full six lines. But all my singletons use this pattern, and it's at least very explicit (which is pythonic).The one time I wrote a singleton in Python I used a class where all the member functions had the classmethod decorator.Creating a singleton decorator (aka an annotation) is an elegant way if you want to decorate (annotate) classes going forward. Then you just put @singleton before your class definition. There are also some interesting articles on the Google Testing blog, discussing why singleton are/may be bad and are an anti-pattern:I think that forcing a class or an instance to be a singleton is overkill. Personally, I like to define a normal instantiable class, a semi-private reference, and a simple factory function.Or if there is no issue with instantiating when the module is first imported:That way you can write tests against fresh instances without side effects, and there is no need for sprinkling the module with global statements, and if needed you can derive variants in the future.The Singleton Pattern implemented with Python courtesy of ActiveState.It looks like the trick is to put the class that's supposed to only have one instance inside of another class.Being relatively new to Python I'm not sure what the most common idiom is, but the simplest thing I can think of is just using a module instead of a class. What would have been instance methods on your class become just functions in the module and any data just becomes variables in the module instead of members of the class. I suspect this is the pythonic approach to solving the type of problem that people use singletons for.If you really want a singleton class, there's a reasonable implementation described on the first hit on Google for "Python singleton", specifically:That seems to do the trick.OK, singleton could be good or evil, I know. This is my implementation, and I simply extend a classic approach to introduce a cache inside and produce many instances of a different type or, many instances of same type, but with different arguments.I called it Singleton_group, because it groups similar instances together and prevent that an object of the same class, with same arguments, could be created:Every object carries the singleton cache... This could be evil, but it works great for some :)My simple solution which is based on the default value of function parameters.Singleton's half brotherI completely agree with staale and I leave here a sample of creating a singleton half brother:a will report now as being of the same class as singleton even if it does not look like it. So singletons using complicated classes end up depending on we don't mess much with them.Being so, we can have the same effect and use simpler things like a variable or a module. Still, if we want use classes for clarity and because in Python a class is an object, so we already have the object (not and instance, but it will do just like).There we have a nice assertion error if we try to create an instance, and we can store on derivations static members and make changes to them at runtime (I love Python). This object is as good as other about half brothers (you still can create them if you wish), however it will tend to run faster due to simplicity.In cases where you don't want the metaclass-based solution above, and you don't like the simple function decorator-based approach (e.g. because in that case static methods on the singleton class won't work), this compromise works:

Making Python loggers output all messages to stdout in addition to log file

Martijn Pieters

[Making Python loggers output all messages to stdout in addition to log file](https://stackoverflow.com/questions/14058453/making-python-loggers-output-all-messages-to-stdout-in-addition-to-log-file)

Is there a way to make Python logging using the logging module automatically output things to stdout in addition to the log file where they are supposed to go? For example, I'd like all calls to logger.warning, logger.critical, logger.error to go to their intended places but in addition always be copied to stdout. This is to avoid duplicating messages like:

2012-12-27 17:11:21Z

Is there a way to make Python logging using the logging module automatically output things to stdout in addition to the log file where they are supposed to go? For example, I'd like all calls to logger.warning, logger.critical, logger.error to go to their intended places but in addition always be copied to stdout. This is to avoid duplicating messages like:All logging output is handled by the handlers; just add a logging.StreamHandler() to the root logger.Here's an example configuring a stream handler (using stdout instead of the default stderr) and adding it to the root logger:The simplest way to log to stdout:It's possible using multiple handlers.Please see: https://docs.python.org/2/howto/logging-cookbook.htmlYou could create two handlers for file and stdout and then create one logger with handlers argument to basicConfig. It could be useful if you have the same log_level and format output for both handlers:The simplest way to log to file and to stderr:Here is a solution based on the powerful but poorly documented logging.config.dictConfig method.

Instead of sending every log message to stdout, it sends messages with log level ERROR and higher to stderr and everything else to stdout.

This can be useful if other parts of the system are listening to stderr or stdout.Since no one has shared a neat two liner, I will share my own:Here's an extremely simple example:The output will show "test msg" on stdout and also in the file.

What does -> mean in Python function definitions?

Krotton

[What does -> mean in Python function definitions?](https://stackoverflow.com/questions/14379753/what-does-mean-in-python-function-definitions)

I've recently noticed something interesting when looking at Python 3.3 grammar specification:The optional 'arrow' block was absent in Python 2 and I couldn't find any information regarding its meaning in Python 3. It turns out this is correct Python and it's accepted by the interpreter:I thought that this might be some kind of a precondition syntax, but:Could anyone accustomed with this syntax explain it?

2013-01-17 13:03:35Z

I've recently noticed something interesting when looking at Python 3.3 grammar specification:The optional 'arrow' block was absent in Python 2 and I couldn't find any information regarding its meaning in Python 3. It turns out this is correct Python and it's accepted by the interpreter:I thought that this might be some kind of a precondition syntax, but:Could anyone accustomed with this syntax explain it?It's a function annotation.In more detail, Python 2.x has docstrings, which allow you to attach a metadata string to various types of object. This is amazingly handy, so Python 3 extends the feature by allowing you to attach metadata to functions describing their parameters and return values.There's no preconceived use case, but the PEP suggests several. One very handy one is to allow you to annotate parameters with their expected types; it would then be easy to write a decorator that verifies the annotations or coerces the arguments to the right type. Another is to allow parameter-specific documentation instead of encoding it into the docstring.These are function annotations covered in PEP 3107. Specifically, the -> marks the return function annotation.Examples:Annotations are dictionaries, so you can do this:You can also have a python data structure rather than just a string:Or, you can use function attributes to validate called values:Prints As other answers have stated, the -> symbol is used as part of function annotations. In more recent versions of Python >= 3.5, though, it has a defined meaning. PEP 3107 -- Function Annotations described the specification, defining the grammar changes, the existence of func.__annotations__ in which they are stored and, the fact that it's use case is still open. In Python 3.5 though, PEP 484 -- Type Hints attaches a single meaning to this: -> is used to indicate the type that the function returns. It also seems like this will be enforced in future versions as described in What about existing uses of annotations:(Emphasis mine)This hasn't been actually implemented as of 3.6 as far as I can tell so it might get bumped to future versions.According to this, the example you've supplied:will be forbidden in the future (and in current versions will be confusing), it would need to be changed to:for it to effectively describe that function f returns an object of type int.The annotations are not used in any way by Python itself, it pretty much populates and ignores them. It's up to 3rd party libraries to work with them.In the following code:the -> int just tells that f() returns an integer (but it doesn't force the function to return an integer). It is called a return annotation, and can be accessed as f.__annotations__['return'].Python also supports parameter annotations:: float tells people who read the program (and some third-party libraries/programs, e. g. pylint) that x should be a float. Itis accessed as f.__annotations__['x'], and doesn't have any meaning by itself. See the documentation for more information:https://docs.python.org/3/reference/compound_stmts.html#function-definitions

https://www.python.org/dev/peps/pep-3107/This means the type of result the function returns, but it can be None.It is widespread in modern libraries oriented on Python 3.x.For example, it there is in code of library pandas-profiling in many places for example:def function(arg)->123:It's simply a return type, integer in this case doesn't matter which number you write.like Java :But for Python (how Jim Fasarakis Hilliard said)   the return type it's just an hint, so it's suggest the return but allow anyway to return other type like a string..

What exactly are iterator, iterable, and iteration?

thechrishaddad

[What exactly are iterator, iterable, and iteration?](https://stackoverflow.com/questions/9884132/what-exactly-are-iterator-iterable-and-iteration)

What is the most basic definition of "iterable", "iterator" and "iteration" in Python?I have read multiple definitions but I am unable to identify the exact meaning as it still won't sink in.Can someone please help me with the 3 definitions in layman terms?

2012-03-27 06:03:37Z

What is the most basic definition of "iterable", "iterator" and "iteration" in Python?I have read multiple definitions but I am unable to identify the exact meaning as it still won't sink in.Can someone please help me with the 3 definitions in layman terms?Iteration is a general term for taking each item of something, one after another. Any time you use a loop, explicit or implicit, to go over a group of items, that is iteration.In Python, iterable and iterator have specific meanings.An iterable is an object that has an __iter__ method which returns an iterator, or which defines a __getitem__ method that can take sequential indexes starting from zero (and raises an IndexError when the indexes are no longer valid). So an iterable is an object that you can get an iterator from.An iterator is an object with a next (Python 2) or __next__ (Python 3) method. Whenever you use a for loop, or map, or a list comprehension, etc. in Python, the next method is called automatically to get each item from the iterator, thus going through the process of iteration.A good place to start learning would be the iterators section of the tutorial and the iterator types section of the standard types page. After you understand the basics, try the iterators section of the Functional Programming HOWTO.Here's the explanation I use in teaching Python classes:An ITERABLE is:An ITERATOR is an object:Notes:For example:The above answers are great, but as most of what I've seen, don't stress the distinction enough for people like me.Also, people tend to get "too Pythonic" by putting definitions like "X is an object that has __foo__() method" before.  Such definitions are correct--they are based on duck-typing philosophy, but the focus on methods tends to get between when trying to understand the concept in its simplicity.So I add my version.In natural language,In Python,Iterators are themselves also iterable, with the distinction that their __iter__() method returns the same object (self), regardless of whether or not its items have been consumed by previous calls to next().So what does Python interpreter think when it sees for x in obj: statement?Since Mr. obj succeeded in this test (by having certain method returning a valid iterator), we reward him with adjective: you can now call him "iterable Mr. obj".However, in simple cases, you don't normally benefit from having iterator and iterable separately.  So you define only one object, which is also its own iterator.  (Python does not really care that _i handed out by obj wasn't all that shiny, but just the obj itself.) This is why in most examples I've seen (and what had been confusing me over and over),

you can see:instead ofThere are cases, though, when you can benefit from having iterator separated from the iterable, such as when you want to have one row of items, but more "cursors".  For example when you want to work with "current" and "forthcoming" elements, you can have separate iterators for both.  Or multiple threads pulling from a huge list: each can have its own iterator to traverse over all items. See @Raymond's and @glglgl's answers above.Imagine what you could do:Notes: Disclaimer: I'm not a developer of any Python interpreter, so I don't really know what the interpreter "thinks".  The musings above are solely demonstration of how I understand the topic from other explanations, experiments and real-life experience of a Python newbie.An iterable is a object which has a __iter__() method. It can possibly iterated over several times, such as list()s and tuple()s.An iterator is the object which iterates. It is returned by an __iter__() method, returns itself via its own __iter__() method and has a next() method (__next__() in 3.x).Iteration is the process of calling this next() resp. __next__() until it raises StopIteration.Example:Here's my cheat sheet:Quiz: Do you see how...I don’t know if it helps anybody but I always like to visualize concepts in my head to better understand them. So as I have a little son I visualize iterable/iterator concept with bricks and white paper.Suppose we are in the dark room and on the floor we have bricks for my son. Bricks of different size, color, does not matter now. Suppose we have 5 bricks like those. Those 5 bricks can be described as an object – let’s say bricks kit. We can do many things with this bricks kit – can take one and then take second and then third, can change places of bricks, put first brick above the second. We can do many sorts of things with those. Therefore this bricks kit is an iterable object or sequence as we can go through each brick and do something with it. We can only do it like my little son – we can play with one brick at a time. So again I imagine myself this bricks kit to be an iterable.Now remember that we are in the dark room. Or almost dark. The thing is that we don’t clearly see those bricks, what color they are, what shape etc. So even if we want to do something with them – aka iterate through them – we don’t really know what and how because it is too dark.What we can do is near to first brick – as element of a bricks kit – we can put a piece of white fluorescent paper in order for us to see where the first brick-element is. And each time we take a brick from a kit, we replace the white piece of paper to a next brick in order to be able to see that in the dark room. This white piece of paper is nothing more than an iterator. It is an object as well. But an object with what we can work and play with elements of our iterable object – bricks kit.That by the way explains my early mistake when I tried the following in an IDLE and got a TypeError:List X here was our bricks kit but NOT a white piece of paper. I needed to find an iterator first:Don’t know if it helps, but it helped me. If someone could confirm/correct visualization of the concept, I would be grateful. It would help me to learn more.Iterable:- something that is iterable is iterable; like sequences like lists ,strings etc.

  Also it has either the __getitem__ method or an __iter__ method. Now if we use iter() function on that object, we'll get an iterator.  Iterator:- When we get the iterator object from the iter() function; we call __next__() method (in python3) or simply next() (in python2) to get elements one by one. This class or instance of this class is called an iterator.From docs:-The use of iterators pervades and unifies Python. Behind the scenes, the for statement calls iter() on the container object. The function returns an iterator object that defines the method __next__() which accesses elements in the container one at a time. When there are no more elements, __next__() raises a StopIteration exception which tells the for loop to terminate. You can call the __next__() method using the next() built-in function; this example shows how it all works:Ex of a class:- I don't think that you can get it much simpler than the documentation, however I'll try:so,so, 

    OUTPUT OF ABOVE CODE WILL BE:12Luciano Ramalho, Fluent Python.Before dealing with the  iterables and iterator the major factor that decide the iterable and iterator is sequenceSequence: Sequence is the collection of dataIterable: Iterable are the sequence type object that support __iter__ method.Iter method: Iter method take sequence as an input and create an object which is known as iteratorIterator: Iterator are the object which call next method and transverse through the sequence. On calling the next method it returns the object that it traversed currently.example:x is a sequence which consists of collection of dataOn calling iter(x) it returns a iterator only when the x object has iter method otherwise it raise an exception.If it returns iterator then y is assign like this:As y is a iterator hence it support next() methodOn calling next method it returns the individual elements of the list one by one.After returning the last element of the sequence if we again call the next method it raise an StopIteration errorexample:In Python everything is an object. When an object is said to be iterable, it means that you can step through (i.e. iterate) the object as a collection.Arrays for example are iterable. You can step through them with a for loop, and go from index 0 to index n, n being the length of the array object minus 1.Dictionaries (pairs of key/value, also called associative arrays) are also iterable. You can step through their keys.Obviously the objects which are not collections are not iterable. A bool object for example only have one value, True or False. It is not iterable (it wouldn't make sense that it's an iterable object).Read more. http://www.lepus.org.uk/ref/companion/Iterator.xml

Creating an empty Pandas DataFrame, then filling it?

Matthias Kauer

[Creating an empty Pandas DataFrame, then filling it?](https://stackoverflow.com/questions/13784192/creating-an-empty-pandas-dataframe-then-filling-it)

I'm starting from the pandas DataFrame docs here: http://pandas.pydata.org/pandas-docs/stable/dsintro.htmlI'd like to iteratively fill the DataFrame with values in a time series kind of calculation.

So basically, I'd like to initialize the DataFrame with columns A, B and timestamp rows, all 0 or all NaN.I'd then add initial values and go over this data calculating the new row from the row before, say row[A][t] = row[A][t-1]+1 or so.I'm currently using the code as below, but I feel it's kind of ugly and there must be a  way to do this with a DataFrame directly, or just a better way in general.

Note: I'm using Python 2.7.

2012-12-09 02:50:38Z

I'm starting from the pandas DataFrame docs here: http://pandas.pydata.org/pandas-docs/stable/dsintro.htmlI'd like to iteratively fill the DataFrame with values in a time series kind of calculation.

So basically, I'd like to initialize the DataFrame with columns A, B and timestamp rows, all 0 or all NaN.I'd then add initial values and go over this data calculating the new row from the row before, say row[A][t] = row[A][t-1]+1 or so.I'm currently using the code as below, but I feel it's kind of ugly and there must be a  way to do this with a DataFrame directly, or just a better way in general.

Note: I'm using Python 2.7.Here's a couple of suggestions:Use date_range for the index:Note: we could create an empty DataFrame (with NaNs) simply by writing:To do these type of calculations for the data, use a numpy array:Hence we can create the DataFrame:If you simply want to create an empty data frame and fill it with some incoming data frames later, try this:In this example I am using this pandas doc to create a new data frame and then using append to write to the newDF with data from oldDF. If I have to keep appending new data into this newDF from more than

   one oldDFs, I just use a for loop to iterate over

   pandas.DataFrame.append()Initialize empty frame with column namesAdd a new record to a frameYou also might want to pass a dictionary:Append another frame to your existing framePerformance considerationsIf you are adding rows inside a loop consider performance issues. For around the first 1000 records "my_df.loc" performance is better, but it gradually becomes slower by increasing the number of records in the loop.If you plan to do thins inside a big loop (say 10M‌ records or so), you are better off using a mixture of these two;

fill a dataframe with iloc until the size gets around 1000, then append it to the original dataframe, and empty the temp dataframe.

This would boost your performance by around 10 times.Most answers here will tell you how to create an empty DataFrame and fill it out, but no one will tell you that it is a bad thing to do. Here is my advice: Wait until you are sure you have all the data you need to work with. Use a list to collect your data, then initialise a DataFrame when you are ready.It is always cheaper to append to a list and create a DataFrame in one go than it is to create an empty DataFrame (or one of of NaNs) and append to it over and over again. Lists also take up less memory and are a much lighter data structure to work with, append, and remove (if needed).The other advantage of this method is dtypes are automatically inferred (rather than assigning object to all of them).The last advantage is that a RangeIndex is automatically created for your data, so it is one less thing to worry about (take a look at the poor append and loc methods below, you will see elements in both that require handling the index appropriately).Here is the biggest mistake I've seen from beginners:Memory is re-allocated for every append or concat operation you have. Couple this with a loop and you have a quadratic complexity operation.   From the df.append doc page:

The other mistake associated with df.append is that users tend to forget append is not an in-place function, so the result must be assigned back. You also have to worry about the dtypes:Dealing with object columns is never a good thing, because pandas cannot vectorize operations on those columns. You will need to do this to fix it:I have also seen loc used to append to a DataFrame that was created empty:As before, you have not pre-allocated the amount of memory you need each time, so the memory is re-grown each time you create a new row. It's just as bad as append, and even more ugly.And then, there's creating a DataFrame of NaNs, and all the caveats associated therewith.It creates a DataFrame of object columns, like the others.Appending still has all the issues as the methods above.Timing these methods is the fastest way to see just how much they differ in terms of their memory and utility.Benchmarking code for reference.Assume a dataframe with 19 rowsKeeping Column A as a constantKeeping column b as a variable given by a loopYou can replace the first x in pd.Series([x], index = [x]) with any value

Proper way to use **kwargs in Python

Kekoa

[Proper way to use **kwargs in Python](https://stackoverflow.com/questions/1098549/proper-way-to-use-kwargs-in-python)

What is the proper way to use **kwargs in Python when it comes to default values?kwargs returns a dictionary, but what is the best way to set default values, or is there one?  Should I just access it as a dictionary?  Use get function?  A simple question, but one that I can't find good resources on.  People do it different ways in code that I've seen and it's hard to know what to use.

2009-07-08 14:45:53Z

What is the proper way to use **kwargs in Python when it comes to default values?kwargs returns a dictionary, but what is the best way to set default values, or is there one?  Should I just access it as a dictionary?  Use get function?  A simple question, but one that I can't find good resources on.  People do it different ways in code that I've seen and it's hard to know what to use.You can pass a default value to get() for keys that are not in the dictionary:However, if you plan on using a particular argument with a particular default value, why not use named arguments in the first place?While most answers are saying that, e.g.,is "the same as"this is not true.  In the latter case, f can be called as f(23, 42), while the former case accepts named arguments only -- no positional calls.  Often you want to allow the caller maximum flexibility and therefore the second form, as most answers assert, is preferable: but that is not always the case.  When you accept many optional parameters of which typically only a few are passed, it may be an excellent idea (avoiding accidents and unreadable code at your call sites!) to force the use of named arguments -- threading.Thread is an example.  The first form is how you implement that in Python 2.The idiom is so important that in Python 3 it now has special supporting syntax: every argument after a single * in the def signature is keyword-only, that is, cannot be passed as a positional argument, but only as a named one. So in Python 3 you could code the above as:Indeed, in Python 3 you can even have keyword-only arguments that aren't optional (ones without a default value).However, Python 2 still has long years of productive life ahead, so it's better to not forget the techniques and idioms that let you implement in Python 2 important design ideas that are directly supported in the language in Python 3!I suggest something like thisAnd then use the values any way you wantdictionaryA.update(dictionaryB) adds the contents of dictionaryB to dictionaryA overwriting any duplicate keys.You'd doorIf you use pop, then you can check if there are any spurious values sent, and take the appropriate action (if any).Using **kwargs and default values is easy.  Sometimes, however, you shouldn't be using **kwargs in the first place.In this case, we're not really making best use of **kwargs.The above is a "why bother?" declaration.  It is the same asWhen you're using **kwargs, you mean that a keyword is not just optional, but conditional.  There are more complex rules than simple default values.When you're using **kwargs, you usually mean something more like the following, where simple defaults don't apply.Since **kwargs is used when the number of arguments is unknown, why not doing this?Here's another approach:You could do something like thisFollowing up on @srhegde suggestion of using setattr:This variant is useful when the class is expected to have all of the items in our acceptable list.I think the proper way to use **kwargs in Python when it comes to default values is to use the dictionary method setdefault, as given below:In this way, if a user passes 'val' or 'val2' in the keyword args, they will be used; otherwise, the default values that have been set will be used.If you want to combine this with *args you have to keep *args and **kwargs at the end of the definition.So:@AbhinavGupta and @Steef suggested using update(), which I found very helpful for processing large argument lists:What if we want to check that the user hasn't passed any spurious/unsupported arguments? @VinaySajip pointed out that pop() can be used to iteratively process the list of arguments. Then, any leftover arguments are spurious. Nice.Here's another possible way to do this, which keeps the simple syntax of using update():unknown_args is a set containing the names of arguments that don't occur in the defaults.Another simple solution for processing unknown or multiple arguments can be:**kwargs gives the freedom to add any number of keyword arguments. One may have a list of keys for which he can set default values. But setting default values for an indefinite number of keys seems unnecessary. Finally, it may be important to have the keys as instance attributes. So, I would do this as follows:

No module named pkg_resources

igniteflow

[No module named pkg_resources](https://stackoverflow.com/questions/7446187/no-module-named-pkg-resources)

I'm deploying a Django app to a dev server and am hitting this error when I run pip install -r requirements.txt:pkg_resources appears to be distributed with setuptools.  Initially I thought this might not be installed to the Python in the virtualenv, so I installed setuptools 2.6 (same version as Python) to the Python site-packages in the virtualenv with the following command:EDIT: This only happens inside the virtualenv.  If I open a console outside the virtualenv then pkg_resources is present, but I am still getting the same error.Any ideas as to why pkg_resources is not on the path?

2011-09-16 14:26:54Z

I'm deploying a Django app to a dev server and am hitting this error when I run pip install -r requirements.txt:pkg_resources appears to be distributed with setuptools.  Initially I thought this might not be installed to the Python in the virtualenv, so I installed setuptools 2.6 (same version as Python) to the Python site-packages in the virtualenv with the following command:EDIT: This only happens inside the virtualenv.  If I open a console outside the virtualenv then pkg_resources is present, but I am still getting the same error.Any ideas as to why pkg_resources is not on the path?July 2018 Update Most people should now use pip install setuptools (possibly with sudo).Some may need to (re)install the python-setuptools package via their package manager (apt-get install, yum install, etc.).This issue can be highly dependent on your OS and dev environment. See the legacy/other answers below if the above isn't working for you.ExplanationThis error message is caused by a missing/broken Python setuptools package. Per Matt M.'s comment and setuptools issue #581, the bootstrap script referred to below is no longer the recommended installation method.The bootstrap script instructions will remain below, in case it's still helpful to anyone.Legacy AnswerI encountered the same ImportError today while trying to use pip. Somehow the setuptools package had been deleted in my Python environment.To fix the issue, run the setup script for setuptools:(or if you don't have wget installed (e.g. OS X), trypossibly with sudo prepended.)If you have any version of distribute, or any setuptools below 0.6, you will have to uninstall it first.*See Installation Instructions for further details.* If you already have a working distribute, upgrading it to the "compatibility wrapper" that switches you over to setuptools is easier. But if things are already broken, don't try that.fixed it for me in Debian. Seems like uninstalling some .deb packages (twisted set in my case) has broken the path python uses to find packages I have seen this error while trying to install rhodecode to a virtualenv on ubuntu 13.10. For me the solution was to run before I run easy_install rhodecode.It also happened to me. I think the problem will happen if the requirements.txt contains a "distribute" entry while the virtualenv uses setuptools. Pip will try to patch setuptools to make room for distribute, but unfortunately it will fail half way.The easy solution is delete your current virtualenv then make a new virtualenv with --distribute argument.An example if using virtualenvwrapper:In CentOS 6 installing the package python-setuptools fixed it.I had this error earlier and the highest rated answer gave me an error trying to download the ez_setup.py file. I found another source so you can run the command:I found that I also had to use sudo to get it working, so you may need to run:I've also created another location that the script can be downloaded from:https://gist.github.com/ajtrichards/42e73562a89edb1039f3After trying several of these answers, then reaching out to a colleague, what worked for me on Ubuntu 16.04 was:In my case, it was only an old version of pillow 3.1.1 that was having trouble (pillow 4.x worked fine), and that's now resolved!Needed a little bit more sudo. Then used easy_install to install pip. Works.I fixed the error with virtualenv by doing this:Copied pkg_resources.py from /Library/Python/2.7/site-packages/setuptoolsto /Library/Python/2.7/site-packages/This may be a cheap workaround, but it worked for me..If setup tools doesn't exist, you can try installing system-site-packages by typing virtualenv --system-site-packages /DESTINATION DIRECTORY, changing the last part to be the directory you want to install to. pkg_rousources.py will be under that directory in lib/python2.7/site-packagesFor me, this error was being caused because I had a subdirectory called "site"!  I don't know if this is a pip bug or not, but I started with:/some/dir/requirements.txt

/some/dir/site/pip install -r requirements.txt wouldn't work, giving me the above error!renaming the subfolder from "site" to "src" fixed the problem!  Maybe pip is looking for "site-packages"?  Crazy.I had this problem when I had activated my virtualenv as a different user than the one who created it. It seems to be a permission problem. I discovered this when I tried the answer by @cwc and saw this in the output: Switching back to the user that created the virtualenv, then running the original pip install command went without problems. Hope this helps!I had this problem today as well. I only got the problem inside the virtual env. The solution for me was deactivating the virtual env, deleting and then uninstalling virtualenv with pip and reinstalling it. After that I created a new virtual env for my project, then pip worked fine both inside the virtual environment as in the normal environment.Looks like they have moved away from bitbucket and are now on github (https://github.com/pypa/setuptools)Command to run is:For me, it turned out to be a permissions problem on site-packages. Since it's only my dev environment, I raised the permissions and everything is working again:If you are encountering this issue with an application installed via conda, the solution (as stated in this bug report) is simply to install setup-tools with:On Windows, with python 3.7, this worked for me:--user installs packages in your home directory, which doesn't require admin privileges.Apparently you're missing setuptools. Some virtualenv versions use distribute instead of setuptools by default. Use the --setuptools option when creating the virtualenv or set the VIRTUALENV_SETUPTOOLS=1 in your environment.In my case, I had 2 python versions installed initially and later I had deleted the older one. So while creating the virtual environmentwas referring to the uninstalled pythonWhat worked for me Same is true when you are trying to use pip. I came across this answer when I was trying to follow this guide for OSX. What worked for me was, after running python get-pip, I had to ALSO easy_install pip. That fixed the issue of not being able to run pip at all. I did have a bunch of old macport stuff installed. That may have conflicted.On windows, I installed pip downloaded from www.lfd.uci.edu/~gohlke/pythonlibs/ then encontered this problem. So I should have installed setuptools(easy_install) first.just reinstall your setuptools by :then everything will be fine.I use CentOS 6.7, and my python was just upgrade from 2.6.6 to 2.7.11, after tried so many different answer, finally the following one does the job:Hope help someone in the same situation.None of the posted answers worked for me, so I reinstalled pip and it worked!(reference: http://www.saltycrane.com/blog/2010/02/how-install-pip-ubuntu/)I ran into this problem after updating my Ubuntu build.  It seems to have gone through and removed set up tools in all of my virtual environments.To remedy this I reinstalled the virtual environment back into the target directory.  This cleaned up missing setup tools and got things running again.e.g.:For me a good fix was to use --no-download option to virtualenv (VIRTUALENV_NO_DOWNLOAD=1 tox for tox.)On Opensuse 42.1 the following fixed this issue:ImportError: No module named pkg_resources: the solution is to reinstall python pip using the following  Command are under.Step: 1 Login in root user.Step: 2 Uninstall python-pip package if existing.Step: 3 Download files using wget command(File download in pwd )Step: 4 Run python file.Step: 5 Finaly exicute installation  command.  Note: User must be root.I experienced that error in my Google App Engine environment. And pip install -t lib setuptools fixed the issue.If you are using Python 3, you should use pip3 instead of pip. The command looks like $ pip3 install requirements.txtI have had the same problem when I used easy-install to install pip for python 2.7.14. For me the solution was (might not be the best, but worked for me, and this is probably the simplest) that the folder that contained the easy-install.py also contained a folder pkg_resources, and i have copy-pasted this folder into the same folder where my pip-script.py script was (python27\Scripts).

Since then, I found it in the python27\Lib\site-packages\pip-9.0.1-py2.7.egg\pip\_vendor folder as well, it might be a better solution to modify the pip-script.py file to import this.

Can I get JSON to load into an OrderedDict?

c00kiemonster

[Can I get JSON to load into an OrderedDict?](https://stackoverflow.com/questions/6921699/can-i-get-json-to-load-into-an-ordereddict)

Ok so I can use an OrderedDict in json.dump. That is, an OrderedDict can be used as an input to JSON.But can it be used as an output? If so how? In my case I'd like to load into an OrderedDict so I can keep the order of the keys in the file.If not, is there some kind of workaround?

2011-08-03 04:38:14Z

Ok so I can use an OrderedDict in json.dump. That is, an OrderedDict can be used as an input to JSON.But can it be used as an output? If so how? In my case I'd like to load into an OrderedDict so I can keep the order of the keys in the file.If not, is there some kind of workaround?Yes, you can.  By specifying the object_pairs_hook argument to JSONDecoder.  In fact, this is the exact example given in the documentation.You can pass this parameter to json.loads (if you don't need a Decoder instance for other purposes) like so:Using json.load is done in the same way:Simple version for Python 2.7+Or for Python 2.4 to 2.6Some great news! Since version 3.6 the cPython implementation has preserved the insertion order of dictionaries (https://mail.python.org/pipermail/python-dev/2016-September/146327.html). This means that the json library is now order preserving by default. Observe the difference in behaviour between python 3.5 and 3.6. The code:In py3.5 the resulting order is undefined:In the cPython implementation of python 3.6:The really great news is that this has become a language specification as of python 3.7 (as opposed to an implementation detail of cPython 3.6+): https://mail.python.org/pipermail/python-dev/2017-December/151283.htmlSo the answer to your question now becomes: upgrade to python 3.6! :)You could always write out the list of keys in addition to dumping the dict, and then reconstruct the OrderedDict by iterating through the list?In addition to dumping the ordered list of keys alongside the dictionary, another low-tech solution, which has the advantage of being explicit, is to dump the (ordered) list of key-value pairs ordered_dict.items(); loading is a simple OrderedDict(<list of key-value pairs>). This handles an ordered dictionary despite the fact that JSON does not have this concept (JSON dictionaries have no order).It is indeed nice to take advantage of the fact that json dumps the OrderedDict in the correct order. However, it is in general unnecessarily heavy and not necessarily meaningful to have to read all JSON dictionaries as an OrderedDict (through the object_pairs_hook argument), so an explicit conversion of only the dictionaries that must be ordered makes sense too.The normally used load command will work if you specify the object_pairs_hook parameter:

CSV file written with Python has blank lines between each row

l--''''''---------''''''''''''

[CSV file written with Python has blank lines between each row](https://stackoverflow.com/questions/3348460/csv-file-written-with-python-has-blank-lines-between-each-row)

This code reads thefile.csv, makes changes, and writes results to thefile_subset1.However, when I open the resulting csv in Microsoft Excel, there is an extra blank line after each record!Is there a way to make it not put an extra blank line?

2010-07-27 22:14:42Z

This code reads thefile.csv, makes changes, and writes results to thefile_subset1.However, when I open the resulting csv in Microsoft Excel, there is an extra blank line after each record!Is there a way to make it not put an extra blank line?In Python 2, open outfile with mode 'wb' instead of 'w'.  The csv.writer writes \r\n into the file directly.  If you don't open the file in binary mode, it will write \r\r\n because on Windows text mode will translate each \n into \r\n.In Python 3 the required syntax changed (see documentation links below), so open outfile with the additional parameter newline='' (empty string) instead.Opening the file in binary mode "wb" will not work in Python 3+.  Or rather, you'd have to convert your data to binary before writing it.  That's just a hassle.Instead, you should keep it in text mode, but override the newline as empty.  Like so:The simple answer is that csv files should always be opened in binary mode whether for input or output, as otherwise on Windows there are problems with the line ending. Specifically on output the csv module will write \r\n (the standard CSV row terminator) and then (in text mode) the runtime will replace the \n by \r\n (the Windows standard line terminator) giving a result of \r\r\n.Fiddling with the lineterminator is NOT the solution.Note: It seems this is not the preferred solution because of how the extra line was being added on a Windows system.  As stated in the python document:Windows is one such platform where that makes a difference.  While changing the line terminator as I described below may have fixed the problem, the problem could be avoided altogether by opening the file in binary mode.  One might say this solution is more "elegent".  "Fiddling" with the line terminator would have likely resulted in unportable code between systems in this case, where opening a file in binary mode on a unix system results in no effect.  ie. it results in cross system compatible code.From Python Docs: Original:As part of optional paramaters for the csv.writer if you are getting extra blank lines you may have to change the lineterminator (info here).  Example below adapated from the python page csv docs.  Change it from '\n' to whatever it should be.  As this is just a stab in the dark at the problem this may or may not work, but it's my best guess.I'm writing this answer w.r.t. to python 3, as I've initially got the same problem.I was supposed to get data from arduino using PySerial, and write them in a .csv file. Each reading in my case ended with '\r\n', so newline was always separating each line.In my case, newline='' option didn't work. Because it showed some error like :So it seemed that they don't accept omission of newline here.Seeing one of the answers here only, I mentioned line terminator in the writer object, like, writer = csv.writer(csv_file, delimiter=' ',lineterminator='\r')and that worked for me for skipping the extra newlines.The "lineterminator='\r'" permit to pass to next row, without empty row between two.Borrowing from this answer, it seems like the cleanest solution is to use io.TextIOWrapper.  I managed to solve this problem for myself as follows:The above answer is not compatible with Python 2.  To have compatibility, I suppose one would simply need to wrap all the writing logic in an if block:Use the method defined below to write data to the CSV file. Just add an additional newline='' parameter inside the open method :This will write CSV rows without creating additional rows!When using Python 3 the empty lines can be avoid by using the codecs module. As stated in the documentation, files are opened in binary mode so no change of the newline kwarg is necessary. I was running into the same issue recently and that worked for me:

Install a Python package into a different directory using pip?

Monika Sulik

[Install a Python package into a different directory using pip?](https://stackoverflow.com/questions/2915471/install-a-python-package-into-a-different-directory-using-pip)

I know the obvious answer is to use virtualenv and virtualenvwrapper, but for various reasons I can't/don't want to do that.So how do I modify the commandto make pip install the package somewhere other than the default site-packages?

2010-05-26 17:55:48Z

I know the obvious answer is to use virtualenv and virtualenvwrapper, but for various reasons I can't/don't want to do that.So how do I modify the commandto make pip install the package somewhere other than the default site-packages?Use:You might also want to use --ignore-installed to force all dependencies to be reinstalled using this new prefix.  You can use --install-option to multiple times to add any of the options you can use with python setup.py install (--prefix is probably what you want, but there are a bunch more options you could use).The --target switch is the thing you're looking for:But you still need to add d:\somewhere\other\than\the\default to PYTHONPATH to actually use them from that location.Upgrade pip if target switch is not available:On Linux or OS X:On Windows (this works around an issue):Instead of the --target option or the --install-options option, I have found that the following works well (from discussion on a bug regarding this very thing at https://github.com/pypa/pip/issues/446):(Or set the PYTHONUSERBASE directory in your environment before running the command, using export PYTHONUSERBASE=/path/to/install/to)This uses the very useful --user option but tells it to make the bin, lib, share and other directories you'd expect under a custom prefix rather than $HOME/.local.Then you can add this to your PATH, PYTHONPATH and other variables as you would a normal installation directory.Note that you may also need to specify the --upgrade and --ignore-installed options if any packages upon which this depends require newer versions to be installed in the PYTHONUSERBASE directory, to override the system-provided versions.A full example:..to install the scipy and numpy package most recent versions into a directory which you can then include in your PYTHONPATH like so (using bash and for python 2.6 on CentOS 6 for this example):Using virtualenv is still a better and neater solution!Installing a Python package often only includes some pure Python files. If the package includes data, scripts and or executables, these are installed in different directories from the pure Python files.Assuming your package has no data/scripts/executables, and that you want  your Python files to go into /python/packages/package_name (and not some subdirectory a few levels below /python/packages as when using --prefix), you can use the one time command:If you want all (or most) of your packages to go there, you can edit your ~/.pip/pip.conf to include:That way you can't forget about having to specify it again and again.Any excecutables/data/scripts included in the package will still go to their default places unless you specify addition install options (--prefix/--install-data/--install-scripts, etc., for details look at the custom installation options).To pip install a library exactly where I wanted it, I navigated to the location I wanted the directory with the terminal then usedthe logic of which I took from this page: https://cloud.google.com/appengine/docs/python/googlecloudstorageclient/downloadNobody seems to have mentioned the -t option but that the easiest:Just add one point to @Ian Bicking's answer:Using the --user option to specify the installed directory also work if one wants to install some Python package into one's home directory (without sudo user right) on remote server.E.g., The command will install the package into one of the directories that listed in your PYTHONPATH.Newer versions of pip (8 or later) can directly use the --prefix option:where $PREFIX_PATH is the installation prefix where lib, bin and other top-level folders are placed.Tested these options with python3.5 and pip 9.0.3:pip install --target /myfolder [packages] Installs ALL packages including dependencies under /myfolder. Does not take into account that dependent packages are already installed elsewhere in Python. You will find packages from /myfolder/[package_name]. In case you have multiple Python versions, this doesn't take that into account (no Python version in package folder name).pip install --prefix /myfolder [packages]Checks are dependencies already installed. Will install packages into /myfolder/lib/python3.5/site-packages/[packages]pip install --root /myfolder [packages]Checks dependencies like --prefix but install location will be /myfolder/usr/local/lib/python3.5/site-packages/[package_name].pip install --user [packages]Will install packages into $HOME:

/home/[USER]/.local/lib/python3.5/site-packages

Python searches automatically from this .local path so you don't need to put it to your PYTHONPATH. => In most of the cases --user is the best option to use. 

In case home folder can't be used because of some reason then --prefix.orTo add to the already good advice, as I had an issue installing IPython when I didn't have write permissions to /usr/local.pip uses distutils to do its install and this thread discusses how that can cause a problem as it relies on the sys.prefix setting.My issue happened when the IPython install tried to write to '/usr/local/share/man/man1' with Permission denied. As the install failed it didn't seem to write the IPython files in the bin directory.Using "--user" worked and the files were written to ~/.local. Adding ~/.local/bin to the $PATH meant I could use "ipython" from there.However I'm trying to install this for a number of users and had been given write permission to the /usr/local/lib/python2.7 directory. I created a "bin" directory under there and set directives for distutils:then (-I is used to force the install despite previous failures/.local install):Then I added /usr/local/lib/python2.7/bin to $PATH.I thought I'd include this in case anyone else has similar issues on a machine they don't have sudo access to.If you are using brew with python, unfortunately, pip/pip3 ships with very limited options. You do not have --install-option, --target, --user options as mentioned above.You might find this line very cumbersome. I suggest use pyenv for management. 

If you are using brew upgrade python python3Ironically you are actually downgrade pip functionality.(I post this answer, simply because pip in my mac osx does not have --target option, and I have spent hours fixing it)With pip v1.5.6 on Python v2.7.3 (GNU/Linux), option --root allows to specify a global installation prefix, (apparently) irrespective of specific package's options. Try f.i.,I suggest to follow the documentation and create ~/.pip/pip.conf file. Note in the documentation there are missing specified header directory, which leads to following error:The full working content of conf file is:Unfortunatelly I can install, but when try to uninstall pip tells me there is no such package for uninstallation process.... so something is still wrong but the package goes to its predefined location.pip install /path/to/package/is now possible. The difference with this and using the -e or --editable flag is that -e links to where the package is saved (i.e. your downloads folder), rather than installing it into your python path.This means if you delete/move the package to another folder, you won't be able to use it.

How do I find the duplicates in a list and create another list with them?

MFB

[How do I find the duplicates in a list and create another list with them?](https://stackoverflow.com/questions/9835762/how-do-i-find-the-duplicates-in-a-list-and-create-another-list-with-them)

How can I find the duplicates in a Python list and create another list of the duplicates? The list only contains integers.

2012-03-23 07:59:59Z

How can I find the duplicates in a Python list and create another list of the duplicates? The list only contains integers.To remove duplicates use set(a). To print duplicates, something like:Note that Counter is not particularly efficient (timings) and probably overkill here. set will perform better. This code computes a list of unique elements in the source order:or, more concisely:I don't recommend the latter style, because it is not obvious what not seen.add(x) is doing  (the set add() method always returns None, hence the need for not).To compute the list of duplicated elements without libraries:If list elements are not hashable, you cannot use sets/dicts and have to resort to a quadratic time solution (compare each with each). For example:You don't need the count, just whether or not the item was seen before. Adapted that answer to this problem:Just in case speed matters, here are some timings:Here are the results: (well done @JohnLaRooy!)Interestingly, besides the timings itself, also the ranking slightly changes when pypy is used. Most interestingly, the Counter-based approach benefits hugely from pypy's optimizations, whereas the method caching approach I have suggested seems to have almost no effect.Apparantly this effect is related to the "duplicatedness" of the input data. I have set l = [random.randrange(1000000) for i in xrange(10000)] and got these results:You can use iteration_utilities.duplicates:or if you only want one of each duplicate this can be combined with iteration_utilities.unique_everseen:It can also handle unhashable elements (however at the cost of performance):That's something that only a few of the other approaches here can handle.I did a quick benchmark containing most (but not all) of the approaches mentioned here.The first benchmark included only a small range of list-lengths because some approaches have O(n**2) behavior.In the graphs the y-axis represents the time, so a lower value means better. It's also plotted log-log so the wide range of values can be visualized better:Removing the O(n**2) approaches I did another benchmark up to half a million elements in a list:As you can see the iteration_utilities.duplicates approach is faster than any of the other approaches and even chaining unique_everseen(duplicates(...)) was faster or equally fast than the other approaches.One additional interesting thing to note here is that the pandas approaches are very slow for small lists but can easily compete for longer lists.However as these benchmarks show most of the approaches perform roughly equally, so it doesn't matter much which one is used (except for the 3 that had O(n**2) runtime). 1 This is from a third-party library I have written: iteration_utilities.I came across this question whilst looking in to something related - and wonder why no-one offered a generator based solution?  Solving this problem would be:I was concerned with scalability, so tested several approaches, including naive items that work well on small lists, but scale horribly as lists get larger (note- would have been better to use timeit, but this is illustrative).I included @moooeeeep for comparison (it is impressively fast: fastest if the input list is completely random) and an itertools approach that is even faster again for mostly sorted lists...  Now includes pandas approach from @firelynx -- slow, but not horribly so, and simple. Note - sort/tee/zip approach is consistently fastest on my machine for large mostly ordered lists, moooeeeep is fastest for shuffled lists, but your mileage may vary.AdvantagesAssumptionsFastest solution, 1m entries:Approaches testedThe results for the 'all dupes' test were consistent, finding "first" duplicate then "all" duplicates in this array:When the lists are shuffled first, the price of the sort becomes apparent - the efficiency drops noticeably and the @moooeeeep approach dominates, with set & dict approaches being similar but lessor performers:Using pandas:collections.Counter is new in python 2.7:In an earlier version you can use a conventional dict instead:Here's a neat and concise solution -Without converting to list and probably the simplest way would be something like below.

This may be useful during a interview when they ask not to use sets======= else to get 2 separate lists of unique values and duplicate valuesI would do this with pandas, because I use pandas a lotGivesProbably isn't very efficient, but it sure is less code than a lot of the other answers, so I thought I would contributethe third example of the accepted answer give an erroneous answer and does not attempt to give duplicates. Here is the correct version :How about simply loop through each element in the list by checking the number of occurrences, then adding them to a set which will then print the duplicates. Hope this helps someone out there.A bit late, but maybe helpful for some.

For a largish list, I found this worked for me.Shows just and all duplicates and preserves order.We can use itertools.groupby in order to find all the items that have dups:The output will be:I guess the most effective way to find duplicates in a list is:It uses the Counter the all the elements and all unique elements. Subtracting the first one with the second will leave out only the duplicates.Very simple and quick way of finding dupes with one iteration in Python is:Output will be as follows:This and more in my blog http://www.howtoprogramwithpython.comSome other tests. Of course to do......is too costly. It's about 500 times faster (the more long array gives better results) to use the next final method:Only 2 loops, no very costly l.count() operations.Here is a code to compare the methods for example. The code is below, here is the output:The testing code:I am entering much much late in to this discussion. Even though, I would like to deal with this problem with one liners . Because that's the charm of Python.

if we just want to get the duplicates in to a separate list (or any collection),I would suggest to do as below.Say we have a duplicated list which we can call as 'target'Now if we want to get the duplicates,we can use the one liner as below:This code will put the duplicated records as key and count as value in to the dictionary 'duplicates'.'duplicate' dictionary will look like as below:If you just want all the records with duplicates alone in a list, its again much shorter code:Output will be:This works perfectly in python 2.7.x + versionsMethod 1:Explanation:

[val for idx, val in enumerate(input_list) if val in input_list[idx+1:]] is a list comprehension, that returns an element, if the same element is present from it's current position, in list, the index.Example: 

input_list = [42,31,42,31,3,31,31,5,6,6,6,6,6,7,42]starting with the first element in list, 42, with index 0, it checks if the element 42, is present in input_list[1:] (i.e., from index 1 till end of list)

Because 42 is present in input_list[1:], it will return 42.Then it goes to the next element 31, with index 1, and checks if element 31 is present in the input_list[2:] (i.e., from index 2 till end of list), 

Because 31 is present in input_list[2:], it will return 31.similarly it goes through all the elements in the list, and will return only the  repeated/duplicate elements into a list.Then because we have duplicates, in a list, we need to pick one of each duplicate, i.e. remove duplicate among duplicates, and to do so, we do call a python built-in named set(), and it removes the duplicates,Then we are left with a set, but not a list, and hence to convert from a set to list, we use, typecasting, list(), and that converts the set of elements to a list.Method 2:Explanation:

Here We create two empty lists, to start with.

Then keep traversing through all the elements of the list, to see if it exists in temp_list (initially empty). If it is not there in the temp_list, then we add it to the temp_list, using append method.If it already exists in temp_list, it means, that the current element of the list is a duplicate, and hence we need to add it to dupe_list using append method.You basically remove duplicates by converting to set (clean_list), then iterate the raw_list, while removing each item in the clean list for occurrence in raw_list. If item is not found, the raised ValueError Exception is caught and the item is added to duplicated_items list. If the index of duplicated items is needed, just enumerate the list and play around with the index. (for index, item in enumerate(raw_list):) which is faster and optimised for large lists (like thousands+ of elements)use of list.count() method in the list to find out the duplicate elements of a given listOne line solution:There are a lot of answers up here, but I think this is relatively a very readable and easy to understand approach:Notes:Here's a fast generator that uses a dict to store each element as a key with a boolean value for checking if the duplicate item has already been yielded.For lists with all elements that are hashable types:For lists that might contain lists:When using toolz:one-liner, for fun, and where a single statement is required.this is the way I had to do it because I challenged myself not to use other methods:so your sample works as:Using Set Function

eg:-eg:-eg:-eg:-eg:-

What is a Python equivalent of PHP's var_dump()? [duplicate]

Zoredache

[What is a Python equivalent of PHP's var_dump()? [duplicate]](https://stackoverflow.com/questions/383944/what-is-a-python-equivalent-of-phps-var-dump)

When debugging in PHP, I frequently find it useful to simply stick a var_dump() in my code to show me what a variable is, what its value is, and the same for anything that it contains.What is a good Python equivalent for this?

2008-12-21 00:43:36Z

When debugging in PHP, I frequently find it useful to simply stick a var_dump() in my code to show me what a variable is, what its value is, and the same for anything that it contains.What is a good Python equivalent for this?To display a value nicely, you can use the pprint module. The easiest way to dump all variables with it is to doIf you are running in CGI, a useful debugging feature is the cgitb module, which displays the value of local variables as part of the traceback.I think the best equivalent to PHP's var_dump($foo, $bar) is combine print with vars:The closest thing to PHP's var_dump() is pprint() with the getmembers() function in the built-in inspect module:PHP's var_export() usually shows a serialized version of the object that can be exec()'d to re-create the object.    The closest thing to that in Python is repr()"For many types, this function makes an attempt to return a string that would yield an object with the same value when passed to eval() [...]"I wrote a very light-weight alternative to PHP's var_dump for using in Python and made it open source later.GitHub: https://github.com/sha256/python-var-dumpYou can simply install it using pip:So I have taken the answers from this question and another question and came up below.  I suspect this is not pythonic enough for most people, but I really wanted something that let me get a deep representation of the values some unknown variable has.  I would appreciate any suggestions about how I can improve this or achieve the same behavior easier.Here is the usageand the results.printFor your own classes, just def a __str__ methodOld topic, but worth a try.Here is a simple and efficient var_dump function:Sample output:I don't have PHP experience, but I have an understanding of the Python standard library.For your purposes, Python has several methods:logging module;Object serialization module which is called pickle. You may write your own wrapper of the pickle module.If your using var_dump for testing, Python has its own doctest and unittest modules. It's very simple and fast for design.I use self-written Printer class, but dir() is also good for outputting the instance fields/values.The sample of usage:

Pretty printing XML in Python

Hortitude

[Pretty printing XML in Python](https://stackoverflow.com/questions/749796/pretty-printing-xml-in-python)

What is the best way (or are the various ways) to pretty print XML in Python?

2009-04-15 00:05:41Z

What is the best way (or are the various ways) to pretty print XML in Python?lxml is recent, updated, and includes a pretty print functionCheck out the lxml tutorial:

http://lxml.de/tutorial.htmlAnother solution is to borrow this indent function, for use with the ElementTree library that's built in to Python since 2.5.

Here's what that would look like:Here's my (hacky?) solution to get around the ugly text node problem.The above code will produce:Instead of this:Disclaimer: There are probably some limitations.As others pointed out, lxml has a pretty printer built in.Be aware though that by default it changes CDATA sections to normal text, which can have nasty results.Here's a Python function that preserves the input file and only changes the indentation (notice the strip_cdata=False). Furthermore it makes sure the output uses UTF-8 as encoding instead of the default ASCII (notice the encoding='utf-8'):Example usage:BeautifulSoup has a easy to use prettify() method. It indents one space per indentation level. It works much better than lxml's pretty_print and is short and sweet. I tried to edit "ade"s answer above, but Stack Overflow wouldn't let me edit after I had initially provided feedback anonymously.  This is a less buggy version of the function to pretty-print an ElementTree.If you have xmllint you can spawn a subprocess and use it. xmllint --format <file> pretty-prints its input XML to standard output.Note that this method uses an program external to python, which makes it sort of a hack.If you're using a DOM implementation, each has their own form of pretty-printing built-in:If you're using something else without its own pretty-printer — or those pretty-printers don't quite do it the way you want —  you'd probably have to write or subclass your own serialiser.I had some problems with minidom's pretty print.  I'd get a UnicodeError whenever I tried pretty-printing a document with characters outside the given encoding, eg if I had a β in a document and I tried doc.toprettyxml(encoding='latin-1').  Here's my workaround for it:It won't add spaces or newlines inside text nodes, unless you ask for it with:You can specify what the indentation unit should be and what the newline should look like.The doc is on http://www.yattag.org homepage.XML pretty print for python looks pretty good for this task.  (Appropriately named, too.)An alternative is to use pyXML, which has a PrettyPrint function.I wrote a solution to walk through an existing ElementTree and use text/tail to indent it as one typically expects.You can use popular external library xmltodict, with unparse and pretty=True you will get best result:full_document=False against <?xml version="1.0" encoding="UTF-8"?> at the top.Take a look at the vkbeautify module.It is a python version of my very popular javascript/nodejs plugin with the same name. It can pretty-print/minify XML, JSON and CSS text. Input and output can be string/file in any combinations. It is very compact and doesn't have any dependency.Examples:An alternative if you don't want to have to reparse, there is the xmlpp.py library with the get_pprint() function. It worked nice and smoothly for my use cases, without having to reparse to an lxml ElementTree object.Here's a Python3 solution that gets rid of the ugly newline issue (tons of whitespace), and it only uses standard libraries unlike most other implementations.I found how to fix the common newline issue here.I had this problem and solved it like this:In my code this method is called like this:This works only because etree by default uses two spaces to indent, which I don't find very much emphasizing the indentation and therefore not pretty. I couldn't ind any setting for etree or parameter for any function to change the standard etree indent. I like how easy it is to use etree, but this was really annoying me.For converting an entire xml document to a pretty xml document

(ex: assuming you've extracted [unzipped] a LibreOffice Writer .odt or .ods file, and you want to convert the ugly "content.xml" file to a pretty one for automated git version control and git difftooling of .odt/.ods files, such as I'm implementing here)References:

- Thanks to Ben Noland's answer on this page which got me most of the way there.It's working well for the xml with Chinese!You can try this variation...Install BeautifulSoup and the backend lxml (parser) libraries:Process your XML document:I solved this with some lines of code, opening the file, going trough it and adding indentation, then saving it again. I was working with small xml files, and did not want to add dependencies, or more libraries to install for the user. Anyway, here is what I ended up with:It works for me, perhaps someone will have some use of it :)

How do I resize an image using PIL and maintain its aspect ratio?

saturdayplace

[How do I resize an image using PIL and maintain its aspect ratio?](https://stackoverflow.com/questions/273946/how-do-i-resize-an-image-using-pil-and-maintain-its-aspect-ratio)

Is there an obvious way to do this that I'm missing?  I'm just trying to make thumbnails.

2008-11-07 23:08:04Z

Is there an obvious way to do this that I'm missing?  I'm just trying to make thumbnails.Define a maximum size.

Then, compute a resize ratio by taking min(maxwidth/width, maxheight/height).The proper size is oldsize*ratio.There is of course also a library method to do this: the method Image.thumbnail.

Below is an (edited) example from the PIL documentation.This script will resize an image (somepic.jpg) using PIL (Python Imaging Library) to a width of 300 pixels and a height proportional to the new width. It does this by determining what percentage 300 pixels is of the original width (img.size[0]) and then multiplying the original height (img.size[1]) by that percentage. Change "basewidth" to any other number to change the default width of your images.I also recommend using PIL's thumbnail method, because it removes all the ratio hassles from you.One important hint, though: Replacewithby default, PIL uses the Image.NEAREST filter for resizing which results in good performance, but poor quality.Based in @tomvon, I finished using the following (pick your case):a) Resizing height (I know the new width, so I need the new height)b) Resizing width (I know the new height, so I need the new width)Then just:PIL already has the option to crop an imageIf you are trying to maintain the same aspect ratio, then wouldn't you resize by some percentage of the original size?For example, half the original sizeI use this library:If you don't want / don't have a need to open image with Pillow, use this:I was trying to resize some images for a slideshow video and because of that, I wanted not just one max dimension, but a max width and a max height (the size of the video frame).

And there was always the possibility of a portrait video...

The Image.thumbnail method was promising, but I could not make it upscale a smaller image.So after I couldn't find an obvious way to do that here (or at some other places), I wrote this function and put it here for the ones to come:Just updating this question with a more modern wrapper

This library wraps Pillow (a fork of PIL)

https://pypi.org/project/python-resize-image/Allowing you to do something like this :-Heaps more examples in the above link.A simple method for keeping constrained ratios and passing a max width / height. Not the prettiest but gets the job done and is easy to understand:Here's a python script that uses this function to run batch image resizing.My ugly example.Function get file like: "pic[0-9a-z].[extension]", resize them to 120x120, moves section to center and save to "ico[0-9a-z].[extension]", works with portrait and landscape:I resizeed the image in such a way and it's working very wellHave updated the answer above by "tomvon"Open your image file Use PIL Image.resize(size, resample=0) method, where you substitute (width, height) of your image for the size 2-tuple.This will display your image at original size:This will display your image at 1/2 the size:This will display your image at 1/3 the size:This will display your image at 1/4 the size:etc etc 

Convert a list of characters into a string

nos

[Convert a list of characters into a string](https://stackoverflow.com/questions/4481724/convert-a-list-of-characters-into-a-string)

If I have a list of chars:How do I convert it into a single string?

2010-12-19 05:19:06Z

If I have a list of chars:How do I convert it into a single string?Use the join method of the empty string to join all of the strings together with the empty string in between, like so:This works in many popular languages like JavaScript and Ruby, why not in Python?Strange enough, in Python the join method is on the str class:Why join is not a method in the list object like in JavaScript or other popular script languages? It is one example of how the Python community thinks. Since join is returning a string, it should be placed in the string class, not on the list class, so the str.join(list) method means: join the list into a new string using str as a separator (in this case str is an empty string). Somehow I got to love this way of thinking after a while. I can complain about a lot of things in Python design, but not about its coherence.If your Python interpreter is old (1.5.2, for example, which is common on some older Linux distributions), you may not have join() available as a method on any old string object, and you will instead need to use the string module. Example:The string b will be 'abcd'.This may be the fastest way:The reduce function also worksbesides str.join which is the most natural way, a possibility is to use io.StringIO and abusing writelines to write all elements in one go:prints:When using this approach with a generator function or an iterable which isn't a tuple or a list, it saves the temporary list creation that join does to allocate the right size in one go (and a list of 1-character strings is very expensive memory-wise).If you're low in memory and you have a lazily-evaluated object as input, this approach is the best solution.If the list contains numbers, you can use map() with join().Eg:You could also use operator.concat() like this:If you're using Python 3 you need to prepend:since the builtin reduce() has been removed from Python 3 and now lives in functools.reduce(). 

Generator Expressions vs. List Comprehension

Readonly

[Generator Expressions vs. List Comprehension](https://stackoverflow.com/questions/47789/generator-expressions-vs-list-comprehension)

When should you use generator expressions and when should you use list comprehensions in Python?

2008-09-06 20:07:59Z

When should you use generator expressions and when should you use list comprehensions in Python?John's answer is good (that list comprehensions are better when you want to iterate over something multiple times).  However, it's also worth noting that you should use a list if you want to use any of the list methods.  For example, the following code won't work:Basically, use a generator expression if all you're doing is iterating once.  If you want to store and use the generated results, then you're probably better off with a list comprehension.Since performance is the most common reason to choose one over the other, my advice is to not worry about it and just pick one; if you find that your program is running too slowly, then and only then should you go back and worry about tuning your code.Iterating over the generator expression or the list comprehension will do the same thing. However, the list comprehension will create the entire list in memory first while the generator expression will create the items on the fly, so you are able to use it for very large (and also infinite!) sequences.Use list comprehensions when the result needs to be iterated over multiple times, or where speed is paramount. Use generator expressions where the range is large or infinite.See Generator expressions and list comprehensions for more info.The important point is that the list comprehension creates a new list. The generator creates a an iterable object that will "filter" the source material on-the-fly as you consume the bits.Imagine you have a 2TB log file called "hugefile.txt", and you want the content and length for all the lines that start with the word "ENTRY".So you try starting out by writing a list comprehension:This slurps up the whole file, processes each line, and stores the matching lines in your array. This array could therefore contain up to 2TB of content. That's a lot of RAM, and probably not practical for your purposes.So instead we can use a generator to apply a "filter" to our content. No data is actually read until we start iterating over the result.Not even a single line has been read from our file yet. In fact, say we want to filter our result even further:Still nothing has been read, but we've specified now two generators that will act on our data as we wish.Lets write out our filtered lines to another file:Now we read the input file. As our for loop continues to request additional lines, the long_entries generator demands lines from the entry_lines generator, returning only those whose length is greater than 80 characters. And in turn, the entry_lines generator requests lines (filtered as indicated) from the logfile iterator, which in turn reads the file.So instead of "pushing" data to your output function in the form of a fully-populated list, you're giving the output function a way to "pull" data only when its needed. This is in our case much more efficient, but not quite as flexible. Generators are one way, one pass; the data from the log file we've read gets immediately discarded, so we can't go back to a previous line. On the other hand, we don't have to worry about keeping data around once we're done with it.The benefit of a generator expression is that it uses less memory since it doesn't build the whole list at once.  Generator expressions are best used when the list is an intermediary, such as summing the results, or creating a dict out of the results.For example:The advantage there is that the list isn't completely generated, and thus little memory is used (and should also be faster)You should, though, use list comprehensions when the desired final product is a list.  You are not going to save any memeory using generator expressions, since you want the generated list.  You also get the benefit of being able to use any of the list functions like sorted or reversed.For example:When creating a generator from a mutable object (like a list) be aware that the generator will get evaluated on the state of the list at time of using the generator, not at time of the creation of the generator:If there is any chance of your list getting modified (or a mutable object inside that list) but you need the state at creation of the generator you need to use a list comprehension instead.Sometimes you can get away with the tee function from itertools, it returns multiple iterators for the same generator that can be used independently.I'm using the Hadoop Mincemeat module. I think this is a great example to take a note of:Here the generator gets numbers out of a text file (as big as 15GB) and applies simple math on those numbers using Hadoop's map-reduce. If I had not used the yield function, but instead a list comprehension, it would have taken a much longer time calculating the sums and average (not to mention the space complexity).Hadoop is a great example for using all the advantages of Generators.

How to convert index of a pandas dataframe into a column?

msakya

[How to convert index of a pandas dataframe into a column?](https://stackoverflow.com/questions/20461165/how-to-convert-index-of-a-pandas-dataframe-into-a-column)

This seems rather obvious, but I can't seem to figure out how to convert an index of data frame to a column?For example:To,

2013-12-09 00:34:16Z

This seems rather obvious, but I can't seem to figure out how to convert an index of data frame to a column?For example:To,either:or, .reset_index:so, if you have a multi-index frame with 3 levels of index, like:and you want to convert the 1st (tick) and 3rd (obs) levels in the index into columns, you would do:For MultiIndex you can extract its subindex using where si_name is the name of the subindex.To provide a bit more clarity, let's look at a DataFrame with two levels in its index (a MultiIndex).The reset_index method, called with the default parameters, converts all index levels to columns and uses a simple RangeIndex as new index.Use the level parameter to control which index levels are converted into columns. If possible, use the level name, which is more explicit. If there are no level names, you can refer to each level by its integer location, which begin at 0 from the outside. You can use a scalar value here or a list of all the indexes you would like to reset.In the rare event that you want to preserve the index and turn the index into a column, you can do the following:You can first rename your index to a desired label, then elevate to a series:This works also for MultiIndex dataframes:If you want to use the reset_index method and also preserve your existing index you should use:or to change it in place:For example:And if you want to get rid of the index label you can do:

How to retrieve an element from a set without removing it?

Daren Thomas

[How to retrieve an element from a set without removing it?](https://stackoverflow.com/questions/59825/how-to-retrieve-an-element-from-a-set-without-removing-it)

Suppose the following:How do I get a value (any value) out of s without doing s.pop()? I want to leave the item in the set until I am sure I can remove it - something I can only be sure of after an asynchronous call to another host.Quick and dirty:But do you know of a better way? Ideally in constant time.

2008-09-12 19:58:33Z

Suppose the following:How do I get a value (any value) out of s without doing s.pop()? I want to leave the item in the set until I am sure I can remove it - something I can only be sure of after an asynchronous call to another host.Quick and dirty:But do you know of a better way? Ideally in constant time.Two options that don't require copying the whole set:Or...But in general, sets don't support indexing or slicing.Least code would be:Obviously this would create a new list which contains each member of the set, so not great if your set is very large.for first_item in muh_set: break remains the optimal approach in Python 3.x. Curse you, Guido.Welcome to yet another set of Python 3.x timings, extrapolated from wr.'s excellent Python 2.x-specific response. Unlike AChampion's equally helpful Python 3.x-specific response, the timings below also time outlier solutions suggested above – including:Turn on, tune in, time it:Behold! Ordered by fastest to slowest snippets:Unsurprisingly, manual iteration remains at least twice as fast as the next fastest solution. Although the gap has decreased from the Bad Old Python 2.x days (in which manual iteration was at least four times as fast), it disappoints the PEP 20 zealot in me that the most verbose solution is the best. At least converting a set into a list just to extract the first element of the set is as horrible as expected. Thank Guido, may his light continue to guide us.Surprisingly, the RNG-based solution is absolutely horrible. List conversion is bad, but random really takes the awful-sauce cake. So much for the Random Number God.I just wish the amorphous They would PEP up a set.get_first() method for us already. If you're reading this, They: "Please. Do something."To provide some timing figures behind the different approaches, consider the following code.

The get() is my custom addition to Python's setobject.c, being just a pop() without removing the element.The output is:This means that the for/break solution is the fastest (sometimes faster than the custom get() solution).I wondered how the functions will perform for different sets, so I did a benchmark:This plot clearly shows that some approaches (RandomSample, SetUnpacking and ListIndex) depend on the size of the set and should be avoided in the general case (at least if performance might be important). As already shown by the other answers the fastest way is ForLoop.However as long as one of the constant time approaches is used the performance difference will be negligible.iteration_utilities (Disclaimer: I'm the author) contains a convenience function for this use-case: first:I also included it in the benchmark above. It can compete with the other two "fast" solutions but the difference isn't much either way.Since you want a random element, this will also work:The documentation doesn't seem to mention performance of random.sample. From a really quick empirical test with a huge list and a huge set, it seems to be constant time for a list but not for the set. Also, iteration over a set isn't random; the order is undefined but predictable:If randomness is important and you need a bunch of elements in constant time (large sets), I'd use random.sample and convert to a list first:Seemingly the most compact (6 symbols) though very slow way to get a set element (made possible by PEP 3132):With Python 3.5+ you can also use this 7-symbol expression (thanks to PEP 448):Both options are roughly 1000 times slower on my machine than the for-loop method.I use a utility function I wrote.  Its name is somewhat misleading because it kind of implies it might be a random item or something like that.Following @wr. post, I get similar results (for Python3.5)Output:However, when changing the underlying set (e.g. call to remove()) things go badly for the iterable examples (for, iter):Results in:How about s.copy().pop()? I haven't timed it, but it should work and it's simple. It works best for small sets however, as it copies the whole set.Another option is to use a dictionary with values you don't care about.  E.g.,You can treat the keys as a set except that they're just an array:A side effect of this choice is that your code will be backwards compatible with older, pre-set versions of Python.  It's maybe not the best answer but it's another option.Edit: You can even do something like this to hide the fact that you used a dict instead of an array or set:

Unzipping files in Python

John Howard

[Unzipping files in Python](https://stackoverflow.com/questions/3451111/unzipping-files-in-python)

I read through the zipfile documentation, but couldn't understand how to unzip a file, only how to zip a file. How do I unzip all the contents of a zip file into the same directory?

2010-08-10 16:19:32Z

I read through the zipfile documentation, but couldn't understand how to unzip a file, only how to zip a file. How do I unzip all the contents of a zip file into the same directory?That's pretty much it!If you are using Python 3.2 or later:You dont need to use the close or try/catch with this as it uses the

context manager construction.Use the extractall method, if you're using Python 2.6+You can also import only ZipFile:Works in Python 2 and Python 3.try this :

    path : unzip file's pathThis does not contain validation for the file if its not zip. If the folder contains non .zip file it will fail.  

Convert string to JSON using Python

Frias

[Convert string to JSON using Python](https://stackoverflow.com/questions/4528099/convert-string-to-json-using-python)

I'm a little bit confused with JSON in Python.

To me, it seems like a dictionary, and for that reason

I'm trying to do that:But when I do print dict(json), it gives an error.How can I transform this string into a structure and then call json["title"] to obtain "example glossary"?

2010-12-24 19:48:19Z

I'm a little bit confused with JSON in Python.

To me, it seems like a dictionary, and for that reason

I'm trying to do that:But when I do print dict(json), it gives an error.How can I transform this string into a structure and then call json["title"] to obtain "example glossary"?json.loads()When I started using json, I was confused and unable to figure it out for some time, but finally I got what I wanted

Here is the simple solution    use simplejson or cjson for speedupsIf you trust the data source, you can use eval to convert your string into a dictionary:eval(your_json_format_string)Example:

Difference between exit() and sys.exit() in Python

Drake Guan

[Difference between exit() and sys.exit() in Python](https://stackoverflow.com/questions/6501121/difference-between-exit-and-sys-exit-in-python)

In Python, there are two similarly-named functions, exit() and sys.exit(). What's the difference and when should I use one over the other?

2011-06-28 03:05:31Z

In Python, there are two similarly-named functions, exit() and sys.exit(). What's the difference and when should I use one over the other?exit is a helper for the interactive shell - sys.exit is intended for use in programs.Technically, they do mostly the same: raising SystemExit. sys.exit does so in sysmodule.c:While exit is defined in site.py and _sitebuiltins.py, respectively.Note that there is a third exit option, namely os._exit, which exits without calling cleanup handlers, flushing stdio buffers, etc. (and which  should normally only be used in the child process after a fork()).If I use exit() in a code and run it in the shell, it shows a message asking whether I want to kill the program or not. It's really disturbing.

See hereBut sys.exit() is better in this case. It closes the program and doesn't create any dialogue box. 

How do I read CSV data into a record array in NumPy?

hatmatrix

[How do I read CSV data into a record array in NumPy?](https://stackoverflow.com/questions/3518778/how-do-i-read-csv-data-into-a-record-array-in-numpy)

I wonder if there is a direct way to import the contents of a CSV file into a record array, much in the way that R's read.table(), read.delim(), and read.csv() family imports data to R's data frame?Or is the best way to use csv.reader() and then apply something like numpy.core.records.fromrecords()?

2010-08-19 04:41:53Z

I wonder if there is a direct way to import the contents of a CSV file into a record array, much in the way that R's read.table(), read.delim(), and read.csv() family imports data to R's data frame?Or is the best way to use csv.reader() and then apply something like numpy.core.records.fromrecords()?You can use Numpy's genfromtxt() method to do so, by setting the delimiter kwarg to a comma.More information on the function can be found at its respective documentation.I would recommend the read_csv function from the pandas library:This gives a pandas DataFrame - allowing many useful data manipulation functions which are not directly available with numpy record arrays.I would also recommend genfromtxt. However, since the question asks for a record array, as opposed to a normal array, the dtype=None parameter needs to be added to the genfromtxt call:Given an input file, myfile.csv:gives an array:and gives a record array:This has the advantage that file with multiple data types (including strings) can be easily imported.I timed theversuson 4.6 million rows with about 70 columns and found that the NumPy path took 2 min 16 secs and the csv-list comprehension method took 13 seconds.I would recommend the csv-list comprehension method as it is most likely relies on pre-compiled libraries and not the interpreter as much as NumPy. I suspect the pandas method would have similar interpreter overhead.You can also try recfromcsv() which can guess data types and return a properly formatted record array.As I tried both ways using NumPy and Pandas, using pandas has a lot of advantages:This is my test code:With NumPy and pandas at versions:You can use this code to send CSV file data into an array:This is the easiest way:import csv

 with open('testfile.csv', newline='') as csvfile:

     data = list(csv.reader(csvfile))Now each entry in data is a record, represented as an array. So you have a 2D array. It saved me so much time.I tried this:Using numpy.loadtxtA quite simple method. But it requires all the elements being float (int and so on)I would suggest using tables (pip3 install tables). You can save your .csv file to .h5 using pandas (pip3 install pandas),You can then easily, and with less time even for huge amount of data, load your data in a NumPy array.This work as a charm...

Difference between __getattr__ vs __getattribute__

Yarin

[Difference between __getattr__ vs __getattribute__](https://stackoverflow.com/questions/3278077/difference-between-getattr-vs-getattribute)

I am trying to understand when to use __getattr__ or __getattribute__.  The documentation mentions __getattribute__ applies to new-style classes. What are new-style classes?  

2010-07-19 02:11:19Z

I am trying to understand when to use __getattr__ or __getattribute__.  The documentation mentions __getattribute__ applies to new-style classes. What are new-style classes?  A key difference between __getattr__ and __getattribute__ is that __getattr__ is only invoked if the attribute wasn't found the usual ways.  It's good for implementing a fallback for missing attributes, and is probably the one of two you want.__getattribute__ is invoked before looking at the actual attributes on the object, and so can be tricky to implement correctly.  You can end up in infinite recursions very easily.New-style classes derive from object, old-style classes are those in Python 2.x with no explicit base class.  But the distinction between old-style and new-style classes is not the important one when choosing between __getattr__ and __getattribute__.You almost certainly want __getattr__.Lets see some simple examples of both __getattr__ and __getattribute__ magic methods.Python will call  __getattr__ method whenever you request an attribute that hasn't already been defined. In the following example my class Count has no __getattr__ method. Now in main when I try to access both obj1.mymin and obj1.mymax attributes everything works fine. But when I try to access obj1.mycurrent attribute -- Python gives me AttributeError: 'Count' object has no attribute 'mycurrent'Now my class Count has __getattr__ method. Now when I try to access  obj1.mycurrent attribute -- python returns me whatever I have implemented in my __getattr__ method. In my example whenever I try to call an attribute which doesn't exist, python creates that attribute and set it to integer value 0. Now lets see the __getattribute__ method. If you have  __getattribute__ method in your class,  python invokes this method for every attribute regardless whether it exists or not. So why we need __getattribute__ method? One good reason is that you can prevent access to attributes and make them more secure as shown in the following example.Whenever someone try to access my attributes that starts with substring 'cur' python raises AttributeError exception. Otherwise it returns that attribute. Important: In order to avoid infinite recursion in __getattribute__ method, its implementation should always call the base class method with the same name to access any attributes it needs. For example: object.__getattribute__(self, name) or  super().__getattribute__(item) and not self.__dict__[item]If your class  contain both getattr and getattribute magic methods then  __getattribute__ is called first. But if  __getattribute__ raises 

AttributeError exception then the exception will be ignored and __getattr__ method will be invoked. See the following example:This is just an example based on Ned Batchelder's explanation. __getattr__ example:And if same example is used with __getattribute__ You would get >>> RuntimeError: maximum recursion depth exceeded while calling a Python objectNew-style classes inherit from object, or from another new style class:Old-style classes don't:This only applies to Python 2 - in Python 3 all the above will create new-style classes.See 9. Classes (Python tutorial), NewClassVsClassicClass and What is the difference between old style and new style classes in Python? for details.New-style classes are ones that subclass "object" (directly or indirectly).  They have a __new__ class method in addition to __init__ and have somewhat more rational low-level behavior.Usually, you'll want to override __getattr__ (if you're overriding either), otherwise you'll have a hard time supporting "self.foo" syntax within your methods.Extra info: http://www.devx.com/opensource/Article/31482/0/page/4

Expanding tuples into arguments

AkiRoss

[Expanding tuples into arguments](https://stackoverflow.com/questions/1993727/expanding-tuples-into-arguments)

Is there a way to expand a Python tuple into a function - as actual parameters?For example, here expand() does the magic:I know one could define myfun as myfun((a, b, c)), but of course there may be legacy code.

Thanks

2010-01-03 02:22:49Z

Is there a way to expand a Python tuple into a function - as actual parameters?For example, here expand() does the magic:I know one could define myfun as myfun((a, b, c)), but of course there may be legacy code.

Thanksmyfun(*tuple) does exactly what you request. The * operator simply unpacks the tuple (or any iterable) and passes them as the positional arguments to the function. See here for more info.Side issue: don't use as your identifiers builtin type names such as tuple, list, file, set, and so forth -- it's horrible practice and it will come back and bite you when you least expect it,

so just get into the habit of actively avoiding hiding builtin names with your own identifiers.Note that you can also expand part of argument list:Take a look at the Python tutorial section 4.7.3 and 4.7.4.

It talks about passing tuples as arguments.I would also consider using named parameters (and passing a dictionary) instead of using a tuple and passing a sequence. I find the use of positional arguments to be a bad practice when the positions are not intuitive or there are multiple parameters. This is the functional programming method. It lifts the tuple expansion feature out of syntax sugar:apply_tuple = lambda f, t: f(*t)Example usage:curry redefiniton of apply_tuple saves a lot of partial calls in the long run.I ran across similar problem and created this function which expand the fixed function. Hope this helps.

Find all packages installed with easy_install/pip?

Jürgen A. Erhard

[Find all packages installed with easy_install/pip?](https://stackoverflow.com/questions/6600878/find-all-packages-installed-with-easy-install-pip)

Is there a way to find all Python PyPI packages that were installed with easy_install or pip?  I mean, excluding everything that was/is installed with the distributions tools (in this case apt-get on Debian).

2011-07-06 18:00:02Z

Is there a way to find all Python PyPI packages that were installed with easy_install or pip?  I mean, excluding everything that was/is installed with the distributions tools (in this case apt-get on Debian).pip freeze will output a list of installed packages and their versions. It also allows you to write those packages to a file that can later be used to set up a new environment.https://pip.pypa.io/en/stable/reference/pip_freeze/#pip-freezeAs of version 1.3 of pip you can now use pip listIt has some useful options including the ability to show outdated packages. Here's the documentation: https://pip.pypa.io/en/latest/reference/pip_list/If anyone is wondering you can use the 'pip show' command.This will list the install directory of the given package.If Debian behaves like recent Ubuntu versions regarding pip install default target, it's dead easy: it installs to /usr/local/lib/ instead of /usr/lib (apt default target). Check https://askubuntu.com/questions/173323/how-do-i-detect-and-remove-python-packages-installed-via-pip/259747#259747I am an ArchLinux user and as I experimented with pip I met this same problem. Here's how I solved it in Arch.Key here is /usr/lib/python2.7/site-packages, which is the directory pip installs to, YMMV. pacman -Qo is how Arch's pac kage man ager checks for ownership of the file. No package is part of the return it gives when no package owns the file: error: No package owns $FILENAME. Tricky workaround: I'm querying about __init__.py because pacman -Qo is a little bit ignorant when it comes to directories :(In order to do it for other distros, you have to find out where pip installs stuff (just sudo pip install something), how to query ownership of a file (Debian/Ubuntu method is dpkg -S) and what is the "no package owns that path" return (Debian/Ubuntu is no path found matching pattern). Debian/Ubuntu users, beware: dpkg -S will fail if you give it a symbolic link. Just resolve it first by using realpath. Like this:Fedora users can try (thanks @eddygeek):Start with:To list all packages. Once you found the package you want, use:This will show you details about this package, including its folder. You can skip the first part if you already know the package nameClick here for more information on pip show and here for more information on pip list.Example:pip.get_installed_distributions() will give a list of installed packagesThe below is a little slow, but it gives a nicely formatted list of packages that pip is aware of.  That is to say, not all of them were installed "by" pip, but all of them should be able to be upgraded by pip.The reason it is slow is that it lists the contents of the entire pypi repo.  I filed a ticket suggesting pip list provide similar functionality but more efficiently. Sample output: (restricted the search to a subset instead of '.' for all.)Take note that if you have multiple versions of Python installed on your computer, you may have a few versions of pip associated with each. Depending on your associations, you might need to be very cautious of what pip command you use: Worked for me, where I'm running Python3.4. Simply using pip list returned the error The program 'pip' is currently not installed. You can install it by typing: sudo apt-get install python-pip. Newer versions of pip have the ability to do what the OP wants via 

    pip list -l or pip freeze -l.

On Debian (at least) the man page doesn't make this clear, and I only discovered it - under the assumption that the feature must exist - with pip list --help.  There are recent comments that suggest this feature is not obvious in either the documentation or the existing answers (although hinted at by some), so I thought I should post. I would have preferred to do so as a comment, but I don't have the reputation points. Adding to @Paul Woolcock's answer,will create a requirements file with all installed packages in the  active environment at the current location which you can runto install the requirements at another environment.Here is the one-liner for fedora or other rpm distros (based on @barraponto tips):Append this to the previous command to get cleaner output:As @almenon pointed out, this no longer works and it is not the supported way to get package information in your code. The following raises an exception:To accomplish this, you can import pkg_resources. Here's an example:I'm on v3.6.5Get all file/folder names in site-packages/ (and dist-packages/ if it exists), and use your package manager to strip the ones that were installed via package.pip freeze lists all installed packages even if not by pip/easy_install.

On CentOs/Redhat a package installed through rpm is found.If you use the Anaconda python distribution, you can use the conda list command to see what was installed by what method:To grab the entries installed by pip (including possibly pip itself):Of course you probably want to just select the first column, which you can do with (excluding pip if needed):Finally you can grab these values and pip uninstall all of them using the following:Note the use of the -y flag for the pip uninstall to avoid having to give confirmation to delete.For those who don't have pip installed, I found this quick script on github (works with Python 2.7.13):  pip list [options]

You can see the complete reference hereAt least for Ubuntu (maybe also others) works this (inspired by a previous post in this thread):

Should import statements always be at the top of a module?

Adam J. Forster

[Should import statements always be at the top of a module?](https://stackoverflow.com/questions/128478/should-import-statements-always-be-at-the-top-of-a-module)

PEP 08 states:However if the class/method/function that I am importing is only used in rare cases, surely it is more efficient to do the import when it is needed?Isn't this:more efficient than this?

2008-09-24 17:21:47Z

PEP 08 states:However if the class/method/function that I am importing is only used in rare cases, surely it is more efficient to do the import when it is needed?Isn't this:more efficient than this?Module importing is quite fast, but not instant. This means that:So if you care about efficiency, put the imports at the top. Only move them into a function if your profiling shows that would help (you did profile to see where best to improve performance, right??)The best reasons I've seen to perform lazy imports are:Putting the import statement inside of a function can prevent circular dependencies.

For example, if you have 2 modules, X.py and Y.py, and they both need to import each other, this will cause a circular dependency when you import one of the modules causing an infinite loop. If you move the import statement in one of the modules then it won't try to import the other module till the function is called, and that module will already be imported, so no infinite loop. Read here for more - effbot.org/zone/import-confusion.htmI have adopted the practice of putting all imports in the functions that use them, rather than at the top of the module.The benefit I get is the ability to refactor more reliably. When I move a function from one module to another, I know that the function will continue to work with all of its legacy of testing intact. If I have my imports at the top of the module, when I move a function, I find that I end up spending a lot of time getting the new module's imports complete and minimal. A refactoring IDE might make this irrelevant.There is a speed penalty as mentioned elsewhere. I have measured this in my application and found it to be insignificant for my purposes. It is also nice to be able to see all module dependencies up front without resorting to search (e.g. grep). However, the reason I care about module dependencies is generally because I'm installing, refactoring, or moving an entire system comprising multiple files, not just a single module. In that case, I'm going to perform a global search anyway to make sure I have the system-level dependencies. So I have not found global imports to aid my understanding of a system in practice.I usually put the import of sys inside the if __name__=='__main__' check and then pass arguments (like sys.argv[1:]) to a main() function. This allows me to use main in a context where sys has not been imported.Most of the time this would be useful for clarity and sensible to do but it's not always the case.  Below are a couple of examples of circumstances where module imports might live elsewhere.Firstly, you could have a module with a unit test of the form:Secondly, you might have a requirement to conditionally import some different module at runtime.There are probably other situations where you might place imports in other parts in the code.The first variant is indeed more efficient than the second when the function is called either zero or one times.  With the second and subsequent invocations, however, the "import every call" approach is actually less efficient.  See this link for a lazy-loading technique that combines the best of both approaches by doing a "lazy import".But there are reasons other than efficiency why you might prefer one over the other.  One approach is makes it much more clear to someone reading the code as to the dependencies that this module has.  They also have very different failure characteristics -- the first will fail at load time if there's no "datetime" module while the second won't fail until the method is called.Added Note: In IronPython, imports can be quite a bit more expensive than in CPython because the code is basically being compiled as it's being imported.Curt makes a good point: the second version is clearer and will fail at load time rather than later, and unexpectedly.Normally I don't worry about the efficiency of loading modules, since it's (a) pretty fast, and (b) mostly only happens at startup.If you have to load heavyweight modules at unexpected times, it probably makes more sense to load them dynamically with the __import__ function, and be sure to catch ImportError exceptions, and handle them in a reasonable manner.I wouldn't worry about the efficiency of loading the module up front too much.  The memory taken up by the module won't be very big (assuming it's modular enough) and the startup cost will be negligible.In most cases you want to load the modules at the top of the source file.  For somebody reading your code, it makes it much easier to tell what function or object came from what module.One good reason to import a module elsewhere in the code is if it's used in a debugging statement.For example:I could debug this with:Of course, the other reason to import modules elsewhere in the code is if you need to dynamically import them.  This is because you pretty much don't have any choice.I wouldn't worry about the efficiency of loading the module up front too much.  The memory taken up by the module won't be very big (assuming it's modular enough) and the startup cost will be negligible.It's a tradeoff, that only the programmer can decide to make. Case 1 saves some memory and startup time by not importing the datetime module (and doing whatever initialization it might require) until needed.  Note that doing the import 'only when called' also means doing it 'every time when called', so each call after the first one is still incurring the additional overhead of doing the import. Case 2 save some execution time and latency by importing datetime beforehand so that not_often_called() will return more quickly when it is called, and also by not incurring the overhead of an import on every call.Besides efficiency, it's easier to see module dependencies up front if the import statements are ... up front. Hiding them down in the code can make it more difficult to easily find what modules something depends on.Personally I generally follow the PEP except for things like unit tests and such that I don't want always loaded because I know they aren't going to be used except for test code.Here's an example where all the imports are at the very top (this is the only time I've needed to do this).  I want to be able to terminate a subprocess on both Un*x and Windows.(On review: what John Millikin said.)This is like many other optimizations - you sacrifice some readability for speed.  As John mentioned, if you've done your profiling homework and found this to be a significantly useful enough change and you need the extra speed, then go for it.  It'd probably be good to put a note up with all the other imports:Module initialization only occurs once - on the first import.  If the module in question is from the standard library, then you will likely import it from other modules in your program as well.  For a module as prevalent as datetime, it is also likely a dependency for a slew of other standard libraries.  The import statement would cost very little then since the module intialization would have happened already.  All it is doing at this point is binding the existing module object to the local scope.Couple that information with the argument for readability and I would say that it is best to have the import statement at module scope.  Just to complete Moe's answer and the original question:When we have to deal with circular dependences we can do some "tricks". Assuming we're working with modules a.py and b.py that contain x() and b y(), respectively. Then:So, to conclude. If you aren't dealing with circular dependencies and doing some kind of trick to avoid them, then it's better to put all your imports at the top because of the reasons already explained in other answers to this question. And please, when doing this "tricks" include a comment, it's always welcome! :)In addition to the excellent answers already given, it's worth noting that the placement of imports is not merely a matter of style. Sometimes a module has implicit dependencies that need to be imported or initialized first, and a top-level import could lead to violations of the required order of execution. This issue often comes up in Apache Spark's Python API, where you need to initialize the SparkContext before importing any pyspark packages or modules. It's best to place pyspark imports in a scope where the SparkContext is guaranteed to be available.I do not aspire to provide complete answer, because others have already done this very well. I just want to mention one use case when I find especially useful to import modules inside functions. My application uses python packages and modules stored in certain location as plugins. During application startup, the application walks through all the modules in the location and imports them, then it looks inside the modules and if it finds some mounting points for the plugins (in my case it is a subclass of a certain base class having a unique ID) it registers them. The number of plugins is large (now dozens, but maybe hundreds in the future) and each of them is used quite rarely. Having imports of third party libraries at the top of my plugin modules was a bit penalty during application startup. Especially some thirdparty libraries are heavy to import (e.g. import of plotly even tries to connect to internet and download something which was adding about one second to startup). By optimizing imports (calling them only in the functions where they are used) in the plugins I managed to shrink the startup from 10 seconds to some 2 seconds. That is a big difference for my users.So my answer is no, do not always put the imports at the top of your modules.I was surprised not to see actual cost numbers for the repeated load-checks posted already, although there are many good explanations of what to expect.If you import at the top, you take the load hit no matter what. That's pretty small, but commonly in the milliseconds, not nanoseconds.If you import within a function(s), then you only take the hit for loading if and when one of those functions is first called. As many have pointed out, if that doesn't happen at all, you save the load time. But if the function(s) get called a lot, you take a repeated though much smaller hit (for checking that it has been loaded; not for actually re-loading). On the other hand, as @aaronasterling pointed out you also save a little because importing within a function lets the function use slightly-faster local variable lookups to identify the name later (http://stackoverflow.com/questions/477096/python-import-coding-style/4789963#4789963).Here are the results of a simple test that imports a few things from inside a function. The times reported (in Python 2.7.14 on a 2.3 GHz Intel Core i7) are shown below (the 2nd call taking more than later calls seems consistent, though I don't know why).The code:It's interesting that not a single answer mentioned parallel processing so far, where it might be REQUIRED that the imports are in the function, when the serialized function code is what is being pushed around to other cores, e.g. like in the case of ipyparallel.There can be a performance gain by importing variables/local scoping inside of a function. This depends on the usage of the imported thing inside the function. If you are looping many times and accessing a module global object, importing it as local can help. A time on Linux shows a small gainreal is wall clock. user is time in program. sys is time for system calls.https://docs.python.org/3.5/reference/executionmodel.html#resolution-of-namesI would like to mention a usecase of mine, very similar to those mentioned by @John Millikin and @V.K. : I do data analysis with Jupyter Notebook, and I use the same IPython notebook as a template for all analyses. In some occasions, I need to import Tensorflow to do some quick model runs, but sometimes I work in places where tensorflow isn't set up / is slow to import. In those cases, I encapsulate my Tensorflow-dependent operations in a helper function, import tensorflow inside that function, and bind it to a button.This way, I could do "restart-and-run-all" without having to wait for the import, or having to resume the rest of the cells when it fails.This is a fascinating discussion. Like many others I had never even considered this topic. I got cornered into having to have the imports in the functions because of wanting to use the Django ORM in one of my libraries. I was having to call django.setup() before importing my model classes and because this was at the top of the file it was being dragged into completely non-Django library code because of the IoC injector construction.I kind of hacked around a bit and ended up putting the django.setup() in the singleton constructor and the relevant import at the top of each class method. Now this worked fine but made me uneasy because the imports weren't at the top and also I started worrying about the extra time hit of the imports. Then I came here and read with great interest everybody's take on this.I have a long C++ background and now use Python/Cython. My take on this is that why not put the imports in the function unless it causes you a profiled bottleneck. It's only like declaring space for variables just before you need them. The trouble is I have thousands of lines of code with all the imports at the top! So I think I will do it from now on and change the odd file here and there when I'm passing through and have the time.In addition to startup performance, there is a readability argument to be made for localizing import statements. For example take python line numbers 1283 through 1296 in my current first python project:If the import statement was at the top of file I would have to scroll up a long way, or press Home, to find out what ET was. Then I would have to navigate back to line 1283 to continue reading code.Indeed even if the import statement was at the top of the function (or class) as many would place it, paging up and back down would be required.Displaying the Gnome version number will rarely be done so the import at top of file introduces unnecessary startup lag.

What is the maximum recursion depth in Python, and how to increase it?

quantumSoup

[What is the maximum recursion depth in Python, and how to increase it?](https://stackoverflow.com/questions/3323001/what-is-the-maximum-recursion-depth-in-python-and-how-to-increase-it)

I have this tail recursive function here:It works up to n=997, then it just breaks and spits out a RecursionError: maximum recursion depth exceeded in comparison. Is this just a stack overflow? Is there a way to get around it?

2010-07-23 23:04:50Z

I have this tail recursive function here:It works up to n=997, then it just breaks and spits out a RecursionError: maximum recursion depth exceeded in comparison. Is this just a stack overflow? Is there a way to get around it?It is a guard against a stack overflow, yes. Python (or rather, the CPython implementation) doesn't optimize tail recursion, and unbridled recursion causes stack overflows. You can check the recursion limit with sys.getrecursionlimit and change the recursion limit with sys.setrecursionlimit, but doing so is dangerous -- the standard limit is a little conservative, but Python stackframes can be quite big.Python isn't a functional language and tail recursion is not a particularly efficient technique. Rewriting the algorithm iteratively, if possible, is generally a better idea.Looks like you just need to set a higher recursion depth:It's to avoid a stack overflow. The Python interpreter limits the depths of recursion to help you avoid infinite recursions, resulting in stack overflows.

Try increasing the recursion limit (sys.setrecursionlimit) or re-writing your code without recursion.From the Python documentation:If you often need to change the recursion limit (e.g. while solving programming puzzles) you can define a simple context manager like this:Then to call a function with a custom limit you can do:On exit from the body of the with statement the recursion limit will be restored to the default value.Use a language that guarantees tail-call optimisation. Or use iteration. Alternatively, get cute with decorators.resource.setrlimit must also be used to increase the stack size and prevent segfaultThe Linux kernel limits the stack of processes.Python stores local variables on the stack of the interpreter, and so recursion takes up stack space of the interpreter.If the Python interpreter tries to go over the stack limit, the Linux kernel makes it segmentation fault.The stack limit size is controlled with the getrlimit and setrlimit system calls.Python offers access to those system calls through the resource module.Of course, if you keep increasing ulimit, your RAM will run out, which will either slow your computer to a halt due to swap madness, or kill Python via the OOM Killer.From bash, you can see and set the stack limit (in kb) with:The default value for me is 8Mb.See also:Tested on Ubuntu 16.10, Python 2.7.12.I realize this is an old question but for those reading, I would recommend against using recursion for problems such as this - lists are much faster and avoid recursion entirely.  I would implement this as:(Use n+1 in xrange if you start counting your fibonacci sequence from 0 instead of 1.)Of course Fibonacci numbers can be computed in O(n) by applying the Binet formula:As the commenters note it's not O(1) but O(n) because of 2**n. Also a difference is that you only get one value, while with recursion you get all values of Fibonacci(n) up to that value. I had a similar issue with the error "Max recursion depth exceeded". I discovered the error was being triggered by a corrupt file in the directory I was looping over with os.walk. If you have trouble solving this issue and you are working with file paths, be sure to narrow it down, as it might be a corrupt file. Use generators?above fib() function adapted from: http://intermediatepythonista.com/python-generatorsIf you want to get only few Fibonacci numbers, you can use matrix method.It's fast as numpy uses fast exponentiation algorithm. You get answer in O(log n). And it's better than Binet's formula because it uses only integers. But if you want all Fibonacci numbers up to n, then it's better to do it by memorisation.Many recommend that increasing recursion limit is a good solution however it is not because there will be always limit. Instead use an iterative solution. I wanted to give you an example for using memoization to compute Fibonacci as this will allow you to compute significantly larger numbers using recursion:This is still recursive, but uses a simple hashtable that allows the reuse of previously calculated Fibonacci numbers instead of doing them again.As @alex suggested, you could use a generator function to do this sequentially instead of recursively. Here's the equivalent of the code in your question:We can do that using @lru_cache decorator and setrecursionlimit() method:functools lru_cacheWe could also use a variation of dynamic programming bottom up approach

Flattening a shallow list in Python [duplicate]

cdleary

[Flattening a shallow list in Python [duplicate]](https://stackoverflow.com/questions/406121/flattening-a-shallow-list-in-python)

Is there a simple way to flatten a list of iterables with a list comprehension, or failing that, what would you all consider to be the best way to flatten a shallow list like this, balancing performance and readability?I tried to flatten such a list with a nested list comprehension, like this:But I get in trouble of the NameError variety there, because the name 'menuitem' is not defined.  After googling and looking around on Stack Overflow, I got the desired results with a reduce statement:But this method is fairly unreadable because I need that list(x) call there because x is a Django QuerySet object.Conclusion: Thanks to everyone who contributed to this question.  Here is a summary of what I learned.  I'm also making this a community wiki in case others want to add to or correct these observations.My original reduce statement is redundant and is better written this way:This is the correct syntax for a nested list comprehension (Brilliant summary dF!):But neither of these methods are as efficient as using itertools.chain:And as @cdleary notes, it's probably better style to avoid * operator magic by using chain.from_iterable like so:

2009-01-02 06:49:50Z

Is there a simple way to flatten a list of iterables with a list comprehension, or failing that, what would you all consider to be the best way to flatten a shallow list like this, balancing performance and readability?I tried to flatten such a list with a nested list comprehension, like this:But I get in trouble of the NameError variety there, because the name 'menuitem' is not defined.  After googling and looking around on Stack Overflow, I got the desired results with a reduce statement:But this method is fairly unreadable because I need that list(x) call there because x is a Django QuerySet object.Conclusion: Thanks to everyone who contributed to this question.  Here is a summary of what I learned.  I'm also making this a community wiki in case others want to add to or correct these observations.My original reduce statement is redundant and is better written this way:This is the correct syntax for a nested list comprehension (Brilliant summary dF!):But neither of these methods are as efficient as using itertools.chain:And as @cdleary notes, it's probably better style to avoid * operator magic by using chain.from_iterable like so:If you're just looking to iterate over a flattened version of the data structure and don't need an indexable sequence, consider itertools.chain and company.It will work on anything that's iterable, which should include Django's iterable QuerySets, which it appears that you're using in the question.Edit: This is probably as good as a reduce anyway, because reduce will have the same overhead copying the items into the list that's being extended. chain will only incur this (same) overhead if you run list(chain) at the end.Meta-Edit: Actually, it's less overhead than the question's proposed solution, because you throw away the temporary lists you create when you extend the original with the temporary.Edit: As J.F. Sebastian says itertools.chain.from_iterable avoids the unpacking and you should use that to avoid * magic, but the timeit app shows negligible performance difference.You almost have it! The way to do nested list comprehensions is to put the for statements in the same order as they would go in regular nested for statements.Thus, thiscorresponds toSo you want@S.Lott: You inspired me to write a timeit app.I figured it would also vary based on the number of partitions (number of iterators within the container list) -- your comment didn't mention how many partitions there were of the thirty items. This plot is flattening a thousand items in every run, with varying number of partitions. The items are evenly distributed among the partitions.Code (Python 2.6):Edit: Decided to make it community wiki.Note: METHODS should probably be accumulated with a decorator, but I figure it'd be easier for people to read this way.sum(list_of_lists, []) would flatten it.This solution works for arbitrary nesting depths - not just the "list of lists" depth that some (all?) of the other solutions are limited to:It's the recursion which allows for arbitrary depth nesting - until you hit the maximum recursion depth, of course...Performance Results.  Revised.I flattened a 2-level list of 30 items 1000 timesReduce is always a poor choice.In Python 2.6, using chain.from_iterable():It avoids creating of intermediate list.There seems to be a confusion with operator.add! When you add two lists together, the correct term for that is concat, not add. operator.concat is what you need to use.If you're thinking functional, it is as easy as this::You see reduce respects the sequence type, so when you supply a tuple, you get back a tuple. let's try with a list::Aha, you get back a list.How about performance::from_iterable is pretty fast! But it's no comparison to reduce with concat.Off the top of my head, you can eliminate the lambda:Or even eliminate the map, since you've already got a list-comp:You can also just express this as a sum of lists:Here is the correct solution using list comprehensions (they're backward in the question):In your case it would beor you could use join and sayIn either case, the gotcha was the nesting of the for loops.If you have to flat a more complicated list with not iterable elements or with depth more than 2 you can use following function:It will return a generator object which you can convert to a list with list() function. Notice that yield from syntax is available starting from python3.3, but you can use explicit iteration instead.

Example:Here is a version working for multiple levels of list using collectons.Iterable:have you tried flatten?

From matplotlib.cbook.flatten(seq, scalarp=) ?UPDATE

Which gave me another idea:So to test how effective it is when recursive gets deeper: How much deeper?I will bet "flattenlist" I am going to use this rather than matploblib for a long long time unless I want a yield generator and fast result as "flatten" uses in matploblib.cbookThis, is fast. :This version is a generator.Tweak it if you want a list.You can add a predicate ,if want to flatten those which satisfy a conditionTaken from python cookbookFrom my experience, the most efficient way to flatten a list of lists is:Some timeit comparisons with the other proposed methods:Now, the efficiency gain appears better when processing longer sublists:And this methods also works with any iterative object:Test:What about:But, Guido is recommending against performing too much in a single line of code since it reduces readability. There is minimal, if any, performance gain by performing what you want in a single line vs. multiple lines.pylab provides a flatten:

 link to numpy flattenIf you're looking for a built-in, simple, one-liner you can use:returnsIf each item in the list is a string (and any strings inside those strings use " " rather than ' '), you can use regular expressions (re module)The above code converts in_list into a string, uses the regex to find all the substrings within quotes (i.e. each item of the list) and spits them out as a list.A simple alternative is to use numpy's concatenate but it converts the contents to float:The easiest way to achieve this in either Python 2 or 3 is to use the morph library using pip install morph.The code is:In Python 3.4 you will be able to do:

How to detect a Christmas Tree? [closed]

karlphillip

[How to detect a Christmas Tree? [closed]](https://stackoverflow.com/questions/20772893/how-to-detect-a-christmas-tree)

Which image processing techniques could be used to implement an application that detects the Christmas trees displayed in the following images? 

 

I'm searching for solutions that are going to work on all these images. Therefore, approaches that require training haar cascade classifiers or template matching are not very interesting.I'm looking for something that can be written in any programming language, as long as it uses only Open Source technologies. The solution must be tested with the images that are shared on this question. There are 6 input images and the answer should display the results of processing each of them. Finally, for each output image there must be red lines draw to surround the detected tree.How would you go about programmatically detecting the trees in these images?

2013-12-25 12:40:55Z

Which image processing techniques could be used to implement an application that detects the Christmas trees displayed in the following images? 

 

I'm searching for solutions that are going to work on all these images. Therefore, approaches that require training haar cascade classifiers or template matching are not very interesting.I'm looking for something that can be written in any programming language, as long as it uses only Open Source technologies. The solution must be tested with the images that are shared on this question. There are 6 input images and the answer should display the results of processing each of them. Finally, for each output image there must be red lines draw to surround the detected tree.How would you go about programmatically detecting the trees in these images?I have an approach which I think is interesting and a bit different from the rest.  The main difference in my approach, compared to some of the others, is in how the image segmentation step is performed--I used the DBSCAN clustering algorithm from Python's scikit-learn; it's optimized for finding somewhat amorphous shapes that may not necessarily have a single clear centroid.At the top level, my approach is fairly simple and can be broken down into about 3 steps.  First I apply a threshold (or actually, the logical "or" of two separate and distinct thresholds).  As with many of the other answers, I assumed that the Christmas tree would be one of the brighter objects in the scene, so the first threshold is just a simple monochrome brightness test; any pixels with values above 220 on a 0-255 scale (where black is 0 and white is 255) are saved to a binary black-and-white image.  The second threshold tries to look for red and yellow lights, which are particularly prominent in the trees in the upper left and lower right of the six images, and stand out well against the blue-green background which is prevalent in most of the photos.  I convert the rgb image to hsv space, and require that the hue is either less than 0.2 on a 0.0-1.0 scale (corresponding roughly to the border between yellow and green) or greater than 0.95 (corresponding to the border between purple and red) and additionally I require bright, saturated colors: saturation and value must both be above 0.7.  The results of the two threshold procedures are logically "or"-ed together, and the resulting matrix of black-and-white binary images is shown below:You can clearly see that each image has one large cluster of pixels roughly corresponding to the location of each tree, plus a few of the images also have some other small clusters corresponding either to lights in the windows of some of the buildings, or to a background scene on the horizon.  The next step is to get the computer to recognize that these are separate clusters, and label each pixel correctly with a cluster membership ID number.For this task I chose DBSCAN.  There is a pretty good visual comparison of how DBSCAN typically behaves, relative to other clustering algorithms, available here.  As I said earlier, it does well with amorphous shapes.  The output of DBSCAN, with each cluster plotted in a different color, is shown here:There are a few things to be aware of when looking at this result.  First is that DBSCAN requires the user to set a "proximity" parameter in order to regulate its behavior, which effectively controls how separated a pair of points must be in order for the algorithm to declare a new separate cluster rather than agglomerating a test point onto an already pre-existing cluster.  I set this value to be 0.04 times the size along the diagonal of each image.  Since the images vary in size from roughly VGA up to about HD 1080, this type of scale-relative definition is critical.Another point worth noting is that the DBSCAN algorithm as it is implemented in scikit-learn has memory limits which are fairly challenging for some of the larger images in this sample.  Therefore, for a few of the larger images, I actually had to "decimate" (i.e., retain only every 3rd or 4th pixel and drop the others) each cluster in order to stay within this limit.  As a result of this culling process, the remaining individual sparse pixels are difficult to see on some of the larger images.  Therefore, for display purposes only, the color-coded pixels in the above images have been effectively "dilated" just slightly so that they stand out better.  It's purely a cosmetic operation for the sake of the narrative; although there are comments mentioning this dilation in my code, rest assured that it has nothing to do with any calculations that actually matter.Once the clusters are identified and labeled, the third and final step is easy: I simply take the largest cluster in each image (in this case, I chose to measure "size" in terms of the total number of member pixels, although one could have just as easily instead used some type of metric that gauges physical extent) and compute the convex hull for that cluster.  The convex hull then becomes the tree border.  The six convex hulls computed via this method are shown below in red:The source code is written for Python 2.7.6 and it depends on numpy, scipy, matplotlib and scikit-learn.  I've divided it into two parts.  The first part is responsible for the actual image processing:and the second part is a user-level script which calls the first file and generates all of the plots above:EDIT NOTE: I edited this post to (i) process each tree image individually, as requested in the requirements, (ii) to consider both object brightness and shape in order to improve the quality of the result. Below is presented an approach that takes in consideration the object brightness and shape. In other words, it seeks for objects with triangle-like shape and with significant brightness. It was implemented in Java, using Marvin image processing framework.The first step is the color thresholding. The objective here is to focus the analysis on objects with significant brightness. output images:

source code:In the second step, the brightest points in the image are dilated in order to form shapes. The result of this process is the probable shape of the objects with significant brightness. Applying flood fill segmentation, disconnected shapes are detected.output images:

source code:As shown in the output image, multiple shapes was detected. In this problem, there a just a few bright points in the images. However, this approach was implemented to deal with more complex scenarios. In the next step each shape is analyzed. A simple algorithm detects shapes with a pattern similar to a triangle. The algorithm analyze the object shape line by line. If the center of the mass of each shape line is almost the same (given a threshold) and mass increase as y increase, the object has a triangle-like shape. The mass of the shape line is the number of pixels in that line that belongs to the shape. Imagine you slice the object horizontally and analyze each horizontal segment. If they are centralized to each other and the length increase from the first segment to last one in a linear pattern, you probably has an object that resembles a triangle.source code:Finally, the position of each shape similar to a triangle and with significant brightness, in this case a Christmas tree, is highlighted in the original image, as shown below.final output images:

final source code:The advantage of this approach is the fact it will probably work with images containing other luminous objects since it analyzes the object shape.Merry Christmas!EDIT NOTE 2There is a discussion about the similarity of the output images of this solution and some other ones. In fact, they are very similar. But this approach does not just segment objects. It also analyzes the object shapes in some sense. It can handle multiple luminous objects in the same scene. In fact, the Christmas tree does not need to be the brightest one. I'm just abording it to enrich the discussion. There is a bias in the samples that just looking for the brightest object, you will find the trees. But, does we really want to stop the discussion at this point? At this point, how far the computer is really recognizing an object that resembles a Christmas tree? Let's try to close this gap. Below is presented a result just to elucidate this point:input imageoutputHere is my simple and dumb solution.

It is based upon the assumption that the tree will be the most bright and big thing in the picture.The first step is to detect the most bright pixels in the picture, but we have to do a distinction between the tree itself and the snow which reflect its light. Here we try to exclude the snow appling a really simple filter on the color codes:Then we find every "bright" pixel:Finally we join the two results:Now we look for the biggest bright object:Now we have almost done, but there are still some imperfection due to the snow.

To cut them off we'll build a mask using a circle and a rectangle to approximate the shape of a tree to delete unwanted pieces:The last step is to find the contour of our tree and draw it on the original picture.I'm sorry but at the moment I have a bad connection so it is not possible for me to upload pictures. I'll try to do it later.Merry Christmas.EDIT:Here some pictures of the final output:

I wrote the code in Matlab R2007a. I used k-means to roughly extract the christmas tree. I 

will show my intermediate result only with one image, and final results with all the six.First, I mapped the RGB space onto Lab space, which could enhance the contrast of red in its b channel:Besides the feature in color space, I also used texture feature that is relevant with the 

neighborhood rather than each pixel itself. Here I linearly combined the intensity from the 

3 original channels (R,G,B). The reason why I formatted this way is because the christmas 

trees in the picture all have red lights on them, and sometimes green/sometimes blue 

illumination as well. I applied a 3X3 local binary pattern on I0, used the center pixel as the threshold, and 

obtained the contrast by calculating the difference between the mean pixel intensity value 

above the threshold and the mean value below it.Since I have 4 features in total, I would choose K=5 in my clustering method. The code for 

k-means are shown below (it is from Dr. Andrew Ng's machine learning course. I took the 

course before, and I wrote the code myself in his programming assignment). 

  

   

   

     Since the program runs very slow in my computer, I just ran 3 iterations. Normally the stop 

criteria is (i) iteration time at least 10, or (ii) no change on the centroids any more. To 

my test, increasing the iteration may differentiate the background (sky and tree, sky and 

building,...) more accurately, but did not show a drastic changes in christmas tree 

extraction. Also note k-means is not immune to the random centroid initialization, so running the program several times to make a comparison is recommended. After the k-means, the labelled region with the maximum intensity of I0 was chosen. And 

boundary tracing was used to extracted the boundaries. To me, the last christmas tree is the most difficult one to extract since the contrast in  that picture is not high enough as they are in the first five. Another issue in my method  is that I used bwboundaries function in Matlab to trace the boundary, but sometimes the  inner boundaries are also included as you can observe in 3rd, 5th, 6th results. The dark  side within the christmas trees are not only failed to be clustered with the illuminated side, but they also lead to so many tiny inner boundaries tracing (imfill doesn't improve very much). In all my algorithm still has a lot improvement space.  

  

  

  Some publications indicates that mean-shift may be more robust than k-means, and many 

graph-cut based algorithms are also very competitive on complicated boundaries 

segmentation. I wrote a mean-shift algorithm myself, it seems to better extract the regions 

without enough light. But mean-shift is a little bit over-segmented, and some strategy of 

merging is needed. It ran even much slower than k-means in my computer, I am afraid I have 

to give it up. I eagerly look forward to see others would submit excellent results here 

with those modern algorithms mentioned above. Yet I always believe the feature selection is the key component in image segmentation. With 

a proper feature selection that can maximize the margin between object and background, many 

segmentation algorithms will definitely work. Different algorithms may improve the result 

from 1 to 10, but the feature selection may improve it from 0 to 1.Merry Christmas !This is my final post using the traditional image processing approaches...Here I somehow combine my two other proposals, achieving even better results. As a matter of fact I cannot see how these results could be better (especially when you look at the masked images that the method produces).At the heart of the approach is the combination of three key assumptions:With these assumptions in mind the method works as follows:Here is the code in MATLAB (again, the script loads all jpg images in the current folder and, again, this is far from being an optimized piece of code):High resolution results still available here!

Even more experiments with additional images can be found here.My solution steps:Step by step:

The first result - most simple but not in open source software - "Adaptive Vision Studio + Adaptive Vision Library":

This is not open source but really fast to prototype:Whole algorithm to detect christmas tree (11 blocks):

Next step. We want open source solution. Change AVL filters to OpenCV filters:

Here I did little changes e.g. Edge Detection use cvCanny filter, to respect roi i did multiply region image with edges image, to select the biggest element i used findContours + contourArea but idea is the same.https://www.youtube.com/watch?v=sfjB3MigLH0&index=1&list=UUpSRrkMHNHiLDXgylwhWNQQI can't show images with intermediate steps now because I can put only 2 links.Ok now we use openSource filters but it's not still whole open source. 

Last step - port to c++ code. I used OpenCV in version 2.4.4The result of final c++ code is:

c++ code is also quite short:...another old fashioned solution - purely based on HSV processing:A word on the heuristics in the HSV processing:Of course one may experiment with numerous other possibilities to fine-tune this approach...Here is the MATLAB code to do the trick (warning: the code is far from being optimized!!! I used techniques not recommended for MATLAB programming just to be able to track anything in the process-this can be greatly optimized):In the results I show the masked image and the bounding box.

Some old-fashioned image processing approach...

The idea is based on the assumption that images depict lighted trees on typically darker and smoother backgrounds (or foregrounds in some cases). The lighted tree area is more "energetic" and has higher intensity. 

The process is as follows:What you get is a binary mask and a bounding box for each image.Here are the results using this naive technique:

Code on MATLAB follows:

The code runs on a folder with JPG images. Loads all images and returns detected results.Using a quite different approach from what I've seen, I created a php script that detects christmas trees by their lights. The result ist always a symmetrical triangle, and if necessary numeric values like the angle ("fatness") of the tree.The biggest threat to this algorithm obviously are lights next to (in great numbers) or in front of the tree (the greater problem until further optimization).

Edit (added): What it can't do: Find out if there's a christmas tree or not, find multiple christmas trees in one image, correctly detect a cristmas tree in the middle of Las Vegas, detect christmas trees that are heavily bent, upside-down or chopped down... ;)The different stages are:Explanation of the markings:Source code:Images:

Bonus: A german Weihnachtsbaum, from Wikipedia

http://commons.wikimedia.org/wiki/File:Weihnachtsbaum_R%C3%B6merberg.jpgI used python with opencv.My algorithm goes like this:The code:If I change the kernel from (25,5) to (10,5)

I get nicer results on all trees but the bottom left,

my algorithm assumes that the tree has lights on it, and

in the bottom left tree, the top has less light then the others.

Python module for converting PDF to text [closed]

cnu

[Python module for converting PDF to text [closed]](https://stackoverflow.com/questions/25665/python-module-for-converting-pdf-to-text)

Which are the best Python modules to convert PDF files into text? 

2008-08-25 04:44:06Z

Which are the best Python modules to convert PDF files into text? Try PDFMiner. It can extract text from PDF files as HTML, SGML or "Tagged PDF" format.The Tagged PDF format seems to be the cleanest, and stripping out the XML tags leaves just the bare text.A Python 3 version is available under:The PDFMiner package has changed since codeape posted.  EDIT (again):PDFMiner has been updated again in version 20100213You can check the version you have installed with the following:Here's the updated version (with comments on what I changed/added):Edit (yet again):Here is an update for the latest version in pypi, 20100619p1.  In short I replaced LTTextItem with LTChar and passed an instance of LAParams to the CsvConverter constructor.EDIT (one more time):Updated for version 20110515 (thanks to Oeufcoque Penteano!):Since none for these solutions support the latest version of PDFMiner I wrote a simple solution that will return text of a pdf using PDFMiner. This will work for those who are getting import errors with process_pdf See below code that works for Python 3:Pdftotext An open source program (part of Xpdf) which you could call from python (not what you asked for but might be useful). I've used it with no problems. I think google use it in google desktop.pyPDF works fine (assuming that you're working with well-formed PDFs).  If all you want is the text (with spaces), you can just do:You can also easily get access to the metadata, image data, and so forth.A comment in the extractText code notes:Whether or not this is a problem depends on what you're doing with the text (e.g. if the order doesn't matter, it's fine, or if the generator adds text to the stream in the order it will be displayed, it's fine).  I have pyPdf extraction code in daily use, without any problems.You can also quite easily use pdfminer as a library. You have access to the pdf's content model, and can create your own text extraction. I did this to convert pdf contents to semi-colon separated text, using the code below.The function simply sorts the TextItem content objects according to their y and x coordinates, and outputs items with the same y coordinate as one text line, separating the objects on the same line with ';' characters.Using this approach, I was able to extract text from a pdf that no other tool was able to extract content suitable for further parsing from. Other tools I tried include pdftotext, ps2ascii and the online tool pdftextonline.com.pdfminer is an invaluable tool for pdf-scraping.UPDATE:The code above is written against an old version of the API, see my comment below.slate is a project that makes it very simple to use PDFMiner from a library:I needed to convert a specific PDF to plain text within a python module. I used PDFMiner 20110515, after reading through their pdf2txt.py tool I wrote this simple snippet:Repurposing the pdf2txt.py code that comes with pdfminer; you can make a function that will take a path to the pdf; optionally, an outtype (txt|html|xml|tag) and opts like the commandline pdf2txt {'-o': '/path/to/outfile.txt' ...}.  By default, you can call:A text file will be created, a sibling on the filesystem to the original pdf.PDFminer gave me perhaps one line [page 1 of 7...] on every page of a pdf file I tried with it.The best answer I have so far is pdftoipe, or the c++ code it's based on Xpdf.see my question for what the output of pdftoipe looks like.Additionally there is PDFTextStream which is a commercial Java library that can also be used from Python.I have used pdftohtml with the -xml argument, read the result with subprocess.Popen(), that will give you x coord, y coord, width, height, and font, of every snippet of text in the pdf. I think this is what 'evince' probably uses too because the same error messages spew out. If you need to process columnar data, it gets slightly more complicated as you have to invent an algorithm that suits your pdf file. The problem is that the programs that make PDF files don't really necessarily lay out the text in any logical format. You can try simple sorting algorithms and it works sometimes, but there can be little 'stragglers' and 'strays', pieces of text that don't get put in the order you thought they would. So you have to get creative. It took me about 5 hours to figure out one for the pdf's I was working on. But it works pretty good now. Good luck. Found that solution today. Works great for me. Even rendering PDF pages to PNG images.

http://www.swftools.org/gfx_tutorial.html

Is there a「not equal」operator in Python?

Aj Entity

[Is there a「not equal」operator in Python?](https://stackoverflow.com/questions/11060506/is-there-a-not-equal-operator-in-python)

How would you say does not equal?Like Is there something equivalent to == that means "not equal"?

2012-06-16 03:19:57Z

How would you say does not equal?Like Is there something equivalent to == that means "not equal"?Use !=. See comparison operators. For comparing object identities, you can use the keyword is and its negation is not.e.g.Not equal  !=  (vs equal ==)Are you asking about something like this?This Python - Basic Operators chart might be helpful.There's the != (not equal) operator that returns True when two values differ, though be careful with the types because "1" != 1. This will always return True and "1" == 1 will always return False, since the types differ. Python is dynamically, but strongly typed, and other statically typed languages would complain about comparing different types.There's also the else clause:The is operator is the object identity operator used to check if two objects in fact are the same:You can use both != or <>.However, note that != is preferred where <> is deprecated.Seeing as everyone else has already listed most of the other ways to say not equal I will just add:in this case it is simple switching the check of positive == (true) to negative and vise versa...There are two operators in Python for the "not equal" condition -a.) != If values of the two operands are not equal, then the condition becomes true.

    (a != b) is true.b.) <> If values of the two operands are not equal, then the condition becomes true.

    (a <> b) is true. This is similar to the != operator.You can use "is not" for "not equal" or "!=". Please see the example below:The above code will print "true" as a = 2 assigned before the "if" condition. Now please see the code below for "not equal"The above code will print "not equal" as a = 2 as assigned earlier. Use != or <>. Both stands for not equal.The comparison operators <> and != are alternate spellings of the same operator. != is the preferred spelling; <> is obsolescent. [Reference: Python language reference]You can simply do:

Correct way to try/except using Python requests module?

John Smith

[Correct way to try/except using Python requests module?](https://stackoverflow.com/questions/16511337/correct-way-to-try-except-using-python-requests-module)

Is this correct? Is there a better way to structure this? Will this cover all my bases? 

2013-05-12 19:44:44Z

Is this correct? Is there a better way to structure this? Will this cover all my bases? Have a look at the Requests exception docs.  In short:To answer your question, what you show will not cover all of your bases. You'll only catch connection-related errors, not ones that time out.What to do when you catch the exception is really up to the design of your script/program. Is it acceptable to exit? Can you go on and try again?  If the error is catastrophic and you can't go on, then yes, a call to sys.exit() is in order.You can either catch the base-class exception, which will handle all cases:Or you can catch them separately and do different things.As Christian pointed out:An example:Will print:One additional suggestion to be explicit. It seems best to go from specific to general down the stack of errors to get the desired error to be caught, so the specific ones don't get masked by the general one.vsException object also contains original response e.response, that could be useful if need to see error body in response from the server. For example:

How do you get a query string on Flask?

Tampa

[How do you get a query string on Flask?](https://stackoverflow.com/questions/11774265/how-do-you-get-a-query-string-on-flask)

Not obvious from the flask documention on how to get the query string. I am new, looked at the docs, could not find!   So

2012-08-02 09:04:40Z

Not obvious from the flask documention on how to get the query string. I am new, looked at the docs, could not find!   SoThe full URL is available as request.url, and the query string is available as request.query_string.Here's an example:To access an individual known param passed in the query string, you can use request.args.get('param'). This is the "right" way to do it, as far as I know.ETA: Before you go further, you should ask yourself why you want the query string. I've never had to pull in the raw string - Flask has mechanisms for accessing it in an abstracted way. You should use those unless you have a compelling reason not to.Werkzeug/Flask as already parsed everything for you. No need to do the same work again with urlparse:The full documentation for the request and response objects is in Werkzeug: http://werkzeug.pocoo.org/docs/wrappers/We can do this by using request.query_string.Example:Lets consider view.pyYou also make it more modular by using Flask Blueprints - http://flask.pocoo.org/docs/0.10/blueprints/Lets consider first name is being passed as a part of query string

/web_url/?first_name=johnAs you see this is just a small example - you can fetch multiple values + formate those and use it or pass it onto the template file.I came here looking for the query string, not how to get values from the query string.request.query_string returns the URL parameters as raw byte string (Ref 1). Example of using request.query_string:Output:References:Try like this for query string:Output:

This can be done using request.args.get().

For example if your query string has a field date, it can be accessed using Don't forget to add "request" to list of imports from flask,

i.e.If the request if GET and we passed some query parameters then, 

Rename a dictionary key

rabin utam

[Rename a dictionary key](https://stackoverflow.com/questions/16475384/rename-a-dictionary-key)

Is there a way to rename a dictionary key, without reassigning its value to a new name and removing the old name key; and without iterating through dict key/value?

In case of OrderedDict, do the same, while keeping that key's position.

2013-05-10 04:52:30Z

Is there a way to rename a dictionary key, without reassigning its value to a new name and removing the old name key; and without iterating through dict key/value?

In case of OrderedDict, do the same, while keeping that key's position.For a regular dict, you can use:For an OrderedDict, I think you must build an entirely new one using a comprehension.  Modifying the key itself, as this question seems to be asking, is impractical because dict keys are usually immutable objects such as numbers, strings or tuples.  Instead of trying to modify the key, reassigning the value to a new key and removing the old key is how you can achieve the "rename" in python.  best method in 1 line:Using a check for newkey!=oldkey, this way you can do:You can use this OrderedDict recipe written by Raymond Hettinger and modify it to add a rename method, but this is going to be a O(N) in complexity:Example:output:In case  of  renaming  all dictionary  keys:A few people before me mentioned the .pop trick to delete and create a key in a one-liner.I personally find the more explicit implementation more readable:The code above returns {'a': 1, 'c': 2}Other answers are pretty good.But in python3.6, regular dict also has order. So it's hard to keep key's position in normal case.In Python 3.6 (onwards?) I would go for the following one-linerwhich producesMay be worth noting that without the print statement the ipython console/jupyter notebook present the dictionary in an order of their choosing...In case someone wants to rename all the keys at once providing a list with the new names:I am using @wim 's answer above, with dict.pop() when renaming keys, but I found a gotcha.  Cycling through the dict to change the keys, without separating the list of old keys completely from the dict instance, resulted in cycling new, changed keys into the loop, and missing some existing keys. To start with, I did it this way:I found that cycling through the dict in this way, the dictionary kept finding keys even when it shouldn't, i.e., the new keys, the ones I had changed!  I needed to separate the instances completely from each other to (a) avoid finding my own changed keys in the for loop, and (b) find some keys that were not being found within the loop for some reason.I am doing this now:Converting the my_dict.keys() to a list was necessary to get free of the reference to the changing dict.  Just using my_dict.keys() kept me tied to the original instance, with the strange side effects.@helloswift123 I like your function. Here is a modification to rename multiple keys in a single call:  Suppose you want to rename key k3 to k4:

Download large file in python with requests

Roman Podlinov

[Download large file in python with requests](https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests)

Requests is a really nice library. I'd like to use it for download big files (>1GB).

The problem is it's not possible to keep whole file in memory I need to read it in chunks. And this is a problem with the following codeBy some reason it doesn't work this way. It still loads response into memory before save it to a file.UPDATEIf you need a small client (Python 2.x /3.x) which can download big files from FTP, you can find it here. It supports multithreading & reconnects (it does monitor connections) also it tunes socket params for the download task. 

2013-05-22 14:47:37Z

Requests is a really nice library. I'd like to use it for download big files (>1GB).

The problem is it's not possible to keep whole file in memory I need to read it in chunks. And this is a problem with the following codeBy some reason it doesn't work this way. It still loads response into memory before save it to a file.UPDATEIf you need a small client (Python 2.x /3.x) which can download big files from FTP, you can find it here. It supports multithreading & reconnects (it does monitor connections) also it tunes socket params for the download task. With the following streaming code, the Python memory usage is restricted regardless of the size of the downloaded file:Note that the number of bytes returned using iter_content is not exactly the chunk_size; it's expected to be a random number that is often far bigger, and is expected to be different in every iteration.See http://docs.python-requests.org/en/latest/user/advanced/#body-content-workflow for further reference.It's much easier if you use Response.raw and shutil.copyfileobj():This streams the file to disk without using excessive memory, and the code is simple.Not exactly what OP was asking, but... it's ridiculously easy to do that with urllib:Or this way, if you want to save it to a temporary file:I watched the process:And I saw the file growing, but memory usage stayed at 17 MB. Am I missing something?Your chunk size could be too large, have you tried dropping that - maybe 1024 bytes at a time? (also, you could use with to tidy up the syntax)Incidentally, how are you deducing that the response has been loaded into memory?It sounds as if python isn't flushing the data to file, from other SO questions you could try f.flush() and os.fsync() to force the file write and free memory;

Python: Checking if a 'Dictionary' is empty doesn't seem to work

Unsparing

[Python: Checking if a 'Dictionary' is empty doesn't seem to work](https://stackoverflow.com/questions/23177439/python-checking-if-a-dictionary-is-empty-doesnt-seem-to-work)

I am trying to check if a dictionary is empty but it doesn't behave properly. It just skips it and displays ONLINE without anything except of display the message. Any ideas why ?

2014-04-20 01:29:19Z

I am trying to check if a dictionary is empty but it doesn't behave properly. It just skips it and displays ONLINE without anything except of display the message. Any ideas why ?Empty dictionaries evaluate to False in Python:Thus, your isEmpty function is unnecessary.  All you need to do is:Here are three ways you can check if dict is empty. I prefer using the first way only though. The other two ways are way too wordy.if length is zero means that dict is emptySimple ways to check an empty dict are below:Although method 1 is more strict when a = None, method 1 will provide a correct result and method 2 will give an incorrect result.A dictionary can be automatically cast to boolean which evaluates to False for empty dictionary and True for non-empty dictionary. If this looks too idiomatic, you can also test len(myDictionary) for zero, or set(myDictionary.keys()) for an empty set, or simply test for equality with {}.  The isEmpty function is not only unnecessary but also your implementation has multiple issues that I can spot prima-facie.myDictionary={0:'zero', '':'Empty string', None:'None value', False:'Boolean False value', ():'Empty tuple'}You can also use get(). Initially I believed it to only check if key existed.What I like with get is that it does not trigger an exception, so it makes it easy to traverse large structures.Why not use equality test?use 'any' 

How do I get the path of the Python script I am running in? [duplicate]

jblocksom

[How do I get the path of the Python script I am running in? [duplicate]](https://stackoverflow.com/questions/595305/how-do-i-get-the-path-of-the-python-script-i-am-running-in)

How do I get the path of a the Python script I am running in? I was doing dirname(sys.argv[0]), however on Mac I only get the filename - not the full path as I do on Windows.No matter where my application is launched from, I want to open files that are relative to my script file(s).

2009-02-27 15:50:00Z

How do I get the path of a the Python script I am running in? I was doing dirname(sys.argv[0]), however on Mac I only get the filename - not the full path as I do on Windows.No matter where my application is launched from, I want to open files that are relative to my script file(s).os.path.realpath(__file__) will give you the path of the current file, resolving any symlinks in the path. This works fine on my mac.7.2 of Dive Into Python: Finding the Path.The accepted solution for this will not work if you are planning to compile your scripts using py2exe.  If you're planning to do so, this is the functional equivalent:Py2exe does not provide an __file__ variable.  For reference:  http://www.py2exe.org/index.cgi/Py2exeEnvironmentIf you have even the relative pathname (in this case it appears to be ./) you can open files relative to your script file(s). I use Perl, but the same general solution can apply: I split the directory into an array of folders, then pop off the last element (the script), then push (or for you, append) on whatever I want, and then join them together again, and BAM! I have a working pathname that points to exactly where I expect it to point, relative or absolute.Of course, there are better solutions, as posted. I just kind of like mine.

UnicodeDecodeError when reading CSV file in Pandas with Python

TravisVOX

[UnicodeDecodeError when reading CSV file in Pandas with Python](https://stackoverflow.com/questions/18171739/unicodedecodeerror-when-reading-csv-file-in-pandas-with-python)

I'm running a program which is processing 30,000 similar files. A random number of them are stopping and producing this error...The source/creation of these files all come from the same place. What's the best way to correct this to proceed with the import?

2013-08-11 12:06:25Z

I'm running a program which is processing 30,000 similar files. A random number of them are stopping and producing this error...The source/creation of these files all come from the same place. What's the best way to correct this to proceed with the import?read_csv takes an encoding option to deal with files in different formats. I mostly use read_csv('file', encoding = "ISO-8859-1"), or alternatively encoding = "utf-8" for reading, and generally utf-8 for to_csv.You can also use one of several alias options like 'latin' instead of 'ISO-8859-1' (see python docs, also for numerous other encodings you may encounter).See relevant Pandas documentation,

python docs examples on csv files, and plenty of related questions here on SO. A good background resource is What every developer should know about unicode and character sets.To detect the encoding (assuming the file contains non-ascii characters), you can use enca (see man page) or file -i (linux) or file -I (osx) (see man page). Simplest of all Solutions:Alternate Solution:Then, you can read your file as usual:and the other different encoding types are:Pandas allows to specify encoding, but does not allow to ignore errors not to automatically replace the offending bytes. So there is no one size fits all method but different ways depending on the actual use case.after executing this code you will find encoding of 'filename.csv' then execute code as followingthere you goIn my case, a file has USC-2 LE BOM encoding, according to Notepad++. 

It is encoding="utf_16_le" for python. Hope, it helps to find an answer a bit faster for someone.Try specifying the engine='python'. 

It worked for me but I'm still trying to figure out why.In my case this worked for python 2.7:And for python 3, only:Struggled with this a while and thought I'd post on this question as it's the first search result.  Adding the encoding="iso-8859-1" tag to pandas read_csv didn't work, nor did any other encoding, kept giving a UnicodeDecodeError. If you're passing a file handle to pd.read_csv(), you need to put the encoding attribute on the file open, not in read_csv. Obvious in hindsight, but a subtle error to track down.I am posting an answer to provide an updated solution and explanation as to why this problem can occur. Say you are getting this data from a database or Excel workbook. If  you have special characters like La Cañada Flintridge city, well unless you are exporting the data using UTF-8 encoding, you're going to introduce errors. La Cañada Flintridge city will become La Ca\xf1ada Flintridge city. If you are using pandas.read_csv without any adjustments to the default parameters, you'll hit the following error Fortunately, there are a few solutions. Option 1, fix the exporting. Be sure to use UTF-8 encoding. Option 2, if fixing the exporting problem is not available to you, and you need to use pandas.read_csv, be sure to include the following paramters, engine='python'. By default, pandas uses engine='C' which is great for reading large clean files, but will crash if anything unexpected comes up. In my experience, setting encoding='utf-8' has never fixed this UnicodeDecodeError. Also, you do not need to use errors_bad_lines, however, that is still an option if you REALLY need it.Option 3: solution is my preferred solution personally. Read the file using vanilla Python.Hope this helps people encountering this issue for the first time.This answer seems to be the catch-all for CSV encoding issues. If you are getting a strange encoding problem with your header like this:Then you have a byte order mark (BOM) character at the beginning of your CSV file. This answer addresses the issue:Python read csv - BOM embedded into the first keyThe solution is to load the CSV with encoding="utf-8-sig":Hopefully this helps someone.I am posting an update to this old thread. I found one solution that worked, but requires opening each file. I opened my csv file in LibreOffice, chose Save As > edit filter settings. In the drop-down menu I chose UTF8 encoding. Then I added encoding="utf-8-sig" to the data = pd.read_csv(r'C:\fullpathtofile\filename.csv', sep = ',', encoding="utf-8-sig").Hope this helps someone.I am using Jupyter-notebook. And in my case, it was showing the file in the wrong format. The 'encoding' option was not working.

So I save the csv in utf-8 format, and it works.Try this:Looks like it will take care of the encoding without explicitly expressing it through argumentCheck the encoding before you pass to pandas. It will slow you down, but... In python 3.7I have trouble opening a CSV file in simplified Chinese downloaded from an online bank, 

I have tried latin1, I have tried iso-8859-1, I have tried cp1252, all to no avail.But pd.read_csv("",encoding ='gbk') simply does the work.

How to avoid Python/Pandas creating an index in a saved csv?

Alexis

[How to avoid Python/Pandas creating an index in a saved csv?](https://stackoverflow.com/questions/20845213/how-to-avoid-python-pandas-creating-an-index-in-a-saved-csv)

I am trying to save a csv to a folder after making some edits to the file. Every time I use pd.to_csv('C:/Path of file.csv') the csv file has a separate column of indexes. I want to avoid printing the index to csv.I tried: And to save the file...However, I still got the unwanted index column. How can I avoid this when I save my files?

2013-12-30 18:24:25Z

I am trying to save a csv to a folder after making some edits to the file. Every time I use pd.to_csv('C:/Path of file.csv') the csv file has a separate column of indexes. I want to avoid printing the index to csv.I tried: And to save the file...However, I still got the unwanted index column. How can I avoid this when I save my files?Use index=False.There are two ways to handle the situation where we do not want the index to be stored in csv file.As others have stated, if you don't want to save the index column in the first place, you can use df.to_csv('processed.csv', index=False)However, since the data you will usually use, have some sort of index themselves, let's say a 'timestamp' column, I would keep the index and load the data using it.So, to save the indexed data, first set their index and then save the DataFrame:Afterwards, you can either read the data with the index:or read the data, and then set the index:If you want no index, read file using:save it using Another solution if you want to keep this column as index. If you want a good format the next statement is the best:In this case you have got a csv file with ',' as separate between columns and utf-8 format.

In addition, numerical index won't appear.

Multiple variables in a 'with' statement?

pufferfish

[Multiple variables in a 'with' statement?](https://stackoverflow.com/questions/893333/multiple-variables-in-a-with-statement)

Is it possible to declare more than one variable using a with statement in Python?Something like:... or is cleaning up two resources at the same time the problem?

2009-05-21 14:51:27Z

Is it possible to declare more than one variable using a with statement in Python?Something like:... or is cleaning up two resources at the same time the problem?It is possible in Python 3 since v3.1 and  Python 2.7. The new with syntax supports multiple context managers:Unlike the contextlib.nested, this guarantees that a and b will have their __exit__()'s called even if C() or it's __enter__() method raises an exception.You can also use earlier variables in later definitions (h/t Ahmad below):contextlib.nested supports this:Update:

To quote the documentation, regarding contextlib.nested:See Rafał Dowgird's answer for more information. Note that if you split the variables into lines, you must use backslashes to wrap the newlines.Parentheses don't work, since Python creates a tuple instead.Since tuples lack a __enter__ attribute, you get an error (undescriptive and does not identify class type):If you try to use as within parentheses, Python catches the mistake at parse time:https://bugs.python.org/issue12782 seems to be related to this issue.I think you want to do this instead:Since Python 3.3, you can use the class ExitStack from the contextlib module.It can manage a dynamic number of context-aware objects, which means that it will prove especially useful if you don't know how many files you are going to handle.The canonical use-case that is mentioned in the documentation is managing a dynamic number of files.Here is a generic example:Output:In Python 3.1+ you can specify multiple context expressions, and they will be processed as if multiple with statements were nested:is equivalent toThis also means that you can use the alias from the first expression in the second (useful when working with db connections/cursors):

return, return None, and no return at all?

Clay Wardell

[return, return None, and no return at all?](https://stackoverflow.com/questions/15300550/return-return-none-and-no-return-at-all)

Consider three functions:They all appear to return None. Are there any differences between how the returned value of these functions behave?  Are there any reasons to prefer one versus the other?

2013-03-08 18:12:15Z

Consider three functions:They all appear to return None. Are there any differences between how the returned value of these functions behave?  Are there any reasons to prefer one versus the other?On the actual behavior, there is no difference. They all return None and that's it. However, there is a time and  place for all of these.

The following instructions are basically how the different methods should be used (or at least how I was taught they should be used), but they are not absolute rules so you can mix them up if you feel necessary to.This tells that the function is indeed meant to return a value for later use, and in this case it returns None. This value None can then be used elsewhere. return None is never used if there are no other possible return values from the function.In the following example, we return person's mother if the person given is a human. If it's not a human, we return None since the person doesn't have a mother (let's suppose it's not an animal or something).This is used for the same reason as break in loops. The return value doesn't matter and you only want to exit the whole function. It's extremely useful in some places, even though you don't need it that often.We've got 15 prisoners and we know one of them has a knife. We loop through each prisoner one by one to check if they have a knife. If we hit the person with a knife, we can just exit the function because we know there's only one knife and no reason the check rest of the prisoners. If we don't find the prisoner with a knife, we raise an alert. This could be done in many different ways and using return is probably not even the best way, but it's just an example to show how to use return for exiting a function.Note: You should never do var = find_prisoner_with_knife(), since the return value is not meant to be caught.This will also return None, but that value is not meant to be used or caught. It simply means that the function ended successfully. It's basically the same as return in void functions in languages such as C++ or Java.In the following example, we set person's mother's name and then the function exits after completing successfully.Note: You should never do var = set_mother(my_person, my_mother), since the return value is not meant to be caught.Yes, they are all the same. We can review the interpreted machine code to confirm that that they're all doing the exact same thing.They each return the same singleton None -- There is no functional difference. I think that it is reasonably idiomatic to leave off the return statement unless you need it to break out of the function early (in which case a bare return is more common), or return something other than None.  It also makes sense and seems to be idiomatic to write return None when it is in a function that has another path that returns something other than None.  Writing return None out explicitly is a visual cue to the reader that there's another branch which returns something more interesting (and that calling code will probably need to handle both types of return values).Often in Python, functions which return None are used like void functions in C -- Their purpose is generally to operate on the input arguments in place (unless you're using global data (shudders)).  Returning None usually makes it more explicit that the arguments were mutated.  This makes it a little more clear why it makes sense to leave off the return statement from a "language conventions" standpoint.That said, if you're working in a code base that already has pre-set conventions around these things, I'd definitely follow suit to help the code base stay uniform...In terms of functionality these are all the same, the difference between them is in code readability and style (which is important to consider)As other have answered, the result is exactly the same, None is returned in all cases.The difference is stylistic, but please note that PEP8 requires the use to be consistent:https://www.python.org/dev/peps/pep-0008/#programming-recommendationsBasically, if you ever return non-None value in a function, it means the return value has meaning and is meant to be caught by callers. So when you return None, it must also be explicit, to convey None in this case has meaning, it is one of the possible return values.If you don't need return at all, you function basically works as a procedure instead of a function, so just don't include the return statement.If you are writing a procedure-like function and there is an opportunity to return earlier (i.e. you are already done at that point and don't need to execute the remaining of the function) you may use empty an returns to signal for the reader it is just an early finish of execution and the None value returned implicitly doesn't have any meaning and is not meant to be caught (the procedure-like function always returns None anyway).

Python List vs. Array - when to use?

Corey Goldberg

[Python List vs. Array - when to use?](https://stackoverflow.com/questions/176011/python-list-vs-array-when-to-use)

If you are creating a 1d array, you can implement it as a List, or else use the 'array' module in the standard library.  I have always used Lists for 1d arrays.What is the reason or circumstance where I would want to use the array module instead?Is it for performance and memory optimization, or am I missing something obvious?

2008-10-06 20:17:43Z

If you are creating a 1d array, you can implement it as a List, or else use the 'array' module in the standard library.  I have always used Lists for 1d arrays.What is the reason or circumstance where I would want to use the array module instead?Is it for performance and memory optimization, or am I missing something obvious?Basically, Python lists are very flexible and can hold completely heterogeneous, arbitrary data, and they can be appended to very efficiently, in amortized constant time.  If you need to shrink and grow your list time-efficiently and without hassle, they are the way to go.  But they use a lot more space than C arrays.The array.array type, on the other hand, is just a thin wrapper on C arrays.  It can hold only homogeneous data, all of the same type, and so it uses only sizeof(one object) * length bytes of memory.  Mostly, you should use it when you need to expose a C array to an extension or a system call (for example, ioctl or fctnl). array.array is also a reasonable way to represent a mutable string in Python 2.x (array('B', bytes)). However, Python 2.6+ and 3.x offers a mutable byte string as bytearray.However, if you want to do math on a homogeneous array of numeric data, then you're much better off using NumPy, which can automatically vectorize operations on complex multi-dimensional arrays.To make a long story short: array.array is useful when you need a homogeneous C array of data for reasons other than doing math.For almost all cases the normal list is the right choice. The arrays module is more like a thin wrapper over C arrays, which give you kind of strongly typed containers (see docs), with access to more C-like types such as signed/unsigned short or double, which are not part of the built-in types. I'd say use the arrays module only if you really need it, in all other cases stick with lists.The array module is kind of one of those things that you probably don't have a need for if you don't know why you would use it (and take note that I'm not trying to say that in a condescending manner!).  Most of the time, the array module is used to interface with C code.  To give you a more direct answer to your question about performance:Arrays are more efficient than lists for some uses.  If you need to allocate an array that you KNOW will not change, then arrays can be faster and use less memory.  GvR has an optimization anecdote in which the array module comes out to be the winner (long read, but worth it).On the other hand, part of the reason why lists eat up more memory than arrays is because python will allocate a few extra elements when all allocated elements get used.  This means that appending items to lists is faster.  So if you plan on adding items, a list is the way to go.TL;DR I'd only use an array if you had an exceptional optimization need or you need to interface with C code (and can't use pyrex).pros of each one :My understanding is that arrays are stored more efficiently (i.e. as contiguous blocks of memory vs. pointers to Python objects), but I am not aware of any performance benefit.  Additionally, with arrays you must store primitives of the same type, whereas lists can store anything.The standard library arrays are useful for binary I/O, such as translating a list of ints to a string to write to, say, a wave file.  That said, as many have already noted, if you're going to do any real work then you should consider using NumPy.If you're going to be using arrays, consider the numpy or scipy packages, which give you arrays with a lot more flexibility.Array can only be used for specific types, whereas lists can be used for any object.Arrays can also only data of one type, whereas a list can have entries of various object types.Arrays are also more efficient for some numerical computation.An important difference between numpy array and list is that array slices are views on the original array. This means that the data is not copied, and any modifications to the view will be reflected in the source array.This answer will sum up almost all the queries about when to use List and Array:

The difference between sys.stdout.write and print?

Johanna Larsson

[The difference between sys.stdout.write and print?](https://stackoverflow.com/questions/3263672/the-difference-between-sys-stdout-write-and-print)

Are there situations in which sys.stdout.write() is preferable to print? (Examples: better performance; code that makes more sense)

2010-07-16 09:53:23Z

Are there situations in which sys.stdout.write() is preferable to print? (Examples: better performance; code that makes more sense)print is just a thin wrapper that formats the inputs (modifiable, but by default with a space between args and newline at the end) and calls the write function of a given object. By default this object is sys.stdout, but you can pass a file using the "chevron" form. For example:See: https://docs.python.org/2/reference/simple_stmts.html?highlight=print#the-print-statementIn Python 3.x, print becomes a function, but it is still possible to pass something other than sys.stdout thanks to the fileargument.See https://docs.python.org/3/library/functions.html#printIn Python 2.6+, print is still a statement, but it can be used as a function withUpdate: Bakuriu commented to point out that there is a small difference between the print function and the print statement (and more generally between a function and a statement).In case of an error when evaluating arguments:print first converts the object to a string (if it is not already a string). It will also put a space before the object if it is not the start of a line and a newline character at the end.When using stdout, you need to convert the object to a string yourself (by calling "str", for example) and there is no newline character.Sois equivalent to:After finishing developing a script the other day, I uploaded it to a unix server. All my debug messages used print statements, and these do not appear on a server log. This is a case where you may need sys.stdout.write instead.Here's some sample code based on the book Learning Python by Mark Lutz that addresses your question:Opening log.txt in a text editor will reveal the following:There's at least one situation in which you want sys.stdout instead of print.When you want to overwrite a line without going to the next line, for instance while drawing a progress bar or a status message, you need to loop over something likeAnd since print adds a newline, you are better off using sys.stdout.If you're writing a command line application that can write to both files and stdout then it is handy.  You can do things like:It does mean you can't use the with open(outfile, 'w') as out: pattern, but sometimes it is worth it.In 2.x, the print statement preprocesses what you give it, turning it into strings along the way, handling separators and newlines, and allowing redirection to a file. 3.x turns it into a function, but it still has the same responsibilities.sys.stdout is a file or file-like that has methods for writing to it which take strings or something along that line.it is preferable when dynamic printing is useful, for instance, to give information in a long process:Observing the example above:If we dive deeply,sys.stdout is a file object which can be used for the output of print()if file argument of print() is not specified, sys.stdout will be usedFor example I'm working on small function which prints stars in pyramid format upon passing the number as argument, although you can accomplish this using end="" to print in a separate line, I used sys.stdout.write in co-ordination with print to make this work. To elaborate on this stdout.write prints in the same line where as print always prints its contents in a separate line.I have found that stdout works better than print in a multithreading situation.  I use Queue (FIFO) to store the lines to print and I hold all threads before the print line until my print Q is empty.  Even so, using print I sometimes lose the final \n on the debug I/O (using wing pro IDE).When I use std.out with \n in the string the debug I/O formats correctly and the \n's are accurately displayed. In Python 3 there is valid reason to use print over sys.stdout.write, but this reason can also be turned into a reason to use sys.stdout.write instead.This reason is that, now print is a function in Python 3, you can override this. So you can use print everywhere in a simple script and decide those print statements need to write to stderr instead. You can now just redefine the print function, you could even change the print function global by changing it using the builtins module. Off course with file.write you can specify what file is, but with overwriting print you can also redefine the line separator, or argument separator. The other way around is. Maybe you are absolutely certain you write to stdout, but also know you are going to change print to something else, you can decide to use sys.stdout.write, and use print for error log or something else. So, what you use depends on how you intend to use it. print is more flexible, but that can be a reason to use and to not use it. I would still opt for flexibility instead, and choose print. Another reason to use print instead is familiarity. More people will now what you mean by print and less know sys.stdout.write.One of the difference is the following, when trying to print a byte into its hexadecimal appearance. For example, we know that decimal value of 255 is 0xFF in hexadecimal appearance:In python 2 if you need to pass around a function then you can assign os.sys.stdout.write to a variable, you cannot do this (in the repl) with print.That works as expected.That does not work. print is a magical function.A difference between print and sys.stdout.write to point out in Python 3, is also the value which is returned when executed in terminal. In Python 3 sys.stdout.write returns the lenght of the string whereas print returns just None.So for example running following code interactively in the terminal would print out the string followed by its lenght, since the lenght is returned and outputed when run interactively:

How to compare two dates?

Cecil Rodriguez

[How to compare two dates?](https://stackoverflow.com/questions/8142364/how-to-compare-two-dates)

How would I compare two dates to see which is later, using Python?For example, I want to check if the current date is past the last date in this list I am creating, of holiday dates, so that it will send an email automatically, telling the admin to update the holiday.txt file.

2011-11-15 19:58:14Z

How would I compare two dates to see which is later, using Python?For example, I want to check if the current date is past the last date in this list I am creating, of holiday dates, so that it will send an email automatically, telling the admin to update the holiday.txt file.Use the datetime method and the operator < and its kin.Use timeLet's say you have the initial dates as strings like these:

date1 = "31/12/2015"

date2 = "01/01/2016"You can do the following:

newdate1 = time.strptime(date1, "%d/%m/%Y") and newdate2 = time.strptime(date2, "%d/%m/%Y") to convert them to python's date format. Then, the comparison is obvious:

newdate1 > newdate2 will return False

newdate1 < newdate2 will return Truedatetime.date(2011, 1, 1) < datetime.date(2011, 1, 2) will return True.datetime.date(2011, 1, 1) - datetime.date(2011, 1, 2) will return datetime.timedelta(-1).datetime.date(2011, 1, 1) + datetime.date(2011, 1, 2) will return datetime.timedelta(1).see the docs.Other answers using datetime and comparisons also work for time only, without a date.For example, to check if right now it is more or less than 8:00 a.m., we can use:And later compare with:which will return TrueFor calculating days in two dates difference, can be done like below:Becuase if one second is more with the due date then we have to charge 

How do I remove a substring from the end of a string in Python?

Ramya

[How do I remove a substring from the end of a string in Python?](https://stackoverflow.com/questions/1038824/how-do-i-remove-a-substring-from-the-end-of-a-string-in-python)

I have the following code:I expected: abcdcI got: abcdNow I do Is there a better way?

2009-06-24 14:44:01Z

I have the following code:I expected: abcdcI got: abcdNow I do Is there a better way?strip doesn't mean "remove this substring". x.strip(y) treats y as a set of characters and strips any characters in that set from the ends of x.Instead, you could use endswith and slicing:Or using regular expressions:If you are sure that the string only appears at the end, then the simplest way would be to use 'replace':Since it seems like nobody has pointed this on out yet:This should be more efficient than the methods using split() as no new list object is created, and this solution works for strings with several dots.Depends on what you know about your url and exactly what you're tryinh to do.  If you know that it will always end in '.com' (or '.net' or '.org') then is the quickest solution. If it's a more general URLs then you're probably better of looking into the urlparse library that comes with python.  If you on the other hand you simply want to remove everything after the final '.' in a string then    will work.  Or if you want just want everything up to the first '.' then tryIf you know it's an extension, thenThis works equally well with abcdc.com or www.abcdc.com or abcdc.[anything] and is more extensible.In one line:How about url[:-4]?For urls (as it seems to be a part of the topic by the given example), one can do something like this:Both will output:

('http://www.stackoverflow', '.com')This can also be combined with str.endswith(suffix) if you need to just split ".com", or anything specific. is not quite right.  What you actually would need to write is, and it looks pretty succinct IMHO.However, my personal preference is this option because it uses only one parameter:I want to repeat this answer as the most expressive way to do it. Of course, the following would take less CPU timeHowever, if CPU is the bottle neck why write in Python?When is CPU a bottle neck anyway?? in drivers , maybe.The advantages of using regular expression is code reusability. What if you next want to remove '.me' , which only has three characters?Same code would do the trick.This is a perfect use for regular expressions:Or you can use split:In my case I needed to raise an exception so I did:If you mean to strip only extensionIt works with any extension, with potential other dots existing in filename as well. It simply splits string to list on dots and joins it without last element.Probably not the fastest, but for me it's more readable than other methods.

Determine Whether Integer Is Between Two Other Integers?

Average kid

[Determine Whether Integer Is Between Two Other Integers?](https://stackoverflow.com/questions/13628791/determine-whether-integer-is-between-two-other-integers)

How do I determine whether a given integer is between two other integers (e.g. greater than/equal to 10000 and less than/equal to 30000)?I'm using 2.3 IDLE and what I've attempted so far is not working:

2012-11-29 15:11:59Z

How do I determine whether a given integer is between two other integers (e.g. greater than/equal to 10000 and less than/equal to 30000)?I'm using 2.3 IDLE and what I've attempted so far is not working:Your operator is incorrect.  Should be if number >= 10000 and number <= 30000:.  Additionally, Python has a shorthand for this sort of thing, if 10000 <= number <= 30000:.Your code snippet,actually checks if number is larger than both 10000 and 30000.Assuming you want to check that the number is in the range 10000 - 30000, you could use the Python interval comparison:This Python feature is further described in the Python documentation.The trouble with comparisons is that they can be difficult to debug when you put a >= where there should be a <=Python lets you just write what you mean in wordsIn Python3, you need to use range instead of xrange.edit: People seem to be more concerned with microbench marks and how cool chaining operations. My answer is about defensive (less attack surface for bugs) programming.As a result of a claim in the comments, I've added the micro benchmark here for Python3.5.2If you are worried about performance, you could compute the range onceDefine the range between the numbers:Then use it:There are two ways to compare three integers and check whether b is between a and c:andThe first one looks like more readable, but the second one runs faster.Let's compare using dis.dis:and using timeit:also, you may use range, as suggested before, however it is much more slower.You want the output to print the given statement if and only if the number falls between 10,000 and 30,000.Code should be;Suppose there are 3 non-negative integers: a, b, and c. Mathematically speaking, if we want to determine if c is between a and b, inclusively, one can use this formula:or in Python:

What is the standard way to add N seconds to datetime.time in Python?

Paul Stephenson

[What is the standard way to add N seconds to datetime.time in Python?](https://stackoverflow.com/questions/100210/what-is-the-standard-way-to-add-n-seconds-to-datetime-time-in-python)

Given a datetime.time value in Python, is there a standard way to add an integer number of seconds to it, so that 11:34:59 + 3 = 11:35:02, for example?These obvious ideas don't work:In the end I have written functions like this:I can't help thinking that I'm missing an easier way to do this though.

2008-09-19 07:19:36Z

Given a datetime.time value in Python, is there a standard way to add an integer number of seconds to it, so that 11:34:59 + 3 = 11:35:02, for example?These obvious ideas don't work:In the end I have written functions like this:I can't help thinking that I'm missing an easier way to do this though.You can use full datetime variables with timedelta, and by providing a dummy date then using time to just get the time value.For example:results in the two values, three seconds apart:You could also opt for the more readableif you're so inclined.If you're after a function that can do this, you can look into using addSecs below:This outputs:As others here have stated, you can just use full datetime objects throughout:However, I think it's worth explaining why full datetime objects are required.  Consider what would happen if I added 2 hours to 11pm.  What's the correct behavior?  An exception, because you can't have a time larger than 11:59pm?  Should it wrap back around?Different programmers will expect different things, so whichever result they picked would surprise a lot of people.  Worse yet, programmers would write code that worked just fine when they tested it initially, and then have it break later by doing something unexpected.  This is very bad, which is why you're not allowed to add timedelta objects to time objects.One little thing, might add clarity to override the default value for secondsThanks to @Pax Diablo, @bvmou and @Arachnid for the suggestion of using full datetimes throughout.  If I have to accept datetime.time objects from an external source, then this seems to be an alternative add_secs_to_time() function:This verbose code can be compressed to this one-liner:but I think I'd want to wrap that up in a function for code clarity anyway.If it's worth adding another file / dependency to your project, I've just written a tiny little class that extends datetime.time with the ability to do arithmetic.  When you go past midnight, it wraps around zero.  Now, "What time will it be, 24 hours from now" has a lot of corner cases, including daylight savings time, leap seconds, historical timezone changes, and so on.  But sometimes you really do need the simple case, and that's what this will do.Your example would be written:nptime inherits from datetime.time, so any of those methods should be usable, too.It's available from PyPi as nptime ("non-pedantic time"), or on GitHub: https://github.com/tgs/nptimeYou cannot simply add number to datetime because it's unclear what unit is used: seconds, hours, weeks...There is timedelta class for manipulations with date and time. datetime minus datetime gives timedelta, datetime plus timedelta gives datetime, two datetime objects cannot be added although two timedelta can.Create timedelta object with how many seconds you want to add and add it to datetime object:There is same concept in C++: std::chrono::duration.For completeness' sake, here's the way to do it with arrow (better dates and times for Python):Try adding a datetime.datetime to a datetime.timedelta. If you only want the time portion, you can call the time() method on the resultant datetime.datetime object to get it.Old question, but I figured I'd throw in a function that handles timezones. The key parts are passing the datetime.time object's tzinfo attribute into combine, and then using timetz() instead of time() on the resulting dummy datetime. This answer partly inspired by the other answers here.And here's a really simple test case class (using built-in unittest):

Fastest way to list all primes below N

jbochi

[Fastest way to list all primes below N](https://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n)

This is the best algorithm I could come up.Can it be made even faster?This code has a flaw: Since numbers is an unordered set, there is no guarantee that numbers.pop() will remove the lowest number from the set. Nevertheless, it works (at least for me) for some input numbers:

2010-01-14 23:40:27Z

This is the best algorithm I could come up.Can it be made even faster?This code has a flaw: Since numbers is an unordered set, there is no guarantee that numbers.pop() will remove the lowest number from the set. Nevertheless, it works (at least for me) for some input numbers:Warning: timeit results may vary due to differences in hardware or

version of Python.  Below is a script which compares a number of implementations: 

Many thanks to stephan for bringing sieve_wheel_30 to my attention.

