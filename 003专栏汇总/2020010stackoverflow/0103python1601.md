2015-08-11 21:08:55Z

I'm very confused as to what np.exp() actually does. In the documentation it says that it: "Calculates the exponential of all elements in the input array." I'm confused as to what exactly this means. Could someone give me more information to what it actually does?The exponential function is e^x where e is a mathematical constant called Euler's number, approximately 2.718281. This value has a close mathematical relationship with pi and the slope of the curve e^x is equal to its value at every point. np.exp() calculates e^x for each value of x in your input array.It calculates ex for each x in your list where e is Euler's number (approximately 2.718). In other words, np.exp(range(5)) is similar to [math.e**x for x in range(5)].exp(x) = e^x where e=  2.718281(approx)  output of the Sample Code-> [ 2  7 20] 

Is a list (potentially) divisible by another?

McGuire

[Is a list (potentially) divisible by another?](https://stackoverflow.com/questions/45906747/is-a-list-potentially-divisible-by-another)

Say you have two lists A = [a_1, a_2, ..., a_n] and B = [b_1, b_2, ..., b_n] of integers. We say A is potentially-divisible by B if there is a permutation of B that makes a_i divisible by b_i for all i. The problem is then: is it possible to reorder (i.e. permute) B so that a_i is divisible by b_i for all i?

For example, if you haveThen the answer would be True, as B can be reordered to be B = [3, 6, 4] and then we would have that a_1 / b_1 = 2, a_2 / b_2 = 2, and a_3 / b_3 = 2, all of which are integers, so A is potentially-divisible by B.As an example which should output False, we could have:The reason this is False is that we can't reorder B as 25 and 5 are in A, but the only divisor in B would be 5, so one would be left out.Obviously the straightforward approach would be to get all the permutations of B and see if one would satisfy potential-divisibility, something along the lines of:What is the fastest way to know if a list is potentially-divisible by another list? Any thoughts? I was thinking if there's is a clever way to do this with primes, but I couldn't come up with a solution.Much appreciated!Edit: It's probably irrelevant to most of you, but for the sake of completeness, I'll explain my motivation. In Group Theory there is a conjecture on finite simple groups on whether or not there is a bijection from irreducible characters and conjugacy classes of the group such that every character degree divides the corresponding class size. For example, for U6(4) here are what A and B would look like. Pretty big lists, mind you!

2017-08-27 15:50:38Z

Say you have two lists A = [a_1, a_2, ..., a_n] and B = [b_1, b_2, ..., b_n] of integers. We say A is potentially-divisible by B if there is a permutation of B that makes a_i divisible by b_i for all i. The problem is then: is it possible to reorder (i.e. permute) B so that a_i is divisible by b_i for all i?

For example, if you haveThen the answer would be True, as B can be reordered to be B = [3, 6, 4] and then we would have that a_1 / b_1 = 2, a_2 / b_2 = 2, and a_3 / b_3 = 2, all of which are integers, so A is potentially-divisible by B.As an example which should output False, we could have:The reason this is False is that we can't reorder B as 25 and 5 are in A, but the only divisor in B would be 5, so one would be left out.Obviously the straightforward approach would be to get all the permutations of B and see if one would satisfy potential-divisibility, something along the lines of:What is the fastest way to know if a list is potentially-divisible by another list? Any thoughts? I was thinking if there's is a clever way to do this with primes, but I couldn't come up with a solution.Much appreciated!Edit: It's probably irrelevant to most of you, but for the sake of completeness, I'll explain my motivation. In Group Theory there is a conjecture on finite simple groups on whether or not there is a bijection from irreducible characters and conjugacy classes of the group such that every character degree divides the corresponding class size. For example, for U6(4) here are what A and B would look like. Pretty big lists, mind you!Build bipartite graph structure - connect a[i] with all its divisors from b[].

Then find maximum matching and check whether it is perfect matching (number of edges in matching is equal to the number of pairs (if graph is directed) or to doubled number).  Arbitrary chosen Kuhn algorithm implementation here. Upd:

@Eric Duminil made great concise Python implementation here This approach has polynomial complexity from O(n^2) to O(n^3) depending on chosen matching algorithm and number of edges (division pairs) against factorial complexity for brute-force algorithm.Building on @MBo's excellent answer, here's an implementation of bipartite graph matching using networkx.According to the documentation:It means that the returned dict should be twice as large as A and B.The nodes are converted fromto:in order to avoid collisions between nodes from A and B. The id is also added in order to keep nodes distinct in case of duplicates.The maximum_matching method uses Hopcroft-Karp algorithm, which runs in O(n**2.5) in the worst case. The graph generation is O(n**2), so the whole method runs in O(n**2.5). It should work fine with large arrays. The permutation solution is O(n!) and won't be able to process arrays with 20 elements.If you're interested in a diagram showing the best matching, you can mix matplotlib and networkx:Here are the corresponding diagrams:

Since you're comfortable with math, I just want to add a gloss to the other answers.  Terms to search for are shown in bold.The problem is an instance of permutations with restricted positions, and there's a whole lot that can be said about those.  In general, a zero-one NxN matrix M can be constructed where M[i][j] is 1 if and only if position j is allowed for the element originally at position i.  The number of distinct permutations meeting all the restrictions is then the permanent of M (defined the same way as the determinant, except that all terms are non-negative).Alas - unlike as for the determinant - there are no known general ways to compute the permanent quicker than exponential in N.  However, there are polynomial time algorithms for determining whether or not the permanent is 0.And that's where the answers you got start ;-)  Here's a good account of how the "is the permanent 0?" question is answered efficiently by considering perfect matchings in bipartite graphs:https://cstheory.stackexchange.com/questions/32885/matrix-permanent-is-0So, in practice, it's unlikely you'll find any general approach faster than the one @Eric Duminil gave in their answer.Note, added later:  I should make that last part clearer.  Given any "restricted permutation" matrix M, it's easy to construct integer "divisibilty lists" corresponding to it.  Therefore your specific problem is no easier than the general problem - unless perhaps there's something special about which integers may appear in your lists.For example, suppose M isView the rows as representing the first 4 primes, which are also the values in B:The first row then "says" that B[0] (= 2) can't divide A[0], but must divide A[1], A[2], and A[3].  And so on.  By construction,corresponds to M.  And there are permanent(M) = 9 ways to permute B such that each element of A is divisible by the corresponding element of the permuted B.This is not the ultimate answer but I think this might be something worthful. You can first list the factors(1 and itself included) of all the elements in the list [(1,2,5,10),(1,2,3,6,12),(1,2,3,6),(1,5),(1,3,7,21),(1,5,25)]. The list we are looking for must have one of the factors in it(to evenly divide).

Since we don't have some factors in the list we arre checking against([2,7,5,3,12,3]) This list can further be filtered as:[(2,5),(2,3,12),(2,3),(5),(3,7),(5)]Here, 5 is needed two places(where we don't have any options at all), but we only have a 5, so, we can pretty much stop here and say that the case is false here.Let's say we had [2,7,5,3,5,3] instead:Then we would have option as such:[(2,5),(2,3),(2,3),(5),(3,7),(5)]Since 5 is needed at two places:[(2),(2,3),(2,3),{5},(3,7),{5}] Where {} signifies ensured position.Also 2 is ensured:[{2},(2,3),(2,3),{5},(3,7),{5}] Now since 2 is taken the two places of 3 are ensured:[{2},{3},{3},{5},(3,7),{5}] Now of course 3 are taken and 7 is ensured:[{2},{3},{3},{5},{7},{5}]. which is still consistent with our list so the casse is true. Remember we shall be looking at the consistencies with our list in every iteration where we can readily break out.  You can try this:Output:Another example:Output:

Import Script from a Parent Directory

sazr

[Import Script from a Parent Directory](https://stackoverflow.com/questions/8951255/import-script-from-a-parent-directory)

How do I import a module(python file) that resides in the parent directory?Both directories have a __init__.py file in them but I still cannot import a file from the parent directory?In this folder layout, Script B is attempting to import Script A:The following code in Script B doesn't work:

2012-01-21 06:48:17Z

How do I import a module(python file) that resides in the parent directory?Both directories have a __init__.py file in them but I still cannot import a file from the parent directory?In this folder layout, Script B is attempting to import Script A:The following code in Script B doesn't work:You don't import scripts in Python you import modules. Some python modules are also scripts that you can run directly (they do some useful work at a module-level).In general it is preferable to use absolute imports rather than relative imports.In moduleB:If you'd like to run moduleB.py as a script then make sure that parent directory for toplevel_package is in your sys.path.From the docs:You can do this in packages, but not in scripts you run directly. From the link above:If you create a script that imports A.B.B, you won't receive the ValueError.If you want to run the script directly, you can:Then:

When do I need to use sqlalchemy back_populates?

Liqang Lau

[When do I need to use sqlalchemy back_populates?](https://stackoverflow.com/questions/39869793/when-do-i-need-to-use-sqlalchemy-back-populates)

When I try SQLAlchemy Relation Example following this guide: Basic Relationship PatternsI have this codeIt works well, but in the guide, it says the model should be:Why don't I need back_populates or backref in my example?  When should I use one or the other?

2016-10-05 09:08:57Z

When I try SQLAlchemy Relation Example following this guide: Basic Relationship PatternsI have this codeIt works well, but in the guide, it says the model should be:Why don't I need back_populates or backref in my example?  When should I use one or the other?If you use backref you don't need to declare the relationship on the second table.If you're not using backref, and defining the relationship's separately, then if you don't use back_populates, sqlalchemy won't know to connect the relationships, so that modifying one also modifies the other.So, in your example, where you've defined the relationship's separately, but didn't provide a back_populates argument, modifying one field wouldn't automatically update the other in your transaction.See how it didn't automatically fill out the children field?  Now, if you supply a back_populates argument, sqlalchemy will connect the fields.So now we getSqlalchemy knows these two fields are related now, and will update each as the other is updated.  It's worth noting that using backref will do this, too.  Using back_populates is nice if you want to define the relationships on every class, so it's easy to see all the fields just be glancing at the model class, instead of having to look at other classes that define fields via backref.

Unittest's assertEqual and iterables - only check the contents

Lucas Hoepner

[Unittest's assertEqual and iterables - only check the contents](https://stackoverflow.com/questions/7473071/unittests-assertequal-and-iterables-only-check-the-contents)

Is there a 'decent' way in unittest to check the equality of the contents of two iterable objects? 

I am using a lot of tuples, lists and numpy arrays and I usually only want to test for the contents and not for the type. Currently I am simply casting the type:I used this list comprehension a while ago:But this solution seems a bit inferior to the typecast because it only prints single values if it fails and also it does not fail for different lengths of reference and data (due to the zip-function).

2011-09-19 15:08:52Z

Is there a 'decent' way in unittest to check the equality of the contents of two iterable objects? 

I am using a lot of tuples, lists and numpy arrays and I usually only want to test for the contents and not for the type. Currently I am simply casting the type:I used this list comprehension a while ago:But this solution seems a bit inferior to the typecast because it only prints single values if it fails and also it does not fail for different lengths of reference and data (due to the zip-function).You can always add your own assertion methods to your TestCase class:or take a look at how 2.7 defined it: http://hg.python.org/cpython/file/14cafb8d1480/Lib/unittest/case.py#l621It looks to me you care about the order of items in the sequences. Therefore, assertItemsEqual/assertCountEqual is not for you.In Python 2.7 and in Python 3, what you want is self.assertSequenceEqual. This is sensitive to the order of the items.

A better way for a Python 'for' loop

Max Paython

[A better way for a Python 'for' loop](https://stackoverflow.com/questions/46996315/a-better-way-for-a-python-for-loop)

We all know that the common way of executing a statement a certain number of times in Python is to use a for loop.The general way of doing this is,I believe nobody will argue that the code above is the common implementation, however there is another option. Using the speed of Python list creation by multiplying references.There is also the old while way.I tested the execution times of these approaches. Here is the code.I would not initiate the subject if there was a small difference, however it can be seen that the difference of speed is 100%. Why does not Python encourage such usage if the second method is much more efficient? Is there a better way?The test is done with Windows 10 and Python 3.6.Following @Tim Peters' suggestion,Which offers a much better way, and this pretty much answers my question. Why is this faster than range, since both are generators. Is it because the value never changes?

2017-10-29 02:30:56Z

We all know that the common way of executing a statement a certain number of times in Python is to use a for loop.The general way of doing this is,I believe nobody will argue that the code above is the common implementation, however there is another option. Using the speed of Python list creation by multiplying references.There is also the old while way.I tested the execution times of these approaches. Here is the code.I would not initiate the subject if there was a small difference, however it can be seen that the difference of speed is 100%. Why does not Python encourage such usage if the second method is much more efficient? Is there a better way?The test is done with Windows 10 and Python 3.6.Following @Tim Peters' suggestion,Which offers a much better way, and this pretty much answers my question. Why is this faster than range, since both are generators. Is it because the value never changes?Usingis the non-obvious way of getting the best of all worlds: tiny constant space requirement, and no new objects created per iteration. Under the covers, the C code for repeat uses a native C integer type (not a Python integer object!) to keep track of the count remaining.For that reason, the count needs to fit in the platform C ssize_t type, which is generally at most 2**31 - 1 on a 32-bit box, and here on a 64-bit box:Which is plenty big for my loops ;-)The first method (in Python 3) creates a range object, which can iterate through the range of values. (It's like a generator object but you can iterate through it several times.) It doesn't take up much memory because it doesn't contain the entire range of values, just a current and a maximum value, where it keeps increasing by the step size (default 1) until it hits or passes the maximum.Compare the size of range(0, 1000) to the size of list(range(0, 1000)): Try It Online!. The former is very memory efficient; it only takes 48 bytes regardless of the size, whereas the entire list increases linearly in terms of size.The second method, although faster, takes up that memory I was talking about in the past one. (Also, it seems that although 0 takes up 24 bytes and None takes 16, arrays of 10000 of each have the same size. Interesting. Probably because they're pointers)Interestingly enough, [0] * 10000 is smaller than list(range(10000)) by about 10000, which kind of makes sense because in the first one, everything is the same primitive value so it can be optimized.The third one is also nice because it doesn't require another stack value (whereas calling range requires another spot on the call stack), though since it's 6 times slower, it's not worth that.The last one might be the fastest just because itertools is cool that way :P I think it uses some C-library optimizations, if I remember correctly.The first two methods need to allocate memory blocks for each iteration while the third one would just make a step for each iteration.Range is a slow function, and I use it only when I have to run small code that doesn't require speed, for example, range(0,50). I think you can't compare the three methods; they are totally different.According to a comment below, the first case is only valid for Python 2.7, in Python 3 it works like xrange and doesn't allocate a block for each iteration. I tested it, and he is right.This answer provides a loop construct for convenience. For additional background about looping with itertools.repeat look up Tim Peters' answer above, Alex Martelli's answer here and Raymond Hettinger's answer here.

Mock attributes in Python mock?

Naftuli Kay

[Mock attributes in Python mock?](https://stackoverflow.com/questions/16867509/mock-attributes-in-python-mock)

I'm having a fairly difficult time using mock in Python:The test actually returns the right value, but r.ok is a Mock object, not True. How do you mock attributes in Python's mock library?

2013-05-31 23:40:19Z

I'm having a fairly difficult time using mock in Python:The test actually returns the right value, but r.ok is a Mock object, not True. How do you mock attributes in Python's mock library?You need to use return_value and PropertyMock:This means: when calling requests.post, on the return value of that call, set a PropertyMock for the property ok to return the value True.A compact and simple way to do it is to use new_callable patch's attribute to force patch to use PropertyMock instead of MagicMock to create the mock object. The  other arguments passed to patch will be used to create PropertyMock object.With mock version '1.0.1' the simpler syntax mentioned in the question is supported and works as is!Example code updated (py.test is used instead of unittest):Run this code with: (make sure you install pytest)

What is a good way to order methods in a Python class?

zetafish

[What is a good way to order methods in a Python class?](https://stackoverflow.com/questions/10289461/what-is-a-good-way-to-order-methods-in-a-python-class)

I want to order methods in a Python class but I don't know what is the correct order.When I extract methods in Eclipse with PyDev, Eclipse puts the extracted method on top of the modified method. But this puts the lower level details before the higher level details. According to Uncle Bob I should do the opposite so that my code reads like the headlines of a newspaper. When I program Java I just follow his advice.What is the best practice for Python?

2012-04-23 23:02:09Z

I want to order methods in a Python class but I don't know what is the correct order.When I extract methods in Eclipse with PyDev, Eclipse puts the extracted method on top of the modified method. But this puts the lower level details before the higher level details. According to Uncle Bob I should do the opposite so that my code reads like the headlines of a newspaper. When I program Java I just follow his advice.What is the best practice for Python?As others have pointed out, there is no right way to order your methods. Maybe a PEP suggestion would be useful, but anyway. Let me try to approach your question as objectively as possible.Hope this helps. Most of these rules are not Python-specific by the way. I'm not aware of a language that enforces method order but if so, it would be quite interesting and please comment.There is no one correct order.  Pick a system and stick with it.  The one I use is:I do something similar to @Ethan that I saw in Django's source, where the main difference is  big "############" block comments to delimit the areas. For example,Obviously this is less useful for smaller classes.

Preferred (or most common) file extension for a Python pickle

Raymond Hettinger

[Preferred (or most common) file extension for a Python pickle](https://stackoverflow.com/questions/40433474/preferred-or-most-common-file-extension-for-a-python-pickle)

This is a question from a student that I didn't have a good answer for. At times, I've seen .pickle, .pck, .pcl, and .db for files that contain Python pickles but am unsure what is the most common or best practice.  I know that the latter three extensions are also used for other things.The related question is what MIME type is preferred for sending pickles between systems using a REST API?

2016-11-05 00:18:42Z

This is a question from a student that I didn't have a good answer for. At times, I've seen .pickle, .pck, .pcl, and .db for files that contain Python pickles but am unsure what is the most common or best practice.  I know that the latter three extensions are also used for other things.The related question is what MIME type is preferred for sending pickles between systems using a REST API?Python 2From the Python 2 docs:I would choose .pkl as the extension when using Python 2.Python 3The example in the Python 3 docs now uses .pickle as the file extension:The MIME type preferred for sending pickles from martineau's comment below:See What is the HTTP "content-type" to use for a blob of bytes?

Why use os.path.join over string concatenation?

user1905410

[Why use os.path.join over string concatenation?](https://stackoverflow.com/questions/13944387/why-use-os-path-join-over-string-concatenation)

I'm not able to see the bigger picture here I think; but basically I have no idea why you would use os.path.join instead of just normal string concatenation?I have mainly used VBScript so I don't understand the point of this function.

2012-12-19 01:44:39Z

I'm not able to see the bigger picture here I think; but basically I have no idea why you would use os.path.join instead of just normal string concatenation?I have mainly used VBScript so I don't understand the point of this function.Write filepath manipulations once and it works across many different platforms, for free. The delimiting character is abstracted away, making your job easier.You no longer need to worry if that directory path had a trailing slash or not. os.path.join will add it if it needs to.Using os.path.join makes it obvious to other people reading your code that you are working with filepaths. People can quickly scan through the code and discover it's a filepath intrinsically. If you decide to construct it yourself, you will likely detract the reader from finding actual problems with your code: "Hmm, some string concats, a substitution. Is this a filepath or what? Gah! Why didn't he use os.path.join?" :)Will work on Windows with '\' and Unix (including Mac OS X) with '/'.for posixpath here's the straightforward codedon't have windows but the same should be there with '\'It is OS-independent. If you hardcode your paths as C:\Whatever they will only work on Windows. If you hardcode them with the Unix standard "/" they will only work on Unix. os.path.join detects the operating system it is running under and joins the paths using the correct symbol.

Purpose of else and finally in exception handling

jhourback

[Purpose of else and finally in exception handling](https://stackoverflow.com/questions/6051934/purpose-of-else-and-finally-in-exception-handling)

Are the else and finally sections of exception handling redundant? For example, is there any difference between the following two code snippets?and More generally, can't the contents of else always be moved into the try, and can't the contents of finally just be moved outside the try/catch block? If so, what is the purpose of else and finally? Is it just to enhance readability?

2011-05-18 22:53:23Z

Are the else and finally sections of exception handling redundant? For example, is there any difference between the following two code snippets?and More generally, can't the contents of else always be moved into the try, and can't the contents of finally just be moved outside the try/catch block? If so, what is the purpose of else and finally? Is it just to enhance readability?finally is executed regardless of whether the statements in the try block fail or succeed. else is executed only if the statements in the try block don't raise an exception.No matter what happens, the block in the finally always gets executed. Even if an exception wasn't handled or the exception handlers themselves generate new exceptions.If you move the contents of the else block inside the try block, you will also catch exceptions that might happen during the else block.  If the linein your example throws an IOError, your first code snippet won't catch that error, while your second snippet will.  You try to keep try blocks as small as possible generally to really only catch the exceptions you want to catch.The finally block gets always executed, no matter what.  If for example the try block contains a return statement, a finally block will still be executed, while any code beneath the whole try/except block won't.

Log output of multiprocessing.Process

astrofrog

[Log output of multiprocessing.Process](https://stackoverflow.com/questions/1501651/log-output-of-multiprocessing-process)

Is there a way to log the stdout output from a given Process when using the multiprocessing.Process class in python?

2009-10-01 02:44:03Z

Is there a way to log the stdout output from a given Process when using the multiprocessing.Process class in python?The easiest way might be to just override sys.stdout.  Slightly modifying an example from the multiprocessing manual:And running it:There are only two things I would add to @Mark Rushakoff answer. When debugging, I found it really useful to change the buffering parameter of my open() calls to 0.Otherwise, madness, because when tail -fing the output file the results can be verrry intermittent. buffering=0 for tail -fing great.And for completeness, do yourself a favor and redirect sys.stderr as well.Also, for convenience you might dump that into a separate process class if you wish, Heres a corresponding gistYou can set sys.stdout = Logger() where Logger is a class whose write method (immediately, or accumulating until a \n is detected) calls logging.info (or any other way you want to log). An example of this in action.I'm not sure what you mean by "a given" process (who's given it, what distinguishes it from all others...?), but if you mean you know what process you want to single out that way at the time you instantiate it, then you could wrap its target function (and that only) -- or the run method you're overriding in a Process subclass -- into a wrapper that performs this sys.stdout "redirection" -- and leave other processes alone.Maybe if you nail down the specs a bit I can help in more detail...?

Counting unique values in a column in pandas dataframe like in Qlik?

Alhpa Delta

[Counting unique values in a column in pandas dataframe like in Qlik?](https://stackoverflow.com/questions/45759966/counting-unique-values-in-a-column-in-pandas-dataframe-like-in-qlik)

If I have a table like this:  I can do count(distinct hID) in Qlik to come up with count of 5 for unique hID.  How do I do that in python using a pandas dataframe?  Or maybe a numpy array?  Similarly, if were to do count(hID) I will get 8 in Qlik.  What is the equivalent way to do it in pandas?

2017-08-18 15:15:32Z

If I have a table like this:  I can do count(distinct hID) in Qlik to come up with count of 5 for unique hID.  How do I do that in python using a pandas dataframe?  Or maybe a numpy array?  Similarly, if were to do count(hID) I will get 8 in Qlik.  What is the equivalent way to do it in pandas?Count distict values, use nunique:Count only non-null values, use count:    Count total values including null values, use size attribute:    Use boolean indexing:OR using query:Output:If I assume data is the name of your dataframe, you can do :this will show you the distinct element and their number of occurence.Or get the number of unique values for each column:New in pandas 0.20.0 pd.DataFrame.aggYou've always been able to do an agg within a groupby.  I used stack at the end because I like the presentation better.You can use nunique in pandas:To count unique values in column, say hID of dataframe df, use:you can use unique property by using len function

How can I denote unused function arguments?

Frerich Raabe

[How can I denote unused function arguments?](https://stackoverflow.com/questions/10025680/how-can-i-denote-unused-function-arguments)

When "deconstructing" a tuple, I can use _ to denote tuple elements I'm not interested in, e.g.Using Python 2.x, how can I express the same with function arguments? I tried to use underscores:I also tried to just omit the argument altogether:Is there another way to achieve the same?

2012-04-05 09:04:05Z

When "deconstructing" a tuple, I can use _ to denote tuple elements I'm not interested in, e.g.Using Python 2.x, how can I express the same with function arguments? I tried to use underscores:I also tried to just omit the argument altogether:Is there another way to achieve the same?Here's what I do with unused arguments:A funny way I just thought of is to delete the variable:This has numerous advantages: The underscore is used for things we don't care about and the * in *args denotes a list of arguments. Therefore, we can use *_ to denote a list of things we don't care about:It even passes PyCharm's checks.You can use '_' as prefix, so that pylint will ignore these parameters:If you have both args and keyword arg you should useI think the accepted answer is bad, but it can still work, if you use what I should call "Perl way" of dealing with arguments (I don't know Perl really, but I quit trying to learn it after seeing the sub syntax, with manual argument unpacking):Your function has 3 arguments - this is what it gets called with (Duck typing, remember?). So you get:2 unused parameters. But now, enter improved larsmans' approach:And there go the warnings...However, I still enjoy more the boxed's way:It keeps the self-documenting quality of parameter names. They're a feature, not a bug.But, the language itself doesn't force you there - you could also go the other way around, and  just let all your function have the signature (*args, **kwargs), and do the argument parsing manually every time. Imagine the level of control that gives you. And no more exceptions when being called in a deprecated way after changing your "signature" (argument count and meaning). This is something worth considering ;)In order to avoid "unused variable" inspection messages for unused *args and/or **kwargs, I replace args and kwargs by _ and __ :In addition to remove messages, it clearly shows that you don't care about these arguments.I can't say if it is a really universal solution, but it worked everywhere I've used it until now.

Can a lambda function call itself recursively in Python?

Greg Hewgill

[Can a lambda function call itself recursively in Python?](https://stackoverflow.com/questions/481692/can-a-lambda-function-call-itself-recursively-in-python)

A regular function can contain a call to itself in its definition, no problem.  I can't figure out how to do it with a lambda function though for the simple reason that the lambda function has no name to refer back to.  Is there a way to do it?  How?

2009-01-26 22:42:42Z

A regular function can contain a call to itself in its definition, no problem.  I can't figure out how to do it with a lambda function though for the simple reason that the lambda function has no name to refer back to.  Is there a way to do it?  How?The only way I can think of to do this amounts to giving the function a name:or alternately, for earlier versions of python:Update: using the ideas from the other answers, I was able to wedge the factorial function into a single unnamed lambda:So it's possible, but not really recommended!without reduce, map, named lambdas or python internals:Contrary to what sth said, you CAN directly do this.The first part is the fixed-point combinator Y that facilitates recursion in lambda calculusthe second part is the factorial function fact defined recursivelyY is applied to fact to form another lambda expressionwhich is applied to the third part, n, which evaulates to the nth factorialor equivalentlyIf however you prefer fibs to facts you can do that too using the same combinatorYou can't directly do it, because it has no name. But with a helper function like the Y-combinator Lemmy pointed to, you can create recursion by passing the function as a parameter to itself (as strange as that sounds):This prints the first ten Fibonacci numbers: [1, 1, 2, 3, 5, 8, 13, 21, 34, 55], Yes. I have two ways to do it, and one was already covered. This is my preferred way.I have never used Python, but this is probably what you are looking for.This answer is pretty basic. It is a little simpler than Hugo Walter's answer:Hugo Walter's answer:Hope it would be helpful to someone.Well, not exactly pure lambda recursion, but it's applicable in places, where you can only use lambdas, e.g. reduce, map and list comprehensions, or other lambdas. The trick is to benefit from list comprehension and Python's name scope. The following example traverses the dictionary by the given chain of keys.The lambda reuses its name defined in the list comprehension expression (fn). The example is rather complicated, but it shows the concept.For this we can use Fixed-point combinators, specifically Z combinator, because it will work in strict languages, also called eager languages:Define fact function and modify it:Notice that:And use it:See also:

Fixed-point combinators in JavaScript: Memoizing recursive functionsLambda can easily replace recursive functions in Python:For example, this basic compound_interest:can be replaced by:or for more visibility :USAGE:Output:By the way, instead of slow calculation of Fibonacci:I suggest fast calculation of Fibonacci:It works really fast.Also here is factorial calculation:If you were truly masochistic, you might be able to do it using C extensions, but to echo Greg (hi Greg!), this exceeds the capability of a lambda (unnamed, anonymous) functon.No.  (for most values of no).

How to get text with selenium web driver in python

user3121891

[How to get text with selenium web driver in python](https://stackoverflow.com/questions/20996392/how-to-get-text-with-selenium-web-driver-in-python)

I'm trying to get text using selenium web driver and here is my code. Please note that I don't want to use Xpath, because in my case id gets change on every re-launch of the web page, help please.my code: HTML:

2014-01-08 13:00:35Z

I'm trying to get text using selenium web driver and here is my code. Please note that I don't want to use Xpath, because in my case id gets change on every re-launch of the web page, help please.my code: HTML:You want just .text.You can then verify it after you've got it, don't attempt to pass in what you expect it should have.PythonJava C# RubyFound it, the answer isYou can use:This will return the text within the element and will allow you to verify it after that.Thanks this is the correct answer it worked!!

Using Colormaps to set color of line in matplotlib

fodon

[Using Colormaps to set color of line in matplotlib](https://stackoverflow.com/questions/8931268/using-colormaps-to-set-color-of-line-in-matplotlib)

How does one set the color of a line in matplotlib with scalar values provided at run time using a colormap (say jet)? I tried a couple of different approaches here and I think I'm stumped. values[] is a storted array of scalars. curves are a set of 1-d arrays, and labels are an array of text strings. Each of the arrays have the same length.

2012-01-19 18:24:01Z

How does one set the color of a line in matplotlib with scalar values provided at run time using a colormap (say jet)? I tried a couple of different approaches here and I think I'm stumped. values[] is a storted array of scalars. curves are a set of 1-d arrays, and labels are an array of text strings. Each of the arrays have the same length.The error you are receiving is due to how you define jet. You are creating the base class Colormap with the name 'jet', but this is very different from getting the default definition of the 'jet' colormap. This base class should never be created directly, and only the subclasses should be instantiated. What you've found with your example is a buggy behavior in Matplotlib. There should be a clearer error message generated when this code is run. This is an updated version of your example: Resulting in:Using a ScalarMappable is an improvement over the approach presented in my related answer:

creating over 20 unique legend colors using matplotlibI thought it would be beneficial to include what I consider to be a more simple method using numpy's linspace coupled with matplotlib's cm-type object. It's possible that the above solution is for an older version. I am using the python 3.4.3, matplotlib 1.4.3, and numpy 1.9.3., and my solution is as follows.This results in 1000 uniquely-colored lines that span the entire cm.jet colormap as pictured below. If you run this script you'll find that you can zoom in on the individual lines. Now say I want my 1000 line colors to just span the greenish portion between lines 400 to 600. I simply change my start and stop values to 0.4 and 0.6 and this results in using only 20% of the cm.jet color map between 0.4 and 0.6. So in a one line summary you can create a list of rgba colors from a matplotlib.cm colormap accordingly:In this case I use the commonly invoked map named jet but you can find the complete list of colormaps available in your matplotlib version by invoking:A combination of line styles, markers, and qualitative colors from matplotlib:UPDATE: Supporting not only ListedColormap, but also LinearSegmentedColormapU may do as I have written from my deleted account (ban for new posts :( there was). Its rather simple and nice looking.Im using 3-rd one of these 3 ones usually, also I wasny checking 1 and 2 version.example of 3: Ship RAO of Roll vs Ikeda damping in function of Roll amplitude A44

Why can't easy_install find MySQLdb?

BryanWheelock

[Why can't easy_install find MySQLdb?](https://stackoverflow.com/questions/3047848/why-cant-easy-install-find-mysqldb)

This is what I tried:

2010-06-15 18:15:11Z

This is what I tried:You have the wrong package name.MySQL-python is the right one:or Adam is right but before you run easy_install MySQL-python you need to make sure python-dev is installed as it is not installed by default.Install is with apt-get install python-dev.If you are using "yum" the command is sudo yum install python-devel (where 'sudo' may be optional depending on your user account)

ImportError: No module named psycopg2

ws_123

[ImportError: No module named psycopg2](https://stackoverflow.com/questions/12906351/importerror-no-module-named-psycopg2)

When installing process of OpenERP 6, I want to generate a config file with this command,cd /home/openerp/openerp-server/bin/./openerp-server.py -s --stop-after-init -c /home/openerp/openerp-server.cfgBut it was always showed a message ImportError: No module named psycopg2When I check for psycopg2 package, it's already installed,Package python-psycopg2-2.4.5-1.rhel5.x86_64 already installed and latest versionNothing to doWhat's wrong with this?My server is CentOS, I've installed python 2.6.7.

2012-10-16 01:30:55Z

When installing process of OpenERP 6, I want to generate a config file with this command,cd /home/openerp/openerp-server/bin/./openerp-server.py -s --stop-after-init -c /home/openerp/openerp-server.cfgBut it was always showed a message ImportError: No module named psycopg2When I check for psycopg2 package, it's already installed,Package python-psycopg2-2.4.5-1.rhel5.x86_64 already installed and latest versionNothing to doWhat's wrong with this?My server is CentOS, I've installed python 2.6.7.Step 1: Install the dependenciesStep 2: Run this command in your virtualenvRef: Fernando MunozI faced the same issue and resolved it with following commands:Use psycopg2-binary instead of psycopg2.Or you will get the warning below:Reference: Psycopg 2.7.4 released | PsycopgPlease try to run the command import psycopg2 on the python console. If you get the error then check the sys.path where the python look for the install module. If the parent directory of the python-psycopg2-2.4.5-1.rhel5.x86_64 is there in the sys.path or not. If its not in the sys.path then run export PYTHONPATH=<parent directory of python-psycopg2-2.4.5-1.rhel5.x86_64> before running the openerp server.Try with these:run python and try to import if you insist on installing it on your systems python try:Try installing with

pip install psycopg2-binary --userYou need to install the psycopg2 module.On CentOS:

Make sure Python 2.7+ is installed. If not, follow these instructions: http://toomuchdata.com/2014/02/16/how-to-install-python-on-centos/Even though this is a CentOS question, here are the instructions for Ubuntu:Cite: http://initd.org/psycopg/install/Recently faced this issue on my production server. I had installed pyscopg2 using It worked beautifully on my local, but had me for a run on my ec2 server.The above command worked for me there. Posting here just in case it would help someone in future.For Python3Step 1: Install DependenciesStep 2: InstallI faced same problem and waste almost a day to resolve this issue .

I have done 2 things 

1- use python 3.6 instead of 3.8

2- change django 2.2 version(may be working some higher but i change to 2.2)Now its working finecheck correctly if you had ON your virtual env of your peoject, if it's OFF then make it ON. execute following cammands:It's working for me

Does performance differ between Python or C++ coding of OpenCV?

erogol

[Does performance differ between Python or C++ coding of OpenCV?](https://stackoverflow.com/questions/13432800/does-performance-differ-between-python-or-c-coding-of-opencv)

I aim to start opencv little by little but first I need to decide which API of OpenCV is more useful. I predict that Python implementation is shorter but running time will be more dense and slow compared to the native C++ implementations. Is there any know can comment about performance and coding differences between these two perspectives?

2012-11-17 17:14:44Z

I aim to start opencv little by little but first I need to decide which API of OpenCV is more useful. I predict that Python implementation is shorter but running time will be more dense and slow compared to the native C++ implementations. Is there any know can comment about performance and coding differences between these two perspectives?As mentioned in earlier answers, Python is slower compared to C++ or C. Python is built for its simplicity, portability and moreover, creativity where users need to worry only about their algorithm, not programming troubles.But here in OpenCV, there is something different. Python-OpenCV is just a wrapper around the original C/C++ code. It is normally used for combining best features of both the languages, Performance of C/C++ & Simplicity of Python.So when you call a function in OpenCV from Python, what actually run is underlying C/C++ source. So there won't be much difference in performance.( I remember I read somewhere that performance penalty is <1%, don't remember where. A rough estimate with some basic functions in OpenCV shows a worst-case penalty of <4%. ie penalty = [maximum time taken in Python - minimum time taken in C++]/minimum time taken in C++ ).The problem arises when your code has a lot of native python codes.For eg, if you are making your own functions that are not available in OpenCV, things get worse. Such codes are ran natively in Python, which reduces the performance considerably. But new OpenCV-Python interface has full support to Numpy. Numpy is a package for scientific computing in Python. It is also a wrapper around native C code. It is a highly optimized library which supports a wide variety of matrix operations, highly suitable for image processing. So if you can combine both OpenCV functions and Numpy functions correctly, you will get a very high speed code.Thing to remember is, always try to avoid loops and iterations in Python. Instead, use array manipulation facilities available in Numpy (and OpenCV). Simply adding two numpy arrays using C = A+B is a lot times faster than using double loops.For eg, you can check these articles : All google results for openCV state the same: that python will only be slightly slower. But not once have I seen any profiling on that. So I decided to do some and discovered:Python is significantly slower than C++ with opencv, even for trivial programs.The most simple example I could think of was to display the output of a webcam on-screen and display the number of frames per second. With python, I achieved 50FPS (on an Intel atom). With C++, I got 65FPS, an increase of 25%. In both cases, the CPU usage was using a single core, and to the best of my knowledge, was bound by the performance of the CPU. 

Additionally this test case about aligns with what I have seen in projects I've ported from one to the other in the past.Where does this difference come from? In python, all of the openCV functions return new copies of the image matrices. Whenever you capture an image, or if you resize it - in C++ you can re-use existing memory. In python you cannot. I suspect this time spent allocating memory is the major difference, because as others have said: the underlying code of openCV is C++.Before you throw python out the window: python is much faster to develop in, and if long as you aren't running into hardware-constraints, or if development speed it more important than performance, then use python. In many applications I've done with openCV, I've started in python and later converted only the computer vision components to C++ (eg using python's ctype module and compiling the CV code into a shared library). Python Code:C++ Code:Possible benchmark limitations:The answer from sdfgeoff is missing the fact that you can reuse arrays in Python. Preallocate them and pass them in, and they will get used. So:You're right, Python is almost always significantly slower than C++ as it requires an interpreter, which C++ does not. However, that does require C++ to be strongly-typed, which leaves a much smaller margin for error. Some people prefer to be made to code strictly, whereas others enjoy Python's inherent leniency.If you want a full discourse on Python coding styles vs. C++ coding styles, this is not the best place, try finding an article.EDIT:

Because Python is an interpreted language, while C++ is compiled down to machine code, generally speaking, you can obtain performance advantages using C++.  However, with regard to using OpenCV, the core OpenCV libraries are already compiled down to machine code, so the Python wrapper around the OpenCV library is executing compiled code.  In other words, when it comes to executing computationally expensive OpenCV algorithms from Python, you're not going to see much of a performance hit since they've already been compiled for the specific architecture you're working with.

How to find overlapping matches with a regexp?

futurenext110

[How to find overlapping matches with a regexp?](https://stackoverflow.com/questions/11430863/how-to-find-overlapping-matches-with-a-regexp)

Since \w\w means two characters, 'he' and 'll' are expected. But why do 'el' and 'lo' not match the regex?

2012-07-11 10:39:16Z

Since \w\w means two characters, 'he' and 'll' are expected. But why do 'el' and 'lo' not match the regex?findall doesn't yield overlapping matches by default. This expression does however:Here (?=...) is a lookahead assertion:You can use the new Python regex module, which supports overlapping matches.Except for zero-length assertion, character in the input will always be consumed in the matching. If you are ever in the case where you want to capture certain character in the input string more the once, you will need zero-length assertion in the regex.There are several zero-length assertion (e.g. ^ (start of input/line), $ (end of input/line), \b (word boundary)), but look-arounds ((?<=) positive look-behind and (?=) positive look-ahead) are the only way that you can capture overlapping text from the input. Negative look-arounds ((?<!) negative look-behind, (?!) negative look-ahead) are not very useful here: if they assert true, then the capture inside failed; if they assert false, then the match fails. These assertions are zero-length (as mentioned before), which means that they will assert without consuming the characters in the input string. They will actually match empty string if the assertion passes.Applying the knowledge above, a regex that works for your case would be:Am no regex expert but I would like to answer my similar question.If you want to use a capture group with the lookahead:example regex: (\d)(?=.\1)string: 5252this will match the first 5 as well as the first 2The (\d) is to make a capture group, (?=\d\1) is to match any digit followed by the capture group 1 without consuming the string, thus allow overlapping

Reason for globals() in Python?

Ian Clelland

[Reason for globals() in Python?](https://stackoverflow.com/questions/12693606/reason-for-globals-in-python)

What is the reason of having globals() function in Python? It only returns dictionary of global variables, which are already global, so they can be used anywhere... I'm asking only out of curiosity, trying to learn python.I can't really see the point here? Only time I would need it, was if I had local and global variables, with same name for both of themBut you should never run into a problem of having two variables with same name, and needing to use them both within same scope.

2012-10-02 15:39:46Z

What is the reason of having globals() function in Python? It only returns dictionary of global variables, which are already global, so they can be used anywhere... I'm asking only out of curiosity, trying to learn python.I can't really see the point here? Only time I would need it, was if I had local and global variables, with same name for both of themBut you should never run into a problem of having two variables with same name, and needing to use them both within same scope.Python gives the programmer a large number of tools for introspecting the running environment. globals() is just one of those, and it can be very useful in a debugging session to see what objects the global scope actually contains.The rationale behind it, I'm sure, is the same as that of using locals() to see the variables defined in a function, or using dir to see the contents of a module, or the attributes of an object.Coming from a C++ background, I can understand that these things seem unnecessary. In a statically linked, statically typed environment, they absolutely would be. In that case, it is known at compile time exactly what variables are global, and what members an object will have, and even what names are exported by another compilation unit.In a dynamic language, however, these things are not fixed; they can change depending on how code is imported, or even during run time. For that reason at least, having access to this sort of information in a debugger can be invaluable.It's also useful when you need to call a function using function's string name. For example:You can pass the result of globals() and locals() to the eval, execfile and __import__ commands. Doing so creates a restricted environment for those commands to work in. Thus, these functions exist to support other functions that benefit from being given an environment potentially different from the current context. You could, for example, call globals() then remove or add some variables before calling one of those functions. globals() is useful for eval() -- if you want to evaluate some code that refers to variables in scope, those variables will either be in globals or locals.To expand a bit, the eval() builtin function will interpret a string of Python code given to it. The signature is: eval(codeString, globals, locals), and you would use it like so:This works, because the interpreter gets the value of x from the locals() dict of variables. You can of course supply your own dict of variables to eval.It can be useful in 'declarative python'. For instance, in the below FooDef and BarDef are classes used to define a series of data structures which are then used by some package as its input, or its configuration. This allows you a lot of flexibility in what your input is, and you don't need to write a parser.Note that this configuration file uses a loop to build up a list of names which are part of the configuration of Foo_other. So, this configuration language comes with a very powerful 'preprocessor', with an available run-time library. In case you want to, say, find a complex log, or extract things from a zip file and base64 decode them, as part of generating your configuration (this approach is not recommended, of course, for cases where the input may be from an untrusted source...)The package reads the configuration using something like the following:So, finally getting to the point, globals() is useful, when using such a package, if you want to mint a series of definitions procedurally:This is equivalent to writing outAn example of a package which obtains its input by gathering a bunch of definitions from a python module is PLY (Python-lex-yacc) http://www.dabeaz.com/ply/  -- in that case the objects are mostly  function objects, but metadata from the function objects (their names, docstrings, and order of definition) also form part of the input. It's not such a good example for use of globals() . Also, it is imported by the 'configuration' - the latter being a normal python script -- rather than the other way around.I've used 'declarative python' on a few projects I've worked on, and have had occasion to use globals() when writing configurations for those. You could certainly argue that this was due to a weakness in the way the configuration 'language' was designed. Use of globals() in this way doesn't produce very clear results; just results which might be easier to maintain than writing out a dozen nearly-identical statements.You can also use it to give variables significance within the configuration file, according to their names:This method could be useful for any python module that defines a lot of tables and structures, to make it easier to add items to the data, without having to maintain the references as well.It can also be used to get an instance of the class 'classname' from a

string:It might be useful if you like to import module you just have built:a.pyb.pyNot really. Global variables Python really has are module-scoped variables.run python a.py, at least two output of globals()['__name__'] is different.Code here in cpython on Github shows it.

How can I remove text within parentheses with a regex?

Technical Bard

[How can I remove text within parentheses with a regex?](https://stackoverflow.com/questions/640001/how-can-i-remove-text-within-parentheses-with-a-regex)

I'm trying to handle a bunch of files, and I need to alter then to remove extraneous information in the filenames; notably, I'm trying to remove text inside parentheses. For example:and I want to regex a whole bunch of files where the parenthetical expression might be in the middle or at the end, and of variable length.What would the regex look like?  Perl or Python syntax would be preferred.

2009-03-12 18:56:57Z

I'm trying to handle a bunch of files, and I need to alter then to remove extraneous information in the filenames; notably, I'm trying to remove text inside parentheses. For example:and I want to regex a whole bunch of files where the parenthetical expression might be in the middle or at the end, and of variable length.What would the regex look like?  Perl or Python syntax would be preferred.So in Python, you'd do:The pattern that matches substrings in parentheses having no other ( and ) characters in between (like (xyz 123) in Text (abc(xyz 123)) isDetails:Removing code snippets:I would use:If you don't absolutely need to use a regex, useconsider using Perl's Text::Balanced to remove the parenthesis.You may be thinking, "Why do all this when a regex does the trick in one line?"Text::Balanced handles nested parenthesis.  So $filename = 'foo_(bar(baz)buz)).foo' will be extracted properly.  The regex based solutions offered here will fail on this string.  The one will stop at the first closing paren, and the other will eat them all.$filename =~ s/([^}]*)//;

   # returns 'foo_buz)).foo'$filename =~ s/(.*)//;

   # returns 'foo_.foo'# text balanced example returns 'foo_).foo'If either of the regex behaviors is acceptable, use a regex--but document the limitations and the assumptions being made.If a path may contain parentheses then the r'\(.*?\)' regex is not enough:By default the function preserves parenthesized chunks in directory and extention parts of the path.Example:If you can stand to use sed (possibly execute from within your program, it'd be as simple as:For those who want to use Python, here's a simple routine that removes parenthesized substrings, including those with nested parentheses. Okay, it's not a regex, but it'll do the job!Java code:

How do I autoformat some Python code to be correctly formatted?

John Feminella

[How do I autoformat some Python code to be correctly formatted?](https://stackoverflow.com/questions/2625294/how-do-i-autoformat-some-python-code-to-be-correctly-formatted)

I have some existing code which isn't formatted consistently -- sometimes two spaces are used for indent, sometimes four, and so on. The code itself is correct and well-tested, but the formatting is awful.Is there a place online where I can simply paste a snippet of Python code and have it be indented/formatted automatically for me? Alternatively, is there an X such that I can do something like X --input=*.py and have it overwrite each file with a formatted version?

2010-04-12 20:42:57Z

I have some existing code which isn't formatted consistently -- sometimes two spaces are used for indent, sometimes four, and so on. The code itself is correct and well-tested, but the formatting is awful.Is there a place online where I can simply paste a snippet of Python code and have it be indented/formatted automatically for me? Alternatively, is there an X such that I can do something like X --input=*.py and have it overwrite each file with a formatted version?Edit: Nowadays, I would recommend autopep8, since it not only  corrects indentation problems but also (at your discretion) makes code conform to many other PEP8 guidelines.Use reindent.py. It should come with the standard distribution of Python, though on Ubuntu you need to install the python2.6-examples package.You can also find it on the web.This script attempts to convert any python script to conform with the 4-space standard.autopep8 would auto-format your python script. not only the code indentation, but also other coding spacing styles. It makes your python script to conform PEP8 Style Guide.Many editors have pep8 plugins that automatically reformat your code right after you save the file. py-autopep8 in emacsyapf is a new and better python code formatter. which tries to get the best formatting, not just to conform the guidelines. The usage is quite the same as autopep8. For more information, like formatting configurations, please read the README.rst on yapf githubBlack is much better than yapf. It's smarter and fits most complex formatting cases. Use black. It has deliberately only one option (line length) to ensure consistency across many projects. It enforces PEP8.In PyCharm do:1. Select all the Code:2. Format all the CodeSome editors have an auto-format feature that does this for you. Eclipse is one example (though you would probably have to install a python plug-in).Have you checked whichever editor you use for such a feature?

Convert number strings with commas in pandas DataFrame to float

pheon

[Convert number strings with commas in pandas DataFrame to float](https://stackoverflow.com/questions/22137723/convert-number-strings-with-commas-in-pandas-dataframe-to-float)

I have a DataFrame that contains numbers as strings with commas for the thousands marker. I need to convert them to floats.I am guessing I need to use locale.atof. Indeed works as expected. I get a Series of floats.But when I apply it to the DataFrame, I get an error.andgives another error:So, how do I convert this DataFrame of strings to a DataFrame of floats?

2014-03-03 02:37:01Z

I have a DataFrame that contains numbers as strings with commas for the thousands marker. I need to convert them to floats.I am guessing I need to use locale.atof. Indeed works as expected. I get a Series of floats.But when I apply it to the DataFrame, I get an error.andgives another error:So, how do I convert this DataFrame of strings to a DataFrame of floats?If you're reading in from csv then you can use the thousands arg:This method is likely to be more efficient than performing the operation as a separate step.You need to set the locale first:You may use the pandas.Series.str.replace method:This method can remove or replace the comma in the string.You can convert one column at a time like this :

Using the same option multiple times in Python's argparse

John Allard

[Using the same option multiple times in Python's argparse](https://stackoverflow.com/questions/36166225/using-the-same-option-multiple-times-in-pythons-argparse)

I'm trying to write a script that accepts multiple input sources and does something to each one. Something like thisBut I can't quite figure out how to do this using argparse. It seems that it's set up so that each option flag can only be used once. I know how to associate multiple arguments with a single option (nargs='*' or nargs='+'), but that still won't let me use the -i flag multiple times. How do I go about accomplishing this?Just to be clear, what I would like in the end is a list of lists of strings. So

2016-03-22 22:13:11Z

I'm trying to write a script that accepts multiple input sources and does something to each one. Something like thisBut I can't quite figure out how to do this using argparse. It seems that it's set up so that each option flag can only be used once. I know how to associate multiple arguments with a single option (nargs='*' or nargs='+'), but that still won't let me use the -i flag multiple times. How do I go about accomplishing this?Just to be clear, what I would like in the end is a list of lists of strings. SoHere's a parser that handles a repeated 2 argument optional - with names defined in the metavar:This does not handle the 2 or 3 argument case (though I wrote a patch some time ago for a Python bug/issue that would handle such a range).How about a separate argument definition with nargs=3 and metavar=('url','name','other')?The tuple metavar can also be used with nargs='+' and nargs='*'; the 2 strings are used as [-u A [B ...]] or  [-u [A [B ...]]].This is simple; just add both action='append' and nargs='*' (or '+').Then when you run it, you get-i should be configured to accept 3 arguments and to use the append action.To handle an optional value, you might try using a simple custom type. In this case, the argument to -i is a single comma-delimited string, with the number of splits limited to 2. You would need to post-process the values to ensure there are at least two values specified.For more control, define a custom action. This one extends the built-in _AppendAction (used by action='append'), but just does some range checking on the number of arguments given to -i.

Module function vs staticmethod vs classmethod vs no decorators: Which idiom is more pythonic?

Doval

[Module function vs staticmethod vs classmethod vs no decorators: Which idiom is more pythonic?](https://stackoverflow.com/questions/11788195/module-function-vs-staticmethod-vs-classmethod-vs-no-decorators-which-idiom-is)

I'm a Java developer who's toyed around with Python on and off. I recently stumbled upon this article which mentions common mistakes Java programmers make when they pick up Python. The first one caught my eye:I found this a bit strange because the documentation for staticmethod says:Even more puzzling is that this code:Fails as expected in Python 2.7.3 but works fine in 3.2.3 (although you can't call the method on an instance of A, only on the class.)So there's three ways to implement static methods (four if you count using classmethod), each with subtle differences, one of them seemingly undocumented. This seems at odds with Python's mantra of There should be one-- and preferably only one --obvious way to do it. Which idiom is the most Pythonic? What are the pros and cons of each?Here's what I understand so far:Module function:staticmethod:classmethod:Regular method (Python 3 only):Am I overthinking this? Is this a non-issue? Please help!

2012-08-03 01:50:00Z

I'm a Java developer who's toyed around with Python on and off. I recently stumbled upon this article which mentions common mistakes Java programmers make when they pick up Python. The first one caught my eye:I found this a bit strange because the documentation for staticmethod says:Even more puzzling is that this code:Fails as expected in Python 2.7.3 but works fine in 3.2.3 (although you can't call the method on an instance of A, only on the class.)So there's three ways to implement static methods (four if you count using classmethod), each with subtle differences, one of them seemingly undocumented. This seems at odds with Python's mantra of There should be one-- and preferably only one --obvious way to do it. Which idiom is the most Pythonic? What are the pros and cons of each?Here's what I understand so far:Module function:staticmethod:classmethod:Regular method (Python 3 only):Am I overthinking this? Is this a non-issue? Please help!The most straightforward way to think about it is to think in terms of what type of object the method needs in order to do its work.  If your method needs access to an instance, make it a regular method.  If it needs access to the class, make it a classmethod.  If it doesn't need access to the class or the instance, make it a function.  There is rarely a need to make something a staticmethod, but if you find you want a function to be "grouped" with a class (e.g., so it can be overridden) even though it doesn't need access to the class, I guess you could make it a staticmethod.I would add that putting functions at the module level doesn't "pollute" the namespace.  If the functions are meant to be used, they're not polluting the namespace, they're using it just as it should be used.  Functions are legitimate objects in a module, just like classes or anything else.  There's no reason to hide a function in a class if it doesn't have any reason to be there.Great answer by BrenBarn, but I would change 'If it doesn't need access to the class or the instance, make it a function' to:'If it doesn't need access to the class or the instance...but is thematically related to the class (typical example: helper functions and conversion functions used by other class methods or used by alternate constructors), then use staticmethodelse make it a module functionThis is not really an answer, but rather a lengthy comment:I'll try to explain what happens here.This is, strictly speaking, an abuse of the "normal" instance method protocol.What you define here is a method, but with the first (and only) parameter not named self, but x. Of course you can call the method in an instance of A, but you'll have to call it like this:orso the instance is given to the function as first argument.The possibility to call regular methods via the class has always been there and works byHere, as you call the method of the class rather than on the instance, it doesn't get its first parameter given automaticvally, but you'll have to provide it.As long as this is an instance of A, everything is ok. Giving it something else is IMO an abuse of the protocol, and thus the difference between Py2 and Py3:In Py2, A.foo gets transformed to an unbound method and thus requires its first argument be an instance of the class it "lives" in. Calling it with something else will fail.In Py3, this check has been dropped and A.foo is just the original function object. So you can call it with everything as first argument, but I wouldn't do it. The first parameter of a method should always be named self and have the semantics of self.The best answer depends on how the function is going to be used.  In my case, I write application packages that will be used in Jupyter notebooks.  My main  goal is to make things easy for the user.  The main advantage of function definitions is that the user can import their defining file using the "as" keyword.  This allows the user to call the functions in the same way that they would call a function in numpy or matplotlib. One of the disadvantages of Python is that names cannot be protected against further assignment.  However, if "import numpy as np" appears at the top of the notebook, it's a strong hint that "np" should not be used as a common variable name. You can accomplish the same thing with class names, obviously, but user familiarity counts for a lot.  Inside the packages, however, I prefer to use static methods. My software architecture is object oriented, and I write with Eclipse, which I use for multiple target languages. It's convenient to open the source file and see the class definition at the top level, method definitions indented one level, and so on. The audience for the code at this level is mainly other analysts and developers, so it's better to avoid language-specific idioms.I don't have a lot of confidence in Python namespace management, especially when using design patterns where (say) an object passes a reference to itself so that the called object can call a method defined on the caller. So I try not to force it too far. I use a lot of fully qualified names and explicit instance variables (with self) where in other languages I could count on the interpreter or the compiler managing the scope more closely. It's easier to do this with classes and static methods, which is why I think they are the better choice for complex packages where abstraction and information hiding are most useful.  

How do you composite an image onto another image with PIL in Python?

Sebastian

[How do you composite an image onto another image with PIL in Python?](https://stackoverflow.com/questions/2563822/how-do-you-composite-an-image-onto-another-image-with-pil-in-python)

I need to take an image and place it onto a new, generated white background in order for it to be converted into a downloadable desktop wallpaper. So the process would go:In PIL, I see the ImageDraw object, but nothing indicates it can draw existing image data onto another image. Suggestions or links anyone can recommend?

2010-04-01 21:29:45Z

I need to take an image and place it onto a new, generated white background in order for it to be converted into a downloadable desktop wallpaper. So the process would go:In PIL, I see the ImageDraw object, but nothing indicates it can draw existing image data onto another image. Suggestions or links anyone can recommend?This can be accomplished with an Image instance's paste method:This and many other PIL tricks can be picked up at Nadia Alramli's PIL Tutorial Based on unutbus answer:Remember to use Pillow (Documentation, GitHub, PyPI) instead of python-imaging as Pillow works with Python 2.X and Python 3.X.This is to do something similarWhere I started was by generating that 'white background' in photoshop and exporting it as a PNG file. Thats where I got im1 (Image 1). Then used the paste function cause it's way easier. Image.blend()? [link]Or, better yet, Image.paste(), same link.Maybe too late, but for such image operations, we do use ImageSpecField [link] in model with original image.

How should I understand the output of dis.dis?

Andy Hayden

[How should I understand the output of dis.dis?](https://stackoverflow.com/questions/12673074/how-should-i-understand-the-output-of-dis-dis)

I would like to understand how to use dis (the dissembler of Python bytecode). Specifically, how should one interpret the output of dis.dis (or dis.disassemble)?.Here is a very specific example (in Python 2.7.3):I see that JUMP_IF_TRUE_OR_POP etc. are bytecode instructions (although interestingly, BUILD_SET does not appear in this list, though I expect it works as BUILD_TUPLE). I think the numbers on the right-hand-side are memory allocations, and the numbers on the left are goto numbers... I notice they almost increment by 3 each time (but not quite).If I wrap dis.dis("heapq.nsmallest(d,3)") inside a function:

2012-10-01 12:14:33Z

I would like to understand how to use dis (the dissembler of Python bytecode). Specifically, how should one interpret the output of dis.dis (or dis.disassemble)?.Here is a very specific example (in Python 2.7.3):I see that JUMP_IF_TRUE_OR_POP etc. are bytecode instructions (although interestingly, BUILD_SET does not appear in this list, though I expect it works as BUILD_TUPLE). I think the numbers on the right-hand-side are memory allocations, and the numbers on the left are goto numbers... I notice they almost increment by 3 each time (but not quite).If I wrap dis.dis("heapq.nsmallest(d,3)") inside a function:You are trying to disassemble a string containing source code, but that's not supported by dis.dis in Python2. With a string argument, it treats the string as if it contained byte code (see the function disassemble_string in dis.py). So you are seeing nonsensical output based on misinterpreting source code as byte code.Things are different in Python 3, where dis.dis compiles a string argument before disassembling it:In Python 2 you need to compile the code yourself before passing it to dis.dis:What do the numbers mean? The number 1 on the far left is the line number in the source code from which this byte code was compiled. The numbers in the column on the left are the offset of the instruction within the bytecode, and the numbers on the right are the opargs. Let's look at the actual byte code:At offset 0 in the byte code we find 65, the opcode for LOAD_NAME, with the oparg 0000; then (at offset 3) 6a is the opcode LOAD_ATTR, with 0100 the oparg, and so on. Note that the opargs are in little-endian order, so that 0100 is the number 1. The undocumented opcode module contains tables opname giving you the name for each opcode, and opmap giving you the opcode for each name:The meaning of the oparg depends on the opcode, and for the full story you need to read the implementation of the CPython virtual machine in ceval.c. For LOAD_NAME and LOAD_ATTR the oparg is an index into the co_names property of the code object:For LOAD_CONST it is an index into the co_consts property of the code object:For CALL_FUNCTION, it is the number of arguments to pass to the function, encoded in 16 bits with the number of ordinary arguments in the low byte, and the number of keyword arguments in the high byte.I am reposting my answer to another question, in order to be sure to find it while Googling dis.dis().To complete the great Gareth Rees's answer, here is just a small column-by-column summary to explain the output of disassembled bytecode.For example, given this function:This may be disassembled into (Python 3.6):Each column has a specific purpose:

Updating a dataframe column in spark

Luke

[Updating a dataframe column in spark](https://stackoverflow.com/questions/29109916/updating-a-dataframe-column-in-spark)

Looking at the new spark dataframe api, it is unclear whether it is possible to modify dataframe columns.How would I go about changing a value in row x column y of a dataframe?In pandas this would be df.ix[x,y] = new_valueEdit: Consolidating what was said below, you can't modify the existing dataframe as it is immutable, but you can return a new dataframe with the desired modifications.If you just want to replace a value in a column based on a condition, like np.where:If you want to perform some operation on a column and create a new column that is added to the dataframe:If you want the new column to have the same name as the old column, you could add the additional step:

2015-03-17 21:19:04Z

Looking at the new spark dataframe api, it is unclear whether it is possible to modify dataframe columns.How would I go about changing a value in row x column y of a dataframe?In pandas this would be df.ix[x,y] = new_valueEdit: Consolidating what was said below, you can't modify the existing dataframe as it is immutable, but you can return a new dataframe with the desired modifications.If you just want to replace a value in a column based on a condition, like np.where:If you want to perform some operation on a column and create a new column that is added to the dataframe:If you want the new column to have the same name as the old column, you could add the additional step:While you cannot modify a column as such, you may operate on a column and return a new DataFrame reflecting that change. For that you'd first create a UserDefinedFunction implementing the operation to apply and then selectively apply that function to the targeted column only. In Python:new_df now has the same schema as old_df (assuming that old_df.target_column was of type StringType as well) but all values in column target_column will be new_value.Commonly when updating a column, we want to map an old value to a new value. Here's a way to do that in pyspark without UDF's:DataFrames are based on RDDs. RDDs are immutable structures and do not allow updating elements on-site. To change values, you will need to create a new DataFrame by transforming the original one either using the SQL-like DSL or RDD operations like map.A highly recommended slide deck: Introducing DataFrames in Spark for Large Scale Data Science.Just as maasg says you can create a new DataFrame from the result of a map applied to the old DataFrame. An example for a given DataFrame df with two rows:Note that if the types of the columns change, you need to give it a correct schema instead of df.schema. Check out the api of org.apache.spark.sql.Row for available methods: https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Row.html[Update] Or using UDFs in Scala:and if the column name needs to stay the same you can rename it back:

Python multiprocessing safely writing to a file

Big Dogg

[Python multiprocessing safely writing to a file](https://stackoverflow.com/questions/13446445/python-multiprocessing-safely-writing-to-a-file)

I am trying to solve a big numerical problem which involves lots of subproblems, and I'm using Python's multiprocessing module (specifically Pool.map) to split up different independent subproblems onto different cores. Each subproblem involves computing lots of sub-subproblems, and I'm trying to effectively memoize these results by storing them to a file if they have not been computed by any process yet, otherwise skip the computation and just read the results from the file. I'm having concurrency issues with the files: different processes sometimes check to see if a sub-subproblem has been computed yet (by looking for the file where the results would be stored), see that it hasn't, run the computation, then try to write the results to the same file at the same time. How do I avoid writing collisions like this?

2012-11-19 01:13:12Z

I am trying to solve a big numerical problem which involves lots of subproblems, and I'm using Python's multiprocessing module (specifically Pool.map) to split up different independent subproblems onto different cores. Each subproblem involves computing lots of sub-subproblems, and I'm trying to effectively memoize these results by storing them to a file if they have not been computed by any process yet, otherwise skip the computation and just read the results from the file. I'm having concurrency issues with the files: different processes sometimes check to see if a sub-subproblem has been computed yet (by looking for the file where the results would be stored), see that it hasn't, run the computation, then try to write the results to the same file at the same time. How do I avoid writing collisions like this?@GP89 mentioned a good solution. Use a queue to send the writing tasks to a dedicated process that has sole write access to the file. All the other workers have read only access. This will eliminate collisions.  Here is an example that uses apply_async, but it will work with map too:It looks to me that you need to use Manager to temporarily save your results to a list and then write the results from the list to a file. Also, use starmap to pass the object you want to process and the managed list. The first step is to build the parameter to be passed to starmap, which includes the managed list.From this point you need to decide how you are going to handle the list. If you have tons of RAM and a huge data set feel free to concatenate using pandas. Then you can save of the file very easily as a csv or a pickle.

What is the proper way to determine if an object is a bytes-like object in Python?

A. Wilcox

[What is the proper way to determine if an object is a bytes-like object in Python?](https://stackoverflow.com/questions/34869889/what-is-the-proper-way-to-determine-if-an-object-is-a-bytes-like-object-in-pytho)

I have code that expects str but will handle the case of being passed bytes in the following way:Unfortunately, this does not work in the case of bytearray.  Is there a more generic way to test whether an object is either bytes or bytearray, or should I just check for both?  Is hasattr('decode') as bad as I feel it would be?

2016-01-19 06:28:09Z

I have code that expects str but will handle the case of being passed bytes in the following way:Unfortunately, this does not work in the case of bytearray.  Is there a more generic way to test whether an object is either bytes or bytearray, or should I just check for both?  Is hasattr('decode') as bad as I feel it would be?There are a few approaches you could use here.Since Python is duck typed, you could simply do as follows (which seems to be the way usually suggested):You could use hasattr as you describe, however, and it'd probably be fine. This is, of course, assuming the .decode() method for the given object returns a string, and has no nasty side effects.I personally recommend either the exception or hasattr method, but whatever you use is up to you.This approach is uncommon, but is possible:Other encodings are permissible, just like with the buffer protocol's .decode(). You can also pass a third parameter to specify error handling.Python 3.4 and above include a nifty feature called single-dispatch generic functions, via functools.singledispatch. This is a bit more verbose, but it's also more explicit:You could also make special handlers for bytearray and bytes objects if you so chose.Beware: single-dispatch functions only work on the first argument! This is an intentional feature, see PEP 433.You can use:Due to the different base class is used here. To check bytesHowever, The above codes are test under python 2.7Unfortunately, under python 3.4, they are same....This code is not correct unless you know something we don't:You do not (appear to) know the encoding of data.  You are assuming it's UTF-8, but that could very well be wrong.  Since you do not know the encoding, you do not have text.  You have bytes, which could have any meaning under the sun.The good news is that most random sequences of bytes are not valid UTF-8, so when this breaks, it will break loudly (errors='strict' is the default) instead of silently doing the wrong thing.  The even better news is that most of those random sequences that happen to be valid UTF-8 are also valid ASCII, which (nearly) everyone agrees on how to parse anyway.The bad news is that there is no reasonable way to fix this.  There is a standard way of providing encoding information: use str instead of bytes.  If some third-party code handed you a bytes or bytearray object without any further context or information, the only correct action is to fail.Now, assuming you do know the encoding, you can use functools.singledispatch here:This doesn't work on methods, and data has to be the first argument.  If those restrictions don't work for you, use one of the other answers instead.It depends what you want to solve. If you want to have the same code that converts both cases to a string, you can simply convert the type to bytes first, and then decode. This way, it is a one-liner:This way, the answer for you may be:Anyway, I suggest to write 'utf-8' explicitly to the decode, if you do not care to spare few bytes. The reason is that the next time you or someone else will read the source code, the situation will be more apparent.There are two questions here, and the answers to them are different.The first question, the title of this post, is What is the proper way to determine if an object is a bytes-like object in Python? This includes a number of built-in types (bytes, bytearray, array.array, memoryview, others?) and possibly also user-defined types. The best way I know of to check for these is to try to create a memoryview out of them:In the body of the original post, though, it sounds like the question is instead How do I test whether an object supports decode()? @elizabeth-myers' above answer to this question is great. Note that not all bytes-like objects support decode().

How to convert a timezone aware string to datetime in python without dateutil?

lxyu

[How to convert a timezone aware string to datetime in python without dateutil?](https://stackoverflow.com/questions/13182075/how-to-convert-a-timezone-aware-string-to-datetime-in-python-without-dateutil)

I have to convert a timezone-aware string to python datetime object.For example 2012-11-01T04:16:13-04:00.I find there's a dateutil module which have a parse function to do it, but I don't really want to use it as it adds a dependency.So how can I do it? I have tried something like the following, but with no luck.

2012-11-01 17:07:24Z

I have to convert a timezone-aware string to python datetime object.For example 2012-11-01T04:16:13-04:00.I find there's a dateutil module which have a parse function to do it, but I don't really want to use it as it adds a dependency.So how can I do it? I have tried something like the following, but with no luck.As of Python 3.7, datetime.datetime.fromisoformat() can handle your format:In older Python versions you can't, not without a whole lot of painstaking manual timezone defining.Python does not include a timezone database, because it would be outdated too quickly. Instead, Python relies on external libraries, which can have a far faster release cycle, to provide properly configured timezones for you.As a side-effect, this means that timezone parsing also needs to be an external library. If dateutil is too heavy-weight for you, use iso8601 instead, it'll parse your specific format just fine:iso8601 is a whopping 4KB small. Compare that tot python-dateutil's 148KB.As of Python 3.2 Python can handle simple offset-based timezones, and %z will parse -hhmm and +hhmm timezone offsets in a timestamp. That means that for a ISO 8601 timestamp you'd have to remove the : in the timezone:The lack of proper ISO 8601 parsing is being tracked in Python issue 15873.Here is the Python Doc for datetime object using dateutil package..There are two issues with the code in the original question: there should not be a : in the timezone and the format string for "timezone as an offset" is lower case %z not upper %Z.This works for me in Python v3.6I'm new to Python, but found a way to convert 2017-05-27T07:20:18.000-04:00

to2017-05-27T07:20:18 without downloading new utilities.I'm sure there are better ways to do this without slicing up the string so much, but this got the job done.You can convert like this.

Pandas DataFrame aggregate function using multiple columns

user1444817

[Pandas DataFrame aggregate function using multiple columns](https://stackoverflow.com/questions/10951341/pandas-dataframe-aggregate-function-using-multiple-columns)

Is there a way to write an aggregation function as is used in DataFrame.agg method, that would have access to more than one column of the data that is being aggregated? Typical use cases would be weighted average, weighted standard deviation funcs.I would like to be able to write something like

2012-06-08 15:01:32Z

Is there a way to write an aggregation function as is used in DataFrame.agg method, that would have access to more than one column of the data that is being aggregated? Typical use cases would be weighted average, weighted standard deviation funcs.I would like to be able to write something likeYes; use the .apply(...) function, which will be called on each sub-DataFrame. For example:My solution is similar to Nathaniel's solution, only it's for a single column and I don't deep-copy the entire data frame each time, which could be prohibitively slow. The performance gain over the solution groupby(...).apply(...) is about 100x(!)It is possible to return any number of aggregated values from a groupby object with apply. Simply, return a Series and the index values will become the new column names.Let's see a quick example:Define a custom function that will be passed to apply. It implicitly accepts a  DataFrame - meaning the data parameter is a  DataFrame. Notice how it uses multiple columns, which is not possible with the agg groupby method:Call the groupby apply method with our custom function:You can get better performance by precalculating the weighted totals into new DataFrame columns as explained in other answers and avoid using apply altogether.The following (based on Wes McKinney' answer) accomplishes exactly what I was looking for.  I'd be happy to learn if there's a simpler way of doing this within pandas.The function df_wavg() returns a dataframe that's grouped by the "groupby" column, and that returns the sum of the weights for the weights column.  Other columns are either the weighted averages or, if non-numeric, the min() function is used for aggregation.Accomplishing this via groupby(...).apply(...) is non-performant. Here's a solution that I use all the time (essentially using kalu's logic).I do this a lot and found the following quite handy:This will compute the weighted average of all the numerical columns in the df and drop non-numeric ones.

What is the problem with reduce()?

jeremy

[What is the problem with reduce()?](https://stackoverflow.com/questions/181543/what-is-the-problem-with-reduce)

There seems to be a lot of heated discussion on the net about the changes to the reduce() function in python 3.0 and how it should be removed. I am having a little difficulty understanding why this is the case; I find it quite reasonable to use it in a variety of cases. If the contempt was simply subjective, I cannot imagine that such a large number of people would care about it. What am I missing? What is the problem with reduce()?

2008-10-08 06:27:12Z

There seems to be a lot of heated discussion on the net about the changes to the reduce() function in python 3.0 and how it should be removed. I am having a little difficulty understanding why this is the case; I find it quite reasonable to use it in a variety of cases. If the contempt was simply subjective, I cannot imagine that such a large number of people would care about it. What am I missing? What is the problem with reduce()?As Guido says in his The fate of reduce() in Python 3000 post:There is an excellent example of a confusing reduce in the Functional Programming HOWTO article:reduce() is not being removed -- it's simply being moved into the functools module. Guido's reasoning is that except for trivial cases like summation, code written using reduce() is usually clearer when written as an accumulation loop.People worry it encourages an obfuscated style of programming, doing something that can be achieved with clearer methods.I'm not against reduce myself, I also find it a useful tool sometimes.The primary reason of reduce's existence is to avoid writing explicit for loops with accumulators. Even though python has some facilities to support the functional style, it is not encouraged. If you like the 'real' and not 'pythonic' functional style - use a modern Lisp (Clojure?) or Haskel instead.  

Django: Where to put helper functions?

The.Anti.9

[Django: Where to put helper functions?](https://stackoverflow.com/questions/1912351/django-where-to-put-helper-functions)

I have a couple of functions that I wrote that I need to use in my django app. Where would I put the file with them and how would I make them callable within my views?

2009-12-16 04:44:50Z

I have a couple of functions that I wrote that I need to use in my django app. Where would I put the file with them and how would I make them callable within my views?I usually put such app specific helper function in file utils.py and use someting like thisbut it depends what you helper does, may be they modify request , the could be part of middleware, so you need to tell what exactly those helper functions docreate a reusable app that include your generic functions so you can share between projects.use for example a git repo to store this app and manage deployments and evolution (submodule)use a public git repo so you can share with the community :)If they are related to a specific app, I usually just put them in the related app folder and name the file, 'functions.py'.If they're not specific to an app, you could just make a 'functions' app folder and place them in there.I am using new python file service.py in app folder. The file contains mostly helper queries for specific app. Also I used to create a folder inside Django application that contains global helper functions and constants.

Multiple assignment semantics

Aillyn

[Multiple assignment semantics](https://stackoverflow.com/questions/5182573/multiple-assignment-semantics)

In Python one can do:I checked the generated bytecode using dis and they are identical.

So why allow this at all? Would I ever need one of these instead of the others?

2011-03-03 15:09:48Z

In Python one can do:I checked the generated bytecode using dis and they are identical.

So why allow this at all? Would I ever need one of these instead of the others?One case when you need to include more structure on the left hand side of the assignment is when you're asking Python unpack a slightly more complicated sequence.  E.g.:This has proved useful for me in the past, for example, when using enumerate to iterate over a sequence of 2-tuples.  Something like:Python tuples can often be written with or without the parentheses:is equivalent to In some cases, you need parentheses to resolve ambiguities, for examples if you want to pass the tuple (1, 2) to the function f, you will have to write f((1, 2)).  Because the parentheses are sometimes needed, they are always allowed for consistency, just like you can always write (a + b) instead of a + b.If you want to unpack a nested sequence, you also need parentheses:There does not seem to be a reason to also allow square brackets, and people rarely do.They are also same because the assignment happens from Right to left and on the right, you have one type, which is a sequence of two elements. When the asignment call is made, the sequence is unpacked and looked for corresponding elements to match and given to those values.

Yes, any one way should be fine in this case where the sequence is unpacked to respective elements. When unpacking a single-element iterable, the list syntax is prettier:An open parenthesis allows for a multi-line assignment. For example, when reading a row from csv.reader(), it makes code more readable (if less efficient) to load the list into named variables with a single assignment.Starting with a parenthesis avoids long or \ escaped lines.(a, b,

c) = [1, 2, 3](Imagine more and longer variable names)

how to turn on minor ticks only on y axis matplotlib

emad

[how to turn on minor ticks only on y axis matplotlib](https://stackoverflow.com/questions/12711202/how-to-turn-on-minor-ticks-only-on-y-axis-matplotlib)

How can I turn the minor ticks only on y axis on a linear vs linear plot?When I use the function minor_ticks_on to turn minor ticks on, they appear on both x and y axis.

2012-10-03 14:58:29Z

How can I turn the minor ticks only on y axis on a linear vs linear plot?When I use the function minor_ticks_on to turn minor ticks on, they appear on both x and y axis.Nevermind, I figured it out.Here's another way I found in the matplotlib documentation:This will place minor ticks on only the y-axis, since minor ticks are off by default.To set minor ticks at custom locations:Also, if you only want minor ticks on the actual y-axis, rather than on both the left and right-hand sides of the graph, you can follow the plt.axes().yaxis.set_minor_locator(ml) with plt.axes().yaxis.set_tick_params(which='minor', right = 'off'), like so:To clarify the procedure of @emad's answer, the steps to show minor ticks at default locations are:A minimal example:Alternatively, we can get minor ticks at default locations using AutoMinorLocator:Either way, the resulting plot has minor ticks on the y-axis only.

What refactoring tools do you use for Python? [closed]

Thomas Vander Stichele

[What refactoring tools do you use for Python? [closed]](https://stackoverflow.com/questions/28796/what-refactoring-tools-do-you-use-for-python)

I have a bunch of classes I want to rename.  Some of them have names that are small and that name is reused in other class names, where I don't want that name changed.  Most of this lives in Python code, but we also have some XML code that references class names.Simple search and replace only gets me so far.  In my case, I want to rename AdminAction to AdminActionPlug and AdminActionLogger to AdminActionLoggerPlug, so the first one's search-and-replace would also hit the second, wrongly.Does anyone have experience with Python refactoring tools ? Bonus points if they can fix class names in the XML documents too.

2008-08-26 18:26:51Z

I have a bunch of classes I want to rename.  Some of them have names that are small and that name is reused in other class names, where I don't want that name changed.  Most of this lives in Python code, but we also have some XML code that references class names.Simple search and replace only gets me so far.  In my case, I want to rename AdminAction to AdminActionPlug and AdminActionLogger to AdminActionLoggerPlug, so the first one's search-and-replace would also hit the second, wrongly.Does anyone have experience with Python refactoring tools ? Bonus points if they can fix class names in the XML documents too.In the meantime, I've tried it two tools that have some sort of integration with vim.The first is Rope, a python refactoring library that comes with a Vim (and emacs) plug-in.  I tried it for a few renames, and that definitely worked as expected.  It allowed me to preview the refactoring as a diff, which is nice.  It is a bit text-driven, but that's alright for me, just takes longer to learn.The second is Bicycle Repair Man which I guess wins points on name.  Also plugs into vim and emacs.  Haven't played much with it yet, but I remember trying it a long time ago.Haven't played with both enough yet, or tried more types of refactoring, but I will do some more hacking with them.I would strongly recommend PyCharm - not just for refactorings. Since the first PyCharm answer was posted here a few years ago the refactoring support in PyCharm has improved significantly.Python Refactorings available in PyCharm (last checked 2016/07/27 in PyCharm 2016.2)XML refactorings (I checked in context menu in an XML file):Javascript refactorings:WingIDE 4.0 (WingIDE is my python IDE of choice) will support a few refactorings, but I just tried out the latest beta, beta6, and... there's still work to be done.  Retract Method works nicely, but Rename Symbol does not.Update: The 4.0 release has fixed all of the refactoring tools.  They work great now.Your IDE can support refactorings !!

Check it Eric, Eclipse, WingIDE have build in tools for refactorings (Rename including). And that are very safe refactorings - if something can go wrong IDE wont do ref.Also consider adding few unit test to ensure your code did not suffer during refactorings.PyCharm have some refactoring features.I would take a look at Bowler (https://pybowler.io).It's better suited for use directly from the command-line than rope and encourages scripting (one-off scripts).You can use sed to perform this. The trick is to recall that regular expressions can recognize word boundaries. This works on all platforms provided you get the tools, which on Windows is Cygwin, Mac OS may require installing the dev tools, I'm not sure, and Linux has this out of the box. So grep, xargs, and sed should do the trick, after 12 hours of reading man pages and trial and error ;)

Best output type and encoding practices for __repr__() functions?

Eric O Lebigot

[Best output type and encoding practices for __repr__() functions?](https://stackoverflow.com/questions/3627793/best-output-type-and-encoding-practices-for-repr-functions)

Lately, I've had lots of trouble with __repr__(), format(), and encodings.  Should the output of __repr__() be encoded or be a unicode string?  Is there a best encoding for the result of __repr__() in Python?  What I want to output does have non-ASCII characters.I use Python 2.x, and want to write code that can easily be adapted to Python3.  The program thus usesHere are some additional problems that have been bothering me, and I'm looking for a solution that solves them:What would you recommend to do in order to write simple __repr__() functions that behave nicely with respect to these encoding questions?

2010-09-02 13:57:16Z

Lately, I've had lots of trouble with __repr__(), format(), and encodings.  Should the output of __repr__() be encoded or be a unicode string?  Is there a best encoding for the result of __repr__() in Python?  What I want to output does have non-ASCII characters.I use Python 2.x, and want to write code that can easily be adapted to Python3.  The program thus usesHere are some additional problems that have been bothering me, and I'm looking for a solution that solves them:What would you recommend to do in order to write simple __repr__() functions that behave nicely with respect to these encoding questions?In Python2, __repr__ (and __str__) must return a string object, not a

unicode object. In Python3, the situation is reversed, __repr__ and __str__

must return unicode objects, not byte (ne string) objects:In Python2, you don't really have a choice. You have to pick an encoding for the

return value of __repr__.By the way, have you read the PrintFails wiki? It may not directly answer

your other questions, but I did find it helpful in illuminating why certain

errors occur.When using from __future__ import unicode_literals, can be more simply written as assuming str encodes to utf-8 on your system. Without from __future__ import unicode_literals, the expression can be written as:I think a decorator can manage __repr__ incompatibilities in a sane way. Here's what i use:I use a function like the following:Then my __repr__ functions look like this:

Python's standard library - is there a module for balanced binary tree?

aeter

[Python's standard library - is there a module for balanced binary tree?](https://stackoverflow.com/questions/2298165/pythons-standard-library-is-there-a-module-for-balanced-binary-tree)

Is there a module for AVL or Red-Black or some other type of a balanced binary tree in the standard library of Python? I have tried to find one, but unsuccessfully (I'm relatively new to Python). 

2010-02-19 17:06:42Z

Is there a module for AVL or Red-Black or some other type of a balanced binary tree in the standard library of Python? I have tried to find one, but unsuccessfully (I'm relatively new to Python). No, there is not a balanced binary tree in the stdlib. However, from your comments, it sounds like you may have some other options:If neither solution works well for you, you'll have to go to a third party module or implement your own.there is nothing of this sort in stdlib, as far as I can see, but quick look at pypi brings up a few alternative:There have been a few instances where I have found the heapq package (in the stadndard library) to be useful, especially if at any given time you want O(1) access time to the smallest element in your collection.For me, I was keeping track of a collection of timers and was usually just interested in checking if the smallest time (the one to be executed first) was ready to go as of yet.There is a new package called "bintrees" which supports ubalanced, AVL and RB trees. You can find it here.Check out also the Sorted Containers project.Here's a PyCon talk about it: https://www.youtube.com/watch?v=7z2Ki44Vs4ENo, but there's AVL Tree Objects for Python (very old!) and a (closed) project on SourceForge - avl-trees for Python.

Insert some string into given string at given index in Python

james

[Insert some string into given string at given index in Python](https://stackoverflow.com/questions/4022827/insert-some-string-into-given-string-at-given-index-in-python)

I am newbie in Python facing a problem: How to insert some fields in already existing string?For example, suppose I have read one line from any file which contains:Now I have to insert 3rd Field(Group) 3 times more in the same line before Class field. It means the output line should be: I can retrieve 3rd field easily (using split method), but please let me know the easiest way of inserting into the string?

2010-10-26 10:35:17Z

I am newbie in Python facing a problem: How to insert some fields in already existing string?For example, suppose I have read one line from any file which contains:Now I have to insert 3rd Field(Group) 3 times more in the same line before Class field. It means the output line should be: I can retrieve 3rd field easily (using split method), but please let me know the easiest way of inserting into the string?An important point that often bites new Python programmers but the other posters haven't made explicit is that strings in Python are immutable -- you can't ever modify them in place. You need to retrain yourself when working with strings in Python so that instead of thinking, "How can I modify this string?" instead you're thinking "how can I create a new string that has some pieces from this one I've already gotten?"For the sake of future 'newbies' tackling this problem, I think a quick answer would be fitting to this thread.Like bgporter said: Python strings are immutable, and so, in order to modify a string you have to make use of the pieces you already have.In the following example I insert 'Fu' in to 'Kong Panda', to create 'Kong Fu Panda'In the example above, I used the index value to 'slice' the string in to 2 substrings:

1 containing the substring before the insertion index, and the other containing the rest.

Then I simply add the desired string between the two and voil, we have inserted a string inside another.Python's slice notation has a great answer explaining the subject of string slicing.I know it's malapropos, but IMHO easy way is:I had a similar problem for my DNA assignment and I used bgporter's advice to answer it. Here is my function which creates a new string...There are several ways to do this:One way is to use slicing:Another would be to use regular expressions:The functions below will allow one to insert one string into another string:The code below demonstrates how to call the str_insert function given earlier    None of the functions above will modify a string "in-place." The functions each return a modified copy of the string, but the original string  remains intact.For example,Answer for Insert characters of string in other string al located positions

Intersecting two dictionaries in Python

nicole

[Intersecting two dictionaries in Python](https://stackoverflow.com/questions/18554012/intersecting-two-dictionaries-in-python)

I am working on a search program over an inverted index. The index itself is a dictionary whose keys are terms and whose values are themselves dictionaries of short documents, with ID numbers as keys and their text content as values. To perform an 'AND' search for two terms, I thus need to intersect their postings lists (dictionaries). What is a clear (not necessarily overly clever) way to do this in Python? I started out by trying it the long way with iter:

2013-09-01 00:13:52Z

I am working on a search program over an inverted index. The index itself is a dictionary whose keys are terms and whose values are themselves dictionaries of short documents, with ID numbers as keys and their text content as values. To perform an 'AND' search for two terms, I thus need to intersect their postings lists (dictionaries). What is a clear (not necessarily overly clever) way to do this in Python? I started out by trying it the long way with iter:You can easily calculate the intersection of sets, so create sets from the keys and use them for the intersection:A little known fact is that you don't need to construct sets to do this:In Python 2:In Python 3 replace viewkeys with keys; the same applies to viewvalues and viewitems.From the documentation of viewitems:For larger dicts this also slightly faster than constructing sets and then intersecting them:We're comparing nanoseconds here, which may or may not matter to you. In any case, you get back a set, so using viewkeys/keys eliminates a bit of clutter.In Python 3, you can useJust wrap the dictionary instances with a simple class that gets both of the values you wantOkay, here is a generalized version of code above in Python3.

It is optimized to use comprehensions and set-like dict views which are fast enough.Function intersects arbitrary many dicts and returns a dict with common keys and a set of common values for each common key:Usage example:Here the dict values must be hashable, if they aren't you could simply change set parentheses { } to list [ ]:Your question isn't precise enough to give single answer.If you want to intersect IDs from posts (credits to James) do:However if you want to iterate documents you have to consider which post has a priority, I assume it's p1. To iterate documents for common_ids, collections.ChainMap will be most useful:Or if you don't want to create separate intersection dictionary:If you want to intersect items of both posts, which means to match IDs and documents, use code below (credits to DCPY). However this is only useful if you're looking for duplicates in terms.In case when by "'AND' search" and using iter you meant to search both posts then again collections.ChainMap is the best to iterate over (almost) all items in multiple posts:I would recommend changing your index from to two indexes one for the terms and then a separate index to hold the valuesthat way you don't store multiple copies of the same data

How do I use a dictionary to update fields in Django models?

TIMEX

[How do I use a dictionary to update fields in Django models?](https://stackoverflow.com/questions/5503925/how-do-i-use-a-dictionary-to-update-fields-in-django-models)

Suppose I have a model like this:Can I create a dictionary, and then insert or update the model using it?

2011-03-31 17:35:38Z

Suppose I have a model like this:Can I create a dictionary, and then insert or update the model using it?Here's an example of create using your dictionary d:To update an existing model, you will need to use the QuerySet filter method.  Assuming you know the pk of the Book you want to update:If you know you want to create it:Assuming you need to check for an existing instance, you can find it with get or create:As mentioned in another answer, you can also use the update function on the queryset manager, but i believe that will not send any signals out (which may not matter to you if you aren't using them). However, you probably shouldn't use it to alter a single object:Use ** for creating a new model. Loop through the dictionary and use setattr() in order to update an existing model.From Tom Christie's Django Rest Frameworkhttps://github.com/tomchristie/django-rest-framework/blob/master/rest_framework/serializers.pyAdding on top of other answers, here's a bit more secure version to prevent messing up with related fields:It checks, that field you're trying to update is editable, is not primary key and is not one of related fields.Example usage:The luxury DRF serializers .create and .update methods have is that there is limited and validated set of fields, which is not the case for manual update.

Exception traceback is hidden if not re-raised immediately

parxier

[Exception traceback is hidden if not re-raised immediately](https://stackoverflow.com/questions/4825234/exception-traceback-is-hidden-if-not-re-raised-immediately)

I've got a piece of code similar to this:When func2 raises an exception I receive the following traceback:From here I don't see where the exception is coming from. The original traceback is lost.How can I preserve original traceback and re-raise it? I want to see something similar to this:

2011-01-28 05:42:44Z

I've got a piece of code similar to this:When func2 raises an exception I receive the following traceback:From here I don't see where the exception is coming from. The original traceback is lost.How can I preserve original traceback and re-raise it? I want to see something similar to this:A blank raise raises the last exception.If you use raise something Python has no way of knowing if something was an exception just caught before, or a new exception with a new stack trace. That's why there is the blank raise that preserves the stack trace.Reference hereIt is possible to modify and rethrow an exception:So if you want to modify the exception and rethrow it, you can do this:You can get a lot of information about the exception via the sys.exc_info() along with the traceback moduletry the following extension to your code.This would print, similar to what you wanted.While @Jochen's answer works well in the simple case, it is not capable of handling more complex cases, where you are not directly catching and rethrowing, but are for some reason given the exception as an object and wish to re-throw in a completely new context (i.e. if you need to handle it in a different process).In this case, I propose the following:Before you do this, define a new exception type that you will rethrow later...In the offending code...Rethrow...Your main function needs to look like this:This is the standard way of handling (and re-raising) errors. Here is a codepad demonstration.

Django check if a related object exists error: RelatedObjectDoesNotExist

Prometheus

[Django check if a related object exists error: RelatedObjectDoesNotExist](https://stackoverflow.com/questions/27064206/django-check-if-a-related-object-exists-error-relatedobjectdoesnotexist)

I have a method has_related_object in my model that needs to check if a related object existsBut I get the error:

2014-11-21 14:59:04Z

I have a method has_related_object in my model that needs to check if a related object existsBut I get the error:This is because the ORM has to go to the database to check to see if customer exists. Since it doesn't exist, it raises an exception.You'll have to change your method to the following:I don't know the situation with self.car so I'll leave it to you to adjust it if it needs it.Side note:

If you were doing this on a Model that has the ForeignKeyField or OneToOneField on it, you would be able to do the following as a shortcut to avoid the database query.Use hasattr(self, 'customers') to avoid the exception check as recommended in Django docs:

python equivalent of R table

Donbeo

[python equivalent of R table](https://stackoverflow.com/questions/25710875/python-equivalent-of-r-table)

I have a list I want to count the frequency of each element in this list. 

Something like In R this can be obtained with the table function. Is there anything similar in python3?

2014-09-07 13:47:11Z

I have a list I want to count the frequency of each element in this list. 

Something like In R this can be obtained with the table function. Is there anything similar in python3?A Counter object from the collections library will function like that.Pandas has a built-in function called value_counts().Example: if your DataFrame has a column with values as 0's and 1's, and you want to count the total frequencies for each of them, then simply use this:you will get the result like:and for [(12,6)] you will get exact number, here 28more about pandas, which is powerful Python data analysis toolkit, you can read in official doc: http://pandas.pydata.org/pandas-docs/stable/UPDATE:If order does not matter just use sorted:

ps = pandas.Series([tuple(sorted(i)) for i in x]) after that result is:Supposing you need to convert the data to a pandas DataFrame anyway, so that you havethen you can do as suggested in this answer, using groupby.size():tab looks as follows:and can easily be changed to a table form with unstack():Fill NaNs and convert to int at your own leisure!IMHO, pandas offers a better solution for this "tabulation" problem:One dimension:Proportion count:Two-dimensions (with totals):Also, as mentioned by other coleagues, pandas value_counts method could be all you need. It is so good that you can have the counts as percentages if you want:I'm very grateful for this blog:http://hamelg.blogspot.com.br/2015/11/python-for-data-analysis-part-19_17.htmlIn Numpy, the best way I've found of doing this is to use unique, e.g:giving me: {(0, 6): 19, (6, 0): 20, (12, 0): 33, (12, 6): 28}

which matches the other answersThis example is a bit more complicated than I normally see, and hence the need for the axis=0 option, if you just want unique values everywhere, you can just miss that out:R seems to make this sort of thing much more convenient!  The above Python code is just plot(table(rnbinom(100000, 10, mu=6))).You can probably do a 1-dimensional count with list comprehension.

How do I create documentation with Pydoc?

michel-slm

[How do I create documentation with Pydoc?](https://stackoverflow.com/questions/13040646/how-do-i-create-documentation-with-pydoc)

I'm trying to create a document out of my module. I  used pydoc from the command-line in Windows 7 using Python 3.2.3:This led to my shell being filled with text, one line for each file in my module, saying:It's as if Pydoc's trying to get documentation for my files, but I want to autocreate it. I couldn't find a good tutorial using Google. Does anyone have any tips on how to use Pydoc?If I try to create documentation from one file using it says wrote myFile.html, and when I open it, it has one line of text saying:Also, it has a link to the file itself on my computer, which I can click and it shows what's inside the file on my web browser.

2012-10-23 23:02:16Z

I'm trying to create a document out of my module. I  used pydoc from the command-line in Windows 7 using Python 3.2.3:This led to my shell being filled with text, one line for each file in my module, saying:It's as if Pydoc's trying to get documentation for my files, but I want to autocreate it. I couldn't find a good tutorial using Google. Does anyone have any tips on how to use Pydoc?If I try to create documentation from one file using it says wrote myFile.html, and when I open it, it has one line of text saying:Also, it has a link to the file itself on my computer, which I can click and it shows what's inside the file on my web browser.As RocketDonkey suggested, your module itself needs to have some docstrings.For example, in myModule/__init__.py:You'd also want to generate documentation for each file in myModule/*.py usingto make sure the generated files match the ones that are referenced from the main module documentation file.Another thing that people may find useful...make sure to leave off ".py" from your module name.  For example, if you are trying to generate documentation for 'original' in 'original.py':pydoc is fantastic for generating documentation, but the documentation has to be written in the first place.  You must have docstrings in your source code as was mentioned by RocketDonkey in the comments:The first docstring provides instructions for creating the documentation with pydoc.  There are examples of different types of docstrings so you can see how they look when generated with pydoc.

Tensorflow One Hot Encoder?

Robert Graves

[Tensorflow One Hot Encoder?](https://stackoverflow.com/questions/33681517/tensorflow-one-hot-encoder)

Does tensorflow have something similar to scikit learn's one hot encoder for processing categorical data?  Would using a placeholder of tf.string behave as categorical data?I realize I can manually pre-process the data before sending it to tensorflow, but having it built in is very convenient.

2015-11-12 21:16:01Z

Does tensorflow have something similar to scikit learn's one hot encoder for processing categorical data?  Would using a placeholder of tf.string behave as categorical data?I realize I can manually pre-process the data before sending it to tensorflow, but having it built in is very convenient.As of TensorFlow 0.8, there is now a native one-hot op, tf.one_hot that can convert a set of sparse labels to a dense one-hot representation.  This is in addition to tf.nn.sparse_softmax_cross_entropy_with_logits, which can in some cases let you compute the cross entropy directly on the sparse labels instead of converting them to one-hot.Previous answer, in case you want to do it the old way:

@Salvador's answer is correct - there (used to be) no native op to do it.  Instead of doing it in numpy, though, you can do it natively in tensorflow using the sparse-to-dense operators:The output, labels, is a one-hot matrix of batch_size x num_labels.Note also that as of 2016-02-12 (which I assume will eventually be part of a 0.7 release), TensorFlow also has the tf.nn.sparse_softmax_cross_entropy_with_logits op, which in some cases can let you do training without needing to convert to a one-hot encoding.Edited to add:  At the end, you may need to explicitly set the shape of labels.  The shape inference doesn't recognize the size of the num_labels component.  If you don't need a dynamic batch size with derived_size, this can be simplified.Edited 2016-02-12 to change the assignment of outshape per comment below.tf.one_hot() is available in TF and easy to use. Lets assume you have 4 possible categories (cat, dog, bird, human) and 2 instances (cat, human). So your depth=4 and your indices=[0, 3]Keep in mind that if you provide index=-1 you will get all zeros in your one-hot vector.Old answer, when this function was not available.After looking though the python documentation, I have not found anything similar. One thing that strengthen my belief that it does not exist is that in their own example they write one_hot manually.You can also do this in scikitlearn.numpy does it!A simple and short way to one-hot encode any integer or list of intergers:Recent versions of TensorFlow (nightlies and maybe even 0.7.1) have an op called tf.one_hot that does what you want.  Check it out!On the other hand if you have a dense matrix and you want to look up and aggregate values in it, you would want to use the embedding_lookup function.Maybe it's due to changes to Tensorflow since Nov 2015, but @dga's answer produced errors. I did get it to work with the following modifications:Take a look at tf.nn.embedding_lookup. It maps from categorical IDs to their embeddings. For an example of how it's used for input data, see here.You can use tf.sparse_to_dense:The sparse_indices argument indicates where the ones should go, output_shape should be set to the number of possible outputs (e.g. the number of labels), and sparse_values should be 1 with the desired type (it will determine the type of the output from the type of sparse_values).There's embedding_ops in Scikit Flow and examples that deal with categorical variables, etc. If you just begin to learn TensorFlow, I would suggest you trying out examples in TensorFlow/skflow first and then once you are more familiar with TensorFlow it would be fairly easy for you to insert TensorFlow code to build a custom model you want (there are also examples for this). Hope those examples for images and text understanding could get you started and let us know if you encounter any issues! (post issues or tag skflow in SO). Current versions of tensorflow implement the following function for creating one-hot tensors:https://www.tensorflow.org/versions/master/api_docs/python/array_ops.html#one_hotAs mentioned above by @dga, Tensorflow has tf.one_hot now:You need to specify depth, otherwise you'll get a pruned one-hot tensor.If you like to do it manually:Note arguments order in tf.concat()Tensorflow 2.0 Compatible Answer: You can do it efficiently using Tensorflow Transform. Code for performing One-Hot Encoding using Tensorflow Transform is shown below:For more information, refer this Tutorial on TF_Transform.There are  a couple ways to do it.The other way to do it is.My version of @CFB and @dga example, shortened a bit to ease understanding.works on TF version 1.3.0. As of Sep 2017. 

What's the difference between dict() and {}?

verix

[What's the difference between dict() and {}?](https://stackoverflow.com/questions/664118/whats-the-difference-between-dict-and)

So let's say I wanna make a dictionary. We'll call it d. But there are multiple ways to initialize a dictionary in Python! For example, I could do this:Or I could do this:Or this, curiously:Or this:And a whole other multitude of ways with the dict() function. So obviously one of the things dict() provides is flexibility in syntax and initialization. But that's not what I'm asking about.Say I were to make d just an empty dictionary. What goes on behind the scenes of the Python interpreter when I do d = {} versus d = dict()? Is it simply two ways to do the same thing? Does using {} have the additional call of dict()? Does one have (even negligible) more overhead than the other? While the question is really completely unimportant, it's a curiosity I would love to have answered.

2009-03-19 21:19:58Z

So let's say I wanna make a dictionary. We'll call it d. But there are multiple ways to initialize a dictionary in Python! For example, I could do this:Or I could do this:Or this, curiously:Or this:And a whole other multitude of ways with the dict() function. So obviously one of the things dict() provides is flexibility in syntax and initialization. But that's not what I'm asking about.Say I were to make d just an empty dictionary. What goes on behind the scenes of the Python interpreter when I do d = {} versus d = dict()? Is it simply two ways to do the same thing? Does using {} have the additional call of dict()? Does one have (even negligible) more overhead than the other? While the question is really completely unimportant, it's a curiosity I would love to have answered.dict() is apparently some C built-in. A really smart or dedicated person (not me) could look at the interpreter source and tell you more. I just wanted to show off dis.dis. :)As far as performance goes:@Jacob: There is a difference in how the objects are allocated, but they are not copy-on-write.  Python allocates a fixed-size "free list" where it can quickly allocate dictionary objects (until it fills).  Dictionaries allocated via the {} syntax (or a C call to PyDict_New) can come from this free list.  When the dictionary is no longer referenced it gets returned to the free list and that memory block can be reused (though the fields are reset first).This first dictionary gets immediately returned to the free list, and the next will reuse its memory space:If you keep a reference, the next dictionary will come from the next free slot:But we can delete the reference to that dictionary and free its slot again:Since the {} syntax is handled in byte-code it can use this optimization mentioned above.  On the other hand dict() is handled like a regular class constructor and Python uses the generic memory allocator, which does not follow an easily predictable pattern like the free list above.Also, looking at compile.c from Python 2.6, with the {} syntax it seems to pre-size the hashtable based on the number of items it's storing which is known at parse time.Basically, {} is syntax and is handled on a language and bytecode level. dict() is just another builtin with a more flexible initialization syntax. Note that dict() was only added in the middle of 2.x series.Update: thanks for the responses.  Removed speculation about copy-on-write.One other difference between {} and dict is that dict always allocates a new dictionary (even if the contents are static) whereas {} doesn't always do so (see mgood's answer for when and why):produces:I'm not suggesting you try to take advantage of this or not, it depends on the particular situation, just pointing it out.  (It's also probably evident from the disassembly if you understand the opcodes).dict() is used when you want to create a dictionary from an iterable, like :Funny usage:    output:In order to create an empty set we should use the keyword set before it 

i.e set() this creates an empty set where as in dicts only the flower brackets can create an empty dictLets go with an example 

Why use **kwargs in python? What are some real world advantages over using named arguments?

meppum

[Why use **kwargs in python? What are some real world advantages over using named arguments?](https://stackoverflow.com/questions/1415812/why-use-kwargs-in-python-what-are-some-real-world-advantages-over-using-named)

I come from a background in static languages. Can someone explain (ideally through example) the real world advantages of using **kwargs over named arguments?To me it only seems to make the function call more ambiguous. Thanks.

2009-09-12 18:38:01Z

I come from a background in static languages. Can someone explain (ideally through example) the real world advantages of using **kwargs over named arguments?To me it only seems to make the function call more ambiguous. Thanks.Real-world examples:Decorators - they're usually generic, so you can't specify the arguments upfront:Places where you want to do magic with an unknown number of keyword arguments. Django's ORM does that, e.g.:You may want to accept nearly-arbitrary named arguments for a series of reasons -- and that's what the **kw form lets you do.The most common reason is to pass the arguments right on to some other function you're wrapping (decorators are one case of this, but FAR from the only one!) -- in this case, **kw loosens the coupling between wrapper and wrappee, as the wrapper doesn't have to know or care about all of the wrappee's arguments. Here's another, completely different reason:if all the names had to be known in advance, then obviously this approach just couldn't exist, right?  And btw, when applicable, I much prefer this way of making a dict whose keys are literal strings to:simply because the latter is quite punctuation-heavy and hence less readable.When none of the excellent reasons for accepting **kwargs applies, then don't accept it: it's as simple as that. IOW, if there's no good reason to allow the caller to pass extra named args with arbitrary names, don't allow that to happen -- just avoid putting a **kw form at the end of the function's signature in the def statement.As for using **kw in a call, that lets you put together the exact set of named arguments that you must pass, each with corresponding values, in a dict, independently of a single call point, then use that dict at the single calling point. Compare:to:Even with just two possibilities (and of the very simplest kind!), the lack of **kw is aleady making the second option absolutely untenable and intolerable -- just imagine how it plays out when there half a dozen possibilities, possibly in slightly richer interaction... without **kw, life would be absolute hell under such circumstances!Another reason you might want to use **kwargs (and *args) is if you're extending an existing method in a subclass. You want to pass all the existing arguments onto the superclass's method, but want to ensure that your class keeps working even if the signature changes in a future version:There are two common cases:First: You are wrapping another function which takes a number of keyword argument, but you are just going to pass them along:Second: You are willing to accept any keyword argument, for example, to set attributes on an object:**kwargs are good if you don't know in advance the name of the parameters. For example the dict constructor uses them to initialize the keys of the new dictionary. Here's an example, I used in CGI Python.  I created a class that took **kwargs to the __init__ function.  That allowed me to emulate the DOM on the server-side with classes:The only problem is that you can't do the following, because class is a Python keyword.The solution is to access the underlying dictionary.I'm not saying that this is a "correct" usage of the feature.  What I'm saying is that there are all kinds of unforseen ways in which features like this can be used.And here's another typical example:One example is implementing python-argument-binders, used like this:This is from the functools.partial python docs:  partial is 'relatively equivalent' to this impl:

How to disable a pep8 error in a specific file?

Flows

[How to disable a pep8 error in a specific file?](https://stackoverflow.com/questions/18444840/how-to-disable-a-pep8-error-in-a-specific-file)

I tried withorI thought the second would work but doesn't seems to work.Do you have an idea how I can handle this ?

2013-08-26 13:03:23Z

I tried withorI thought the second would work but doesn't seems to work.Do you have an idea how I can handle this ?As far as I know, you can't.

You can disable errors or warnings user wide, or per project. See the documentation.Instead, you can use the # noqa comment at the end of a line, to skip that particular line (see patch 136). Of course, that would skip all PEP8 errors.The main author argues against source file noise, so they suggested # pep8 comments don't get included.Note that there is also nopep8, which is the equivalent. noqa (which stands for No Quality Assurance was added in version 1.4.1 to support people running pyflakes next to pep8.Try putting # nopep8 at the end of the line (after two spaces). So if the line of code is:then to ignore the copious pep8 errors for that line it becomes:You can use --ignore flag to disable the error you mentioned abovefor multiple errorsFor more in depth knowledge of other flags you can scan through http://pep8.readthedocs.org/en/latest/intro.html Let me add something that was probably introduced after all the previous answers were posted.If you use Flake8, you can ignore a specific violation raised in a specific line, by adding at the end of the line, where F401 here is an example of an error code. For a list of all violations code, see http://flake8.pycqa.org/en/3.5.0/user/error-codes.html and https://pycodestyle.readthedocs.io/en/latest/intro.html#error-codesYou can also ignore all violations in an entire file by adding anywhere in the file.Reference: http://flake8.pycqa.org/en/3.5.0/user/violations.htmlYou can do that using Flake8 together with https://github.com/jayvdb/flake8-puttyIf you use Flake8 3.7.0+, you can ignore specific warnings for entire files using the --per-file-ignores option.Command-line usage:This can also be specified in a config file:You can do that with, for example, your setup configuration file (setup.cfg):

django modifying the request object

user2801567

[django modifying the request object](https://stackoverflow.com/questions/18930234/django-modifying-the-request-object)

I already have a django project and it logical like those:url:    URL?username=name&pwd=passwdview:but now we need encrypt the data. Then, the request become this:url:    URL?crypt=XXXXXXXXXX       (XXXXXXXX is encrypted str for "username=name&pwd=passwd")so I need modify every view function. But now I want decrypt in django middleware to prevent from modifying every view function.but when I modify request.GET, I recive error msg "This QueryDict instance is immutable". How can I modify it?

2013-09-21 07:48:57Z

I already have a django project and it logical like those:url:    URL?username=name&pwd=passwdview:but now we need encrypt the data. Then, the request become this:url:    URL?crypt=XXXXXXXXXX       (XXXXXXXX is encrypted str for "username=name&pwd=passwd")so I need modify every view function. But now I want decrypt in django middleware to prevent from modifying every view function.but when I modify request.GET, I recive error msg "This QueryDict instance is immutable". How can I modify it?django.http.QueryDict objects that are assigned to request.GET and request.POST are immutable.You can convert it to a mutable QueryDict instance by copying it:Afterwards you'll be able to modify the QueryDict:This has been purposefully designed so that none of the application components are allowed to edit the source request data, so even creating a immutable QueryDict again would break this design. I would still suggest that you follow the guidelines and assign additional request data directly on the request object in your middleware, despite the fact that it might cause you to edit your sources.Remove immutability:UpdateThe Django sanctioned way is: request.GET.copy().According to the docs:Nothing guarantees future Django versions will use _mutable. This has more chances to change than the copy() method. You shouldn't use GET to send the username and password, it's bad practice (since it shows the information on the URL bar, and might pose a security risk). Instead, use POST. Also, I'm guessing you're trying to authenticate your users, and it seems like you're doing too much work (creating a new middleware) to deal with something that is completely built in, to take the example from the docs:I myself really like using the login_required decorator, very simple to use. Hope that helpsyou need this.

Python/psycopg2 WHERE IN statement

Matt

[Python/psycopg2 WHERE IN statement](https://stackoverflow.com/questions/28117576/python-psycopg2-where-in-statement)

What is the correct method to have the list (countryList) be available via %s in the SQL statement?As it is now, it errors out after trying to run "WHERE country in (ARRAY[...])". Is there a way to do this other than through string manipulation?Thanks

2015-01-23 19:46:07Z

What is the correct method to have the list (countryList) be available via %s in the SQL statement?As it is now, it errors out after trying to run "WHERE country in (ARRAY[...])". Is there a way to do this other than through string manipulation?ThanksFor the IN operator, you want a tuple instead of list, and remove parentheses from the SQL string.During debugging you can check that the SQL is built correctly withTo expland on the answer a little and to address named parameters, and converting lists to tuples:You could use a python list directly as below. It acts like the IN operator in SQL and also handles a blank list without throwing any error.source:

http://initd.org/psycopg/docs/usage.html#lists-adaptation

Randint doesn't always follow uniform distribution

Tasos

[Randint doesn't always follow uniform distribution](https://stackoverflow.com/questions/41100287/randint-doesnt-always-follow-uniform-distribution)

I was playing around with the random library in Python to simulate a project I work and I found myself in a very strange position.Let's say that we have the following code in Python:The plot follows adiscrete uniformdistribution as it should.However, when I change the range from 1 to 110, the plot has several peaks.My impression is that the peaks are on 0,10,20,30,... but I am not able to explain it.Edit: The question was not similar with the proposed one as duplicate since the problem in my case was the seaborn library and the way I visualised the data.Edit 2: Following the suggestions on the answers, I tried to verify it by changing the seaborn library. Instead, using matplotlib both graphs were the same

2016-12-12 11:53:16Z

I was playing around with the random library in Python to simulate a project I work and I found myself in a very strange position.Let's say that we have the following code in Python:The plot follows adiscrete uniformdistribution as it should.However, when I change the range from 1 to 110, the plot has several peaks.My impression is that the peaks are on 0,10,20,30,... but I am not able to explain it.Edit: The question was not similar with the proposed one as duplicate since the problem in my case was the seaborn library and the way I visualised the data.Edit 2: Following the suggestions on the answers, I tried to verify it by changing the seaborn library. Instead, using matplotlib both graphs were the sameThe problem seems to be in your grapher, seaborn, not in randint().There are 50 bins in your seaborn distribution diagram, according to my count. It seems that seaborn is actually binning your returned randint() values in those bins, and there is no way to get an even spread of 110 values into 50 bins. Therefore you get those peaks where three values get put into a bin rather than the usual two values for the other bins. The values of your peaks confirm this: they are 50% higher than the other bars, as expected for 3 binned values rather than for 2.Another way for you to check this is to force seaborn to use 55 bins for these 110 values (or perhaps 10 bins or some other divisor of 110). If you still get the peaks, then you should worry about randint().To add to @RoryDaulton 's excellent answer, I ran randint(1:110), generating a frequency count and the converting it to an R-vector of counts like this:I then pasted this to an R-console, reconstructed the observations and used R's hist() on the result, obtaining this histogram (with superimposed density curve):As you can see, this confirms that the problem you observed isn't traceable to randint but is an artifact of sns.displot().

How do I document a module in Python?

Auron

[How do I document a module in Python?](https://stackoverflow.com/questions/44084/how-do-i-document-a-module-in-python)

That's it. If you want to document a function or a class, you put a string just after the definition. For instance:But what about a module? How can I document what a file.py does?

2008-09-04 16:06:48Z

That's it. If you want to document a function or a class, you put a string just after the definition. For instance:But what about a module? How can I document what a file.py does?For the packages, you can document it in __init__.py.

For the modules, you can add a docstring simply in the module file.All the information is here: http://www.python.org/dev/peps/pep-0257/Add your docstring as the first statement in the module.For packages, you can add your docstring to __init__.py.Here is an Example Google Style Python Docstrings on how module can be documented. Basically there is an information about a module, how to execute it and information about module level variables and list of ToDo items.You do it the exact same way.  Put a string in as the first statement in the module.It's easy, you just add a docstring at the top of the module.For PyPI Packages:If you add doc strings like this in your __init__.py file as seen belowThen you will receive this in everyday usage of the help function.help(<YOUR_PACKAGE>)Note, that my help DESCRIPTION is triggered by having that first docstring at the very top of the file.

Pandas: filling missing values by mean in each group

BlueFeet

[Pandas: filling missing values by mean in each group](https://stackoverflow.com/questions/19966018/pandas-filling-missing-values-by-mean-in-each-group)

This should be straightforward, but the closest thing I've found is this post:

pandas: Filling missing values within a group, and I still can't solve my problem....Suppose I have the following dataframeand I'd like to fill in "NaN" with mean value in each "name" group, i.e.I'm not sure where to go after:Thanks a bunch.

2013-11-13 22:43:25Z

This should be straightforward, but the closest thing I've found is this post:

pandas: Filling missing values within a group, and I still can't solve my problem....Suppose I have the following dataframeand I'd like to fill in "NaN" with mean value in each "name" group, i.e.I'm not sure where to go after:Thanks a bunch.One way would be to use transform:@DSM has IMO the right answer, but I'd like to share my generalization and optimization of the question: Multiple columns to group-by and having multiple value columns:... gives ...In this generalized case we would like to group by category and name, and impute only on value.This can be solved as follows:Notice the column list in the group-by clause, and that we select the value column right after the group-by. This makes the transformation only be run on that particular column. You could add it to the end, but then you will run it for all columns only to throw out all but one measure column at the end. A standard SQL query planner might have been able to optimize this, but pandas (0.19.2) doesn't seem to do this.Performance test by increasing the dataset by doing ...... confirms that this increases the speed proportional to how many columns you don't have to impute:On a final note you can generalize even further if you want to impute more than one column, but not all:This seems intuitive:The groupby + transform syntax maps the groupwise mean to the index of the original dataframe. This is roughly equivalent to @DSM's solution, but avoids the need to define an anonymous lambda function.I'd do it this wayMost of above answers involved using "groupby" and "transform" to fill the missing values.But i prefer using "groupby" with "apply" to fill the missing values which is more intuitive to me. Shortcut: Groupby + Apply/Lambda + Fillna + MeanThis solution still works if you want to group by multiple columns to replace missing values.The featured high ranked answer only works for a pandas Dataframe with only two columns. If you have a more columns case use instead: You can also use "dataframe or table_name".apply(lambda x: x.fillna(x.mean())).

Python, print all floats to 2 decimal places in output

Omar

[Python, print all floats to 2 decimal places in output](https://stackoverflow.com/questions/2075128/python-print-all-floats-to-2-decimal-places-in-output)

I need to output 4 different floats to two decimal places.This is what I have:Which is very unclean, and looks bad. Is there a way to make any float in that out put '%.2f'? Note: Using Python 2.6.

2010-01-15 22:15:31Z

I need to output 4 different floats to two decimal places.This is what I have:Which is very unclean, and looks bad. Is there a way to make any float in that out put '%.2f'? Note: Using Python 2.6.Well I would atleast clean it up as follows:Format String Syntax.https://docs.python.org/3/library/string.html#formatstringsThe output would be:If you just want to convert the values to nice looking strings do the following: Alternatively, you could also print out the units like you have in your question:The second way allows you to easily change the delimiter (tab, spaces, newlines, whatever) to suit your needs easily; the delimiter could also be a function argument instead of being hard-coded.Edit: To use your 'name = value' syntax simply change the element-wise operation within the list comprehension:If you are looking for readability, I believe that this is that code:I have just discovered the round function - it is in Python 2.7, not sure about 2.6. It takes a float and the number of dps as arguments, so round(22.55555, 2) gives the result 22.56. If what you want is to have the print operation automatically change floats to only show 2 decimal places, consider writing a function to replace 'print'.  For instance:Use fp() in place of print ...fp("PI is", 3.14159) ... instead of ... print "PI is", 3.14159Not directly in the way you want to write that, no. One of the design tenets of Python is "Explicit is better than implicit" (see import this). This means that it's better to describe what you want rather than having the output format depend on some global formatting setting or something.  You could of course format your code differently to make it look nicer:

How to get most informative features for scikit-learn classifiers?

tobigue

[How to get most informative features for scikit-learn classifiers?](https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers)

The classifiers in machine learning packages like liblinear and nltk offer a method show_most_informative_features(), which is really helpful for debugging features:My question is if something similar is implemented for the classifiers in scikit-learn. I searched the documentation, but couldn't find anything the like.If there is no such function yet, does somebody know a workaround how to get to those values?Thanks alot!

2012-06-20 09:36:19Z

The classifiers in machine learning packages like liblinear and nltk offer a method show_most_informative_features(), which is really helpful for debugging features:My question is if something similar is implemented for the classifiers in scikit-learn. I searched the documentation, but couldn't find anything the like.If there is no such function yet, does somebody know a workaround how to get to those values?Thanks alot!The classifiers themselves do not record feature names, they just see numeric arrays. However, if you extracted your features using a Vectorizer/CountVectorizer/TfidfVectorizer/DictVectorizer, and you are using a linear model (e.g. LinearSVC or Naive Bayes) then you can apply the same trick that the document classification example uses. Example (untested, may contain a bug or two):This is for multiclass classification; for the binary case, I think you should use clf.coef_[0] only. You may have to sort the class_labels.With the help of larsmans code I came up with this code for the binary case:To add an update, RandomForestClassifier now supports the .feature_importances_ attribute. This attribute tells you how much of the observed variance is explained by that feature. Obviously, the sum of all these values must be <= 1. I find this attribute very useful when performing feature engineering. Thanks to the scikit-learn team and contributors for implementing this!edit: This works for both RandomForest and GradientBoosting. So RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier and GradientBoostingRegressor all support this. We've recently released a library (https://github.com/TeamHG-Memex/eli5) which allows to do that: it handles variuos classifiers from scikit-learn, binary / multiclass cases, allows to highlight text according to feature values, integrates with IPython, etc.I actually had to find out Feature Importance on my NaiveBayes classifier and although I used the above functions, I was not able to get feature importance based on classes. I went through the scikit-learn's documentation and tweaked the above functions a bit to find it working for my problem. Hope it helps you too!You can also do something like this to create a graph of importance features by order:RandomForestClassifier does not yet have a coef_ attrubute, but it will in the 0.17 release, I think. However, see the RandomForestClassifierWithCoef class in Recursive feature elimination on Random Forest using scikit-learn. This may give you some ideas to work around the limitation above.Not exactly what you are looking for, but a quick way to get the largest magnitude coefficients (assuming a pandas dataframe columns are your feature names): You trained the model like: Get the 10 largest negative coefficient values (or change to reverse=True for largest positive) like: First make a list, I give this list the name label.  After that extracting all features name and column name I add in label list. Here I use naive bayes model. In naive bayes model, feature_log_prob_ give probability of features.

Debugging a Flask app running in Gunicorn

mafrosis

[Debugging a Flask app running in Gunicorn](https://stackoverflow.com/questions/8950674/debugging-a-flask-app-running-in-gunicorn)

I've been working on a new dev platform using nginx/gunicorn and Flask for my application.Ops-wise, everything works fine - the issue I'm having is with debugging the Flask layer. When there's an error in my code, I just get a straight 500 error returned to the browser and nothing shows up on the console or in my logs.I've tried many different configs/options.. I guess I must be missing something obvious.My gunicorn.conf:An example of some Flask code that borks- testserver.py:And finally, the command to run the flask app in gunicorn:Thanks y'all

2012-01-21 04:33:09Z

I've been working on a new dev platform using nginx/gunicorn and Flask for my application.Ops-wise, everything works fine - the issue I'm having is with debugging the Flask layer. When there's an error in my code, I just get a straight 500 error returned to the browser and nothing shows up on the console or in my logs.I've tried many different configs/options.. I guess I must be missing something obvious.My gunicorn.conf:An example of some Flask code that borks- testserver.py:And finally, the command to run the flask app in gunicorn:Thanks y'allThe acception solution doesn't work for me.Gunicorn is a pre-forking environment and apparently the Flask debugger doesn't work in a forking environment. Even if you set app.debug = True, you will still only get an empty page with the message Internal Server Error if you run with gunicorn testserver:app. The best you can do with gunicorn is to run it with gunicorn --debug testserver:app. That gives you the trace in addition to the Internal Server Error message. However, this is just the same text trace that you see in the terminal and not the Flask debugger.Adding the if __name__ ... section to the testserver.py and running python testserver.py to start the server in development gets you the Flask debugger. In other words, don't use gunicorn in development if you want the Flask debugger.Personally I still like to use foreman start, instead of python testserver.py since it sets up all the env variables for me. To get this to work:Also, dont forget to change the app.config['DEBUG']... line in testserver.py to something that won't run Flask in debug mode in production.The Flask config is entirely separate from gunicorn's. Following the Flask documentation on config files, a good solution would be change my source to this:And in config.py:For Heroku users, there is a simpler solution than creating a bin/web script like suggested by Nick.Instead of foreman start, just use foreman run python app.py  if you want to debug your application in development.I had similiar problem when running flask under gunicorn I didn't see stacktraces in browser (had to look at logs every time). Setting DEBUG, FLASK_DEBUG, or anything mentioned on this page didn't work. Finally I did this:Note evalex is disabled because interactive debbugging won't work with forking (gunicorn).I used this:Try setting the debug flag on the run command like so  and keep the DEBUG = True in your Flask application. There must be a reason why your debug option is not being applied from the config file but for now the above note should get you going.

Download and save PDF file with Python requests module

Jim

[Download and save PDF file with Python requests module](https://stackoverflow.com/questions/34503412/download-and-save-pdf-file-with-python-requests-module)

I am trying to download a PDF file from a website and save it to disk. My attempts either fail with encoding errors or result in blank PDFs.I know it is a codec problem of some kind but I can't seem to get it to work.

2015-12-29 02:00:58Z

I am trying to download a PDF file from a website and save it to disk. My attempts either fail with encoding errors or result in blank PDFs.I know it is a codec problem of some kind but I can't seem to get it to work.You should use response.content in this case:From the document:So that means: response.text return the output as a string object, use it when you're downloading a text file. Such as HTML file, etc.And response.content return the output as bytes object, use it when you're downloading a binary file. Such as PDF file, audio file, image, etc.You can also use response.raw instead. However, use it when the file which you're about to download is large. Below is a basic example which you can also find in the document:chunk_size is the chunk size which you want to use. If you set it as 2000, then requests will download that file the first 2000 bytes, write them into the file, and do this again, again and again, unless it finished.So this can save your RAM. But I'd prefer use response.content instead in this case since your file is small. As you can see use response.raw is complex.Relates: In Python 3, I find pathlib is the easiest way to do this. Request's response.content marries up nicely with pathlib's _write_bytes_.You can use urllib:  Please note I'm a beginner. If My solution is wrong, please feel free to correct and/or let me know. I may learn something new too.My solution:Change the downloadPath accordingly  to where you want your file to be saved. Feel free to use the absolute path too for your usage. Save the below as downloadFile.py.Usage: python downloadFile.py url-of-the-file-to-download new-file-name.extensionRemember to add an extension!Example usage: python downloadFile.py http://www.google.co.uk google.htmlregarding Kevin answer to write in a folder tmp, it should be like this:he forgot . before the address and of-course your folder tmp should have been created already

When import docx in python3.3 I have error ImportError: No module named 'exceptions'

user3472559

[When import docx in python3.3 I have error ImportError: No module named 'exceptions'](https://stackoverflow.com/questions/22765313/when-import-docx-in-python3-3-i-have-error-importerror-no-module-named-excepti)

when I import docx I have this error:How to fix this error (python3.3, docx 0.2.4)?

2014-03-31 15:11:47Z

when I import docx I have this error:How to fix this error (python3.3, docx 0.2.4)?If you are using python 3x don't do pip install docx instead go forit is compatible with python 3x official Document: https://pypi.org/project/python-docx/copied fromYou may be install docx, not python-docx You can see this for install python-docxhttp://python-docx.readthedocs.io/en/latest/user/install.html#installThe problem, as was noted previously in comments, is the docx module was not compatible with Python 3.  It was fixed in this pull-request on github: https://github.com/mikemaccana/python-docx/pull/67Since the exception is now built-in, the solution is to not import it.I had the same problem, but pip install python-docx worked for me, I'm using python 3.7.1

Get folder name of the file in Python

Vasily

[Get folder name of the file in Python](https://stackoverflow.com/questions/33372054/get-folder-name-of-the-file-in-python)

In Python what command should I use to get the name of the folder which contains the file I'm working with?"C:\folder1\folder2\filename.xml"Here "folder2" is what I want to get.The only thing I've come up with is to use os.path.split twice:folderName = os.path.split(os.path.split("C:\folder1\folder2\filename.xml")[0])[1]Is there any better way to do it?

2015-10-27 15:28:21Z

In Python what command should I use to get the name of the folder which contains the file I'm working with?"C:\folder1\folder2\filename.xml"Here "folder2" is what I want to get.The only thing I've come up with is to use os.path.split twice:folderName = os.path.split(os.path.split("C:\folder1\folder2\filename.xml")[0])[1]Is there any better way to do it?You can use dirname:And given the full path, then you can split normally to get the last portion of the path. For example, by using basename:All together:You are looking to use dirname. If you only want that one directory, you can use os.path.basename,When put all together it looks like this: That should get you "other_sub_dir"The following is not the ideal approach, but I originally proposed,using os.path.split, and simply get the last item. which would look like this:os.path.dirname is what you are looking for -Make sure you prepend r to the string so that its considered as a raw string.Demo -If you just want folder2 , you can use os.path.basename with the above, Example -Demo -this is pretty old, but if you are using Python 3.4 or above use PathLib.

Overriding+=in Python? (__iadd__() method)

Evan Fosmark

[Overriding+=in Python? (__iadd__() method)](https://stackoverflow.com/questions/1047021/overriding-in-python-iadd-method)

Is it possible to override += in Python?

2009-06-26 02:04:24Z

Is it possible to override += in Python?Yes, override the __iadd__ method. Example:In addition to what's correctly given in answers above, it is worth explicitly clarifying that when __iadd__ is overriden, the x += y operation does NOT end with the end of __iadd__ method.Instead, it ends with x = x.__iadd__(y). In other words, Python assigns the return value of your __iadd__ implementation to the object you're "adding to", AFTER the implementation completes.This means it is possible to mutate the left side of the x += y operation so that the final implicit step fails. Consider what can happen when you are adding to something that's within a list:>>> x[1] += y # x has two itemsNow, if your __iadd__ implementation (a method of an object at x[1]) erroneously or on purpose removes the first item (x[0]) from the beginning of the list, Python will then run your __iadd__ method) & try to assign its return value to x[1]. Which will no longer exist (it will be at x[0]), resulting in an ndexError. Or, if your __iadd__ inserts something to beginning of x of the above example, your object will be at x[2], not x[1], and whatever was earlier at x[0] will now be at x[1]and be assigned the return value of the __iadd__ invocation.Unless one understands what's happening, resulting bugs can be a nightmare to fix.In addition to overloading __iadd__ (remember to return self!), you can also fallback on __add__, as x += y will work like x = x + y. (This is one of the pitfalls of the += operator.)It even trips up experts:What values do you expect x.id, y.id, and Resource.class_counter to have?http://docs.python.org/reference/datamodel.html#emulating-numeric-types

How to set env variable in Jupyter notebook

Ehab AlBadawy

[How to set env variable in Jupyter notebook](https://stackoverflow.com/questions/37890898/how-to-set-env-variable-in-jupyter-notebook)

I've a problem that Jupyter can't see env variable in bashrc file, is there a way to load these variables in jupyter or add custome variable to it?

2016-06-17 21:53:15Z

I've a problem that Jupyter can't see env variable in bashrc file, is there a way to load these variables in jupyter or add custome variable to it?To set an env variable in a jupyter notebook, just use a % magic commands, either %env or %set_env, e.g., %env MY_VAR=MY_VALUE or %env MY_VAR MY_VALUE.  (Use %env by itself to print out current environmental variables.)See: http://ipython.readthedocs.io/en/stable/interactive/magics.htmlYou can setup environment variables in your code as follows:This if of course a temporary fix, to get a permanent one, you probably need to export the variables into your ~.profile, more information can be found hereMy solution is useful if you need the same environment variables every time you start a jupyter kernel, especially if you have multiple sets of environment variables for different tasks.To create a new ipython kernel with your environment variables, do the following:Your kernel json could look like this (I did not modify anything from the installed kernel.json except display_name and env):If you're using Python, you can define your environment variables in a .env file and load them from within a Jupyter notebook using python-dotenv.Install python-dotenv:Load the .env file in a Jupyter notebook:If you need the variable set before you're starting the notebook, the only solution which worked for me was env VARIABLE=$VARIABLE jupyter notebook with export VARIABLE=value in .bashrc. In my case tensorflow needs the exported variable for successful importing it in a notebook.A gotcha I ran into: The following two commands are equivalent. Note the first cannot use quotes. Somewhat counterintuitively, quoting the string when using %env VAR ... will result in the quotes being included as part of the variable's value, which is probably not what you want. andIf you are using systemd I just found out that you seem to have to add them to the systemd unit file. This on Ubuntu 16. Putting them into the .profile and .bashrc (even the /etc/profile) resulted in the ENV Vars not being available in the juypter notebooks.I had to edit:and put in the variable i wanted to read in the unit file like:and only then could I read it from within juypter notebook.A related (short-term) solution is to store your environment variables in a single file, with a predictable format, that can be sourced when starting a terminal and/or read into the notebook. For example, I have a file, .env, that has my environment variable definitions in the format VARIABLE_NAME=VARIABLE_VALUE (no blank lines or extra spaces). You can source this file in the .bashrc or .bash_profile files when beginning a new terminal session and you can read this into a notebook with something like,I used a relative path to show that this .env file can live anywhere and be referenced relative to the directory containing the notebook file. This also has the advantage of not displaying the variable values within your code anywhere.

Importing modules in Python - best practice

John

[Importing modules in Python - best practice](https://stackoverflow.com/questions/9916878/importing-modules-in-python-best-practice)

I am new to Python as I want to expand skills that I learned using R.

In R I tend to load a bunch of libraries, sometimes resulting in function name conflicts.What is best practice in Python. I have seen some specific variations that I do not see a difference betweenimport pandas, from pandas import *, and  from pandas import DataFrameWhat are the differences between the first two and should I just import what I need.

Also, what would be the worst consequences for someone making small programs to process data and compute simple statistics.UPDATEI found this excellent guide. It explains everything.

2012-03-28 22:58:43Z

I am new to Python as I want to expand skills that I learned using R.

In R I tend to load a bunch of libraries, sometimes resulting in function name conflicts.What is best practice in Python. I have seen some specific variations that I do not see a difference betweenimport pandas, from pandas import *, and  from pandas import DataFrameWhat are the differences between the first two and should I just import what I need.

Also, what would be the worst consequences for someone making small programs to process data and compute simple statistics.UPDATEI found this excellent guide. It explains everything.import pandas imports the pandas module under the pandas namespace, so you would need to call objects within pandas using pandas.foo.from pandas import * imports all objects from the pandas module into your current namespace, so you would call objects within pandas using only foo. Keep in mind this could have unexepcted consequences if there are any naming conflicts between your current namespace and the pandas namespace. from pandas import DataFrame is the same as above, but only imports DataFrame (instead of everything) into your current namespace.In my opinion the first is generally best practice, as it keeps the different modules nicely compartmentalized in your code.When reading other people's code (and those people use very

different importing styles), I noticed the following problems with

each of the styles:import modulewithaverylongname will clutter the code further down 

with the long module name (e.g. concurrent.futures or django.contrib.auth.backends) and decrease readability in those places.from module import * gives me no chance to see syntactically that, 

for instance, classA and classB come from the same module and 

have a lot to do with each other. 

It makes reading the code hard. 

(That names from such an import

may shadow names from an earlier import is the least part of that problem.)from module import classA, classB, functionC, constantD, functionE 

overloads my short-term memory with too many names 

that I mentally need to assign to module in order to 

coherently understand the code.import modulewithaverylongname as mwvln is sometimes insufficiently

mnemonic to me.Based on the above observations, I have developed the following

style in my own code:import module is the preferred style if the module name is short

as for example most of the packages in the standard library.

It is also the preferred style if I need to use names from the module in

only two or three places in my own module; 

clarity trumps brevity then ("Readability counts").import longername as ln is the preferred style in almost every

other case. 

For instance, I might import django.contrib.auth.backends as dj_abe.

By definition of criterion 1 above, the abbreviation will be used

frequently and is therefore sufficiently easy to memorize.Only these two styles are fully pythonic as per the

"Explicit is better than implicit." rule.from module import xx still occurs sometimes in my code.

I use it in cases where even the as format appears exaggerated,

the most famous example being from datetime import datetime.In general it is better to do explicit imports.

As in:Or:Another option in Python, when you have conflicting names, is import x as y:Here are some recommendations from PEP8 Style Guide.Some recommendations about lazy imports from python speed performance tips.the given below is a scenario explained at the page,essentially equals following three statementsThat's it, that is it all.They are all suitable in different contexts (which is why they are all available). There's no deep guiding principle, other than generic motherhood statements around clarity, maintainability and simplicity. Some examples from my own code:

matplotlib bar graph black - how do I remove bar borders

user1893354

[matplotlib bar graph black - how do I remove bar borders](https://stackoverflow.com/questions/15904042/matplotlib-bar-graph-black-how-do-i-remove-bar-borders)

I'm using pyplot.bar but I'm plotting so many points that the color of the bars is always black.  This is because the borders of the bars are black and there are so many of them that they are all squished together so that all you see is the borders (black).  Is there a way to remove the bar borders so that I can see the intended color?

2013-04-09 14:02:08Z

I'm using pyplot.bar but I'm plotting so many points that the color of the bars is always black.  This is because the borders of the bars are black and there are so many of them that they are all squished together so that all you see is the borders (black).  Is there a way to remove the bar borders so that I can see the intended color?Set the edgecolor to "none": bar(..., edgecolor = "none")Another option is to set edgecolor to be the intended color in your call to bar:

How to split but ignore separators in quoted strings, in python?

Sylvain

[How to split but ignore separators in quoted strings, in python?](https://stackoverflow.com/questions/2785755/how-to-split-but-ignore-separators-in-quoted-strings-in-python)

I need to split a string like this, on semicolons. But I don't want to split on semicolons that are inside of a string (' or "). I'm not parsing a file; just a simple string with no line breaks.part 1;"this is ; part 2;";'this is ; part 3';part 4;this "is ; part" 5Result should be:I suppose this can be done with a regex but if not; I'm open to another approach.

2010-05-07 02:13:05Z

I need to split a string like this, on semicolons. But I don't want to split on semicolons that are inside of a string (' or "). I'm not parsing a file; just a simple string with no line breaks.part 1;"this is ; part 2;";'this is ; part 3';part 4;this "is ; part" 5Result should be:I suppose this can be done with a regex but if not; I'm open to another approach.Most of the answers seem massively over complicated. You don't need back references. You don't need to depend on whether or not re.findall gives overlapping matches.  Given that the input cannot be parsed with the csv module so a regular expression is pretty well the only way to go, all you need is to call re.split with a pattern that matches a field.Note that it is much easier here to match a field than it is to match a separator:and the output is:As Jean-Luc Nacif Coelho correctly points out this won't handle empty groups correctly. Depending on the situation that may or may not matter. If it does matter it may be possible to handle it by, for example, replacing ';;' with ';<marker>;' where <marker> would have to be some string (without semicolons) that you know does not appear in the data before the split. Also you need to restore the data after:However this is a kludge. Any better suggestions?Each time it finds a semicolon, the lookahead scans the entire remaining string, making sure there's an even number of single-quotes and an even number of double-quotes.  (Single-quotes inside double-quoted fields, or vice-versa, are ignored.)  If the lookahead succeeds, the semicolon is a delimiter.Unlike Duncan's solution, which matches the fields rather than the delimiters, this one has no problem with empty fields.  (Not even the last one: unlike many other split implementations, Python's does not automatically discard trailing empty fields.)Here is an annotated pyparsing approach:givingBy using pyparsing's provided quotedString, you also get support for escaped quotes.You also were unclear how to handle leading whitespace before or after a semicolon delimiter, and none of your fields in your sample text has any.  Pyparsing would parse "a; b ; c" as:You appears to have a semi-colon seperated string. Why not use the csv module to do all the hard work?Off the top of my head, this should workThis should give you something like

("part 1", "this is ; part 2;", 'this is ; part 3', "part 4", "this \"is ; part\" 5")Edit:

Unfortunately, this doesn't quite work, (even if you do use StringIO, as I intended), due to the mixed string quotes (both single and double). What you actually get is['part 1', 'this is ; part 2;', "'this is ", " part 3'", 'part 4', 'this "is ', ' part" 5'].If you can change the data to only contain single or double quotes at the appropriate places, it should work fine, but that sort of negates the question a bit.While it could be done with PCRE via lookaheads/behinds/backreferences, it's not really actually a task that regex is designed for due to the need to match balanced pairs of quotes.Instead it's probably best to just make a mini state machine and parse through the string like that.As it turns out, due to the handy additional feature of Python re.findall which guarantees non-overlapping matches, this can be more straightforward to do with a regex in Python than it might otherwise be. See comments for details.However, if you're curious about what a non-regex implementation might look like:we can create a function of its ownThis regex will do that: (?:^|;)("(?:[^"]+|"")*"|[^;]*)since you do not have '\n', use it to replace any ';' that is not in a quote stringEven though I'm certain there is a clean regex solution (so far I like @noiflection's answer), here is a quick-and-dirty non-regex answer.(I've never put together something of this sort, feel free to critique my form!)My approach is to replace all non-quoted occurrences of the semi-colon with another character which will never appear in the text, then split on that character. The following code uses the re.sub function with a function argument to search and replace all occurrences of a srch string, not enclosed in single or double quotes or parens, brackets or braces, with a repl string:  If you don't care about the bracketed characters, you can simplify this code a lot.

Say you wanted to use a pipe or vertical bar as the substitute character, you would do:  BTW, this uses nonlocal from Python 3.1, change it to global if you need to. A generalized solution:Outputs:This solution:Although the topic is old and previous answers are working well, I propose my own implementation of the split function in python.This works fine if you don't need to process large number of strings and is easily customizable.Here's my function:So you can run:result:The advantage is that this function works with empty fields and with any number of separators in the string.Hope this helps!Instead of splitting on a separator pattern, just capture whatever you need:This seemed to me an semi-elegant solution.I choose to match if there was an opening quote and wait it to close, and the match an ending semicolon. each "part" you want to match needs to end in semicolon.

so this match things like this : Code:you may have to do some postprocessing to res, but it contains what you want.

Python float to int conversion

B. Richard

[Python float to int conversion](https://stackoverflow.com/questions/6569528/python-float-to-int-conversion)

Basically, I'm converting a float to an int, but I don't always have the expected value.Here's the code I'm executing:x = 2.51And here's the result (first value is the result of the operation, second value is int() of the same operation):2.51 and 4.02 are the only values that lead to that strange behaviour on the 2.50 -> 5.00 range. Every other two digits value in that range converts to int without any problem when given the same operations.So, what am I missing that leads to those results? I'm using Python 2.7.2 by the way.

2011-07-04 09:25:19Z

Basically, I'm converting a float to an int, but I don't always have the expected value.Here's the code I'm executing:x = 2.51And here's the result (first value is the result of the operation, second value is int() of the same operation):2.51 and 4.02 are the only values that lead to that strange behaviour on the 2.50 -> 5.00 range. Every other two digits value in that range converts to int without any problem when given the same operations.So, what am I missing that leads to those results? I'm using Python 2.7.2 by the way.The int() function simply truncates the number at the decimal point, giving 250. Use to get 251 as an integer. In general, floating point numbers cannot be represented exactly. One should therefore be careful of round-off errors. As mentioned, this is not a Python-specific problem. It's a recurring problem in all computer languages.What Every Computer Scientist Should Know About Floating-Point ArithmeticFloating-point numbers cannot represent all the numbers.  In particular, 2.51 cannot be represented by a floating-point number, and is represented by a number very close to it:If you use int, which truncates the numbers, you get:Have a look at the Decimal type.Languages that use binary floating point representations (Python is one) cannot represent all fractional values exactly. If the result of your calculation is 250.99999999999 (and it might be), then taking the integer part will result in 250.A canonical article on this topic is What Every Computer Scientist Should Know About Floating-Point Arithmetic.the floating point numbers are inaccurate. in this case, it is 250.99999999999999, which is really close to 251, but int() truncates the decimal part, in this case 250.you should take a look at the Decimal module or maybe if you have to do a lot of calculation at the mpmath library http://code.google.com/p/mpmath/ :),int converts by truncation, as has been mentioned by others. This can result in the answer being one different than expected. One way around this is to check if the result is 'close enough' to an integer and adjust accordingly, otherwise the usual conversion. This is assuming you don't get too much roundoff and calculation error, which is a separate issue. For example:This function will adjust for off-by-one errors for near integers. The mpmath library does something similar for floating point numbers that are close to integers.

Fail to get data on using read() of StringIO in python

raj

[Fail to get data on using read() of StringIO in python](https://stackoverflow.com/questions/10265593/fail-to-get-data-on-using-read-of-stringio-in-python)

Using Python2.7 version. Below is my sample code.in the above program, read() returns me nothing where as getvalue() returns me "hello". Can anyone help me out in fixing the issue? I need read() because my following code involves reading "n" bytes.

2012-04-22 05:55:02Z

Using Python2.7 version. Below is my sample code.in the above program, read() returns me nothing where as getvalue() returns me "hello". Can anyone help me out in fixing the issue? I need read() because my following code involves reading "n" bytes.You need to reset the buffer position to the beginning. You can do this by doing buff.seek(0). Every time you read or write to the buffer, the position is advanced by one. Say you start with an empty buffer.The buffer value is "", the buffer pos is 0.

You do buff.write("hello"). Obviously the buffer value is now hello. The buffer position, however, is now 5. When you call read(), there is nothing past position 5 to read! So it returns an empty string. or

Boolean operators vs Bitwise operators

Jiew Meng

[Boolean operators vs Bitwise operators](https://stackoverflow.com/questions/3845018/boolean-operators-vs-bitwise-operators)

I am confused as to when I should use Boolean vs bitwise operatorsCould someone enlighten me as to when do i use each and when will using one over the other affect my results? 

2010-10-02 08:52:03Z

I am confused as to when I should use Boolean vs bitwise operatorsCould someone enlighten me as to when do i use each and when will using one over the other affect my results? Here are a couple of guidelines:The short-circuiting behaviour is useful in expressions like this:This would not work correctly with the bitwise & operator because both sides would always be evaluated, giving AttributeError: 'NoneType' object has no attribute 'foo'. When you use the boolean andoperator the second expression is not evaluated when the first is False. Similarly or does not evaluate the second argument if the first is True.In theory, and and or come straight from boolean logic (and therefore operate on two booleans to produce a boolean), while & and | apply the boolean and/or to the individual bits of integers. There are a lot lot of questions here on how the latter work exactly.Here are practical differences that potentially affect your results:But even when e.g. a_boolean & another_boolean would work identically, the right solution is using and - simply because and and or are associated with boolean expression and condition while & and | stand for bit twiddling.Here's a further difference, which had me puzzled for a while just now: because & (and other bitwise operators) have a higher precedence than and (and other boolean operators) the following expressions evaluate to different values:versusTo wit, the first yields False as it is equivalent to 0 < (1 & 0) < 2, hence 0 < 0 < 2, hence 0 < 0 and 0 < 2.If you are trying to do element-wise boolean operations in numpy, the answer is somewhat different.  You can use & and | for element-wise boolean operations, but and and or will return value error.To be on the safe side, you can use the numpy logic functions.Boolean operation are logical operations.Bitwise operations are operations on binary bits.Bitwise operations:The operations:Some of the uses of bitwise operations:1) Setting and Clearing BitsBoolean operations:The hint is in the name:While it is possible and indeed sometimes desirable (typically for efficiency reasons) to perform logical operations with bitwise operators, you should generally avoid them for such purposes to prevent subtle bugs and unwanted side effects.If you need to manipulate bits, then the bitwise operators are purpose built. The fun book: Hackers Delight contains some cool and genuinely useful examples of what can be achieved with bit-twiddling.The general rule is to use the appropriate operator for the existing operands. Use boolean (logical) operators with boolean operands, and bitwise operators with (wider) integral operands (note: False is equivalent to 0, and True to 1). The only "tricky" scenario is applying boolean operators to non boolean operands. Let's take a simple example, as described in [SO]: Python - Differences between 'and' and '&' [duplicate]: 5 & 7 vs. 5 and 7.For the bitwise and (&), things are pretty straightforward:For the logical and, here's what [Python 3]: Boolean operations states (emphasis is mine):Example:Of course, the same applies for | vs. or.Boolean 'and' vs. Bitwise '&':Pseudo-code/Python helped me understand the difference between these:Logical Operationsare usually used for conditional statements. For example:if a==2 and b >10 then

/*Do something...*/

endif

It means if both conditions((a==2) (b>10))  are true at the same time then conditional statement body can be executed.Bitwise OperationsBitwise operations can be used for data manipulation and extraction. For example, if you want to extract four LSB(Least Significant Bits) of an integer, you can do this:Extraction:poo & 0x000FMasking:poo | 0xFFF0

Python/BeautifulSoup - how to remove all tags from an element?

Daniele B

[Python/BeautifulSoup - how to remove all tags from an element?](https://stackoverflow.com/questions/16206380/python-beautifulsoup-how-to-remove-all-tags-from-an-element)

How can I simply strip all tags from an element I find in BeautifulSoup?

2013-04-25 04:26:49Z

How can I simply strip all tags from an element I find in BeautifulSoup?With BeautifulStoneSoup gone in bs4, it's even simpler in Python3why has no answer I've seen mentioned anything about the unwrap method? Or, even easier, the get_text methodhttp://www.crummy.com/software/BeautifulSoup/bs4/doc/#unwrap

http://www.crummy.com/software/BeautifulSoup/bs4/doc/#get-textUse get_text(), it returns all the text in a document or beneath a tag, as a single Unicode string.For instance, remove all different script tags from the following text:The expected result is:Here is the source code:You can use the decompose method in bs4:it looks like this is the way to do! as simple as thatwith this line you are joining together the all text parts within the current elementHere is the source code: you can get the text which is exactly in the URL

In the Django admin interface, is there a way to duplicate an item?

sesh

[In the Django admin interface, is there a way to duplicate an item?](https://stackoverflow.com/questions/180809/in-the-django-admin-interface-is-there-a-way-to-duplicate-an-item)

Just wondering if there is an easy way to add the functionality to duplicate an existing listing in the admin interface?In data entry we have run into a situation where a lot of items share generic data with another item, and to save time it would be very nice to quickly duplicate an existing listing and only alter the changed data. Using a better model structure would be one way of reducing the duplication of the data, but there may be situation where the duplicated data needs to be changed on an individual basis in the future.

2008-10-07 23:23:11Z

Just wondering if there is an easy way to add the functionality to duplicate an existing listing in the admin interface?In data entry we have run into a situation where a lot of items share generic data with another item, and to save time it would be very nice to quickly duplicate an existing listing and only alter the changed data. Using a better model structure would be one way of reducing the duplication of the data, but there may be situation where the duplicated data needs to be changed on an individual basis in the future.You can save as by just enabling adding this to your ModelAdmin:This replaces the "Save and add another" button with a "Save as" button.  "Save as" means the object will be saved as a new object (with a new ID), rather than the old object.There's a better (but not built-in) solution here:https://github.com/RealGeeks/django-modelcloneFrom their README:You can also apply this method: https://stackoverflow.com/a/4054256/7995920In my case, with unique constraint in the 'name' field, this action works, and can be requested from any form:

how to read a long multiline string line by line in python

DKean

[how to read a long multiline string line by line in python](https://stackoverflow.com/questions/15422144/how-to-read-a-long-multiline-string-line-by-line-in-python)

I have a wallop of a string with many lines.  How do I read the lines one by one with a for clause?  Here is what I am trying to do and I get an error on the textData var referenced in the for line in textData line.The textData variable does exist, I print it before going down, but I think that the pre-compiler is kicking up the error.TIADennis 

2013-03-14 23:31:07Z

I have a wallop of a string with many lines.  How do I read the lines one by one with a for clause?  Here is what I am trying to do and I get an error on the textData var referenced in the for line in textData line.The textData variable does exist, I print it before going down, but I think that the pre-compiler is kicking up the error.TIADennis What about using .splitlines()?by splitting with newlines.if you iterate over a string, you are iterating char by char in that string, not by line.This answer fails in a couple of edge cases (see comments). The accepted solution above will handle these. str.splitlines() is the way to go. I will leave this answer nevertheless as reference.Old (incorrect) answer:

Python - TypeError: 'int' object is not iterable

grepit

[Python - TypeError: 'int' object is not iterable](https://stackoverflow.com/questions/19523563/python-typeerror-int-object-is-not-iterable)

Here's my code:I am looking to make a loop so that for each element in the list, it will get broken down into each of it's characters. For example, say the number 137 was in the list then it would be turned into [1,3,7]. Then I want to add these numbers together (I haven't started that bit yet but I have some idea of how to do it).However, I keep getting the error messagewhen I try and run this.What am I doing wrong?

2013-10-22 16:37:54Z

Here's my code:I am looking to make a loop so that for each element in the list, it will get broken down into each of it's characters. For example, say the number 137 was in the list then it would be turned into [1,3,7]. Then I want to add these numbers together (I haven't started that bit yet but I have some idea of how to do it).However, I keep getting the error messagewhen I try and run this.What am I doing wrong?Your problem is with this line:It tries to take cow[n], which returns an integer, and make it a list.  This doesn't work, as demonstrated below:Perhaps you meant to put cow[n] inside a list:See a demonstration below:Also, I wanted to address two things:To split up the digits and then add them like you want, I would first make the number a string.  Then, since strings are iterable, you can use sum:This is very simple you are trying to convert an integer to a list object !!! of course it will fail and it should ...To demonstrate/prove this to you by using the example you provided ...just use type function for each case as below and the results will speak for itself !

Get a list/tuple/dict of the arguments passed to a function?

Phillip B Oldham

[Get a list/tuple/dict of the arguments passed to a function?](https://stackoverflow.com/questions/2521901/get-a-list-tuple-dict-of-the-arguments-passed-to-a-function)

Given the following function:How would one obtain a list/tuple/dict/etc of the arguments passed in, without having to build the structure myself?Specifically, I'm looking for Python's version of JavaScript's arguments keyword or PHP's func_get_args() method.What I'm not looking for is a solution using *args or **kwargs; I need to specify the argument names in the function definition (to ensure they're being passed in) but within the function I want to work with them in a list- or dict-style structure.

2010-03-26 08:30:54Z

Given the following function:How would one obtain a list/tuple/dict/etc of the arguments passed in, without having to build the structure myself?Specifically, I'm looking for Python's version of JavaScript's arguments keyword or PHP's func_get_args() method.What I'm not looking for is a solution using *args or **kwargs; I need to specify the argument names in the function definition (to ensure they're being passed in) but within the function I want to work with them in a list- or dict-style structure.You can use locals() to get a dict of the local variables in your function, like this:This is a bit hackish, however, as locals() returns all variables in the local scope, not only the arguments passed to the function, so if you don't call it at the very top of the function the result might contain more information than you want:I would rather construct a dict or list of the variables you need at the top of your function, as suggested in the other answers.  It's more explicit and communicates the intent of your code in a more clear way, IMHO.You can use the inspect module:This is a duplicate of this and this.I would use *args or **kwargs and throw an exception if the arguments are not as expectedIf you want to have the same errors than the ones checked by python you can do something likeusing with somethin like From the accepted answer from a duplicate (older??) question https://stackoverflow.com/a/582206/1136458 :One solution, using decorators, is here.You've specified the parameters in the header?Why don't you simply use that same info in the body?What have I missed?You can create a list out of them using:You can easily create a tuple out of them using:

list.index() function for Python that doesn't throw exception when nothing found

Yarin

[list.index() function for Python that doesn't throw exception when nothing found](https://stackoverflow.com/questions/8197323/list-index-function-for-python-that-doesnt-throw-exception-when-nothing-found)

Python's list.index(x) throws an exception if the item doesn't exist. Is there a better way to do this that doesn't require handling exceptions?

2011-11-19 21:03:01Z

Python's list.index(x) throws an exception if the item doesn't exist. Is there a better way to do this that doesn't require handling exceptions?If you don't care where the matching element is, then use:If you do care, then use a LBYL style with a conditional expression:implement your own index for list?So, you can use list, and with your index2, return what you want in case of error.You can use it like this:Write a function that does what you need:If you only need to know whether the item exists, but not the index, you can use in:Yes, there is. You can eg. do something similar to this:which works like that:So, basically, test() will return index of the element (second parameter) within given list (first parameter), unless it has not been found (in this case it will return None, but it can be anything you find suitable).TL;DR: Exceptions are your friend, and the best approach for the question as stated.

It's easier to ask for forgiveness than permission (EAFP)The OP clarified in a comment that for their use case, it wasn't actually important to know what the index was. As the accepted answer notes, using x in somelist is the best answer if you don't care.But I'll assume, as the original question suggests, that you do care what the index is. In that case, I'll note that all the other solutions require scanning the list twice, which can bring a large performance penalty.Furthermore, as the venerable Raymond Hettinger wrote in a commentSo I'll push back on the assumption in the original question that exceptions should be avoided. I suggest that exceptions are your friend. They're nothing to be scared of, they aren't inefficient, and in fact you need to be conversant with them to write good code.So I think the best answer is to simply use a try-except approach:"deal with it" just means do what you need to do: set i to a sentinel value, raise an exception of your own, follow a different code branch, etc.This is an example of why the Python principle Easier to ask for forgiveness than permission (EAFP) makes sense, in contrast to the if-then-else style of Look before you leap (LBYL)If you don't care where it is in the sequence, only its presence, then use the in operator. Otherwise, write a function that refactors out the exception handling.hope this helpsThere is no built-in way to do what you want to do.Here is a good post that may help you: Why list doesn't have safe "get" method like dictionary?I like to use Web2py's List class, found in the storage module of its gluon package. The storage module offers list-like (List) and dictionary-like (Storage) data structures that do not raise errors when an element is not found.First download web2py's source, then copy-paste the gluon package folder into your python installation's site-packages.Now try it out:Note, it can also behave like a regular list as well:

python: naming a module that has a two-word name

Jason S

[python: naming a module that has a two-word name](https://stackoverflow.com/questions/2852283/python-naming-a-module-that-has-a-two-word-name)

I'm trying to put together a really simple module with one .py source file in it, and have already run into a roadblock. I was going to call it scons-config but import scons-config doesn't work in Python. I found this SO question and looked at PEP8 style guide but am kind of bewildered, it doesn't talk about two-word-name conventions.What's the right way to deal with this?edit: I did see "the use of underscores is discouraged" and that left me at a dead end: should I use "sconsconfig" or "scons_config" (I guess the other ones are out)?

2010-05-17 19:56:57Z

I'm trying to put together a really simple module with one .py source file in it, and have already run into a roadblock. I was going to call it scons-config but import scons-config doesn't work in Python. I found this SO question and looked at PEP8 style guide but am kind of bewildered, it doesn't talk about two-word-name conventions.What's the right way to deal with this?edit: I did see "the use of underscores is discouraged" and that left me at a dead end: should I use "sconsconfig" or "scons_config" (I guess the other ones are out)?If you have to, always use underscores _.Using a dot . would not even work, otherwise would break.But PEP 8 clearly describes it:UPDATE:To directly target your question: I think sconsconfig is fine. It is not too long and quite readable. But honestly, I don't think anyone will blame you if you use underscores and your code will run with either decision. There is always a certain level where you should not care that much anymore.First, the module name is the same as the name of the single .py file.  In Python-speak, a collection of several .py files is a package.PEP-8 discourages breaking up package names with underscores.  A quick peak at my site-packages directory shows that multiword names are commonly just run together (e.g., setuptools, sqlalchemy)Module names (that is, file names) may be broken up by underscores (and I usually do this, because I hate namesthatruntogethersoyoucanhardlyreadthem).Stick with lower-case only (per PEP-8).  This avoids problems when going from case-sensitive to case-insensitive filesystems and vice versa.Aside from PEP-8, you can also check out how the native Python modules deal with this issue. If you were to compare the native modules of Python 2 to that of Python 3, you would see that the new tendency with the official devs is to avoid uppercase and underscores. For example, ConfigParser in Python 2 becomes configparser in Python 3.Looking at this, the best course of action would be to avoid uppercase and underscores, and just join the words together, i.e. sconsconfig. - is a no go. The symbol is used for minus operator. The same is true in most programming languages. Use _ or otherwise nothing at all.

What is the most efficient graph data structure in Python? [closed]

bgoncalves

[What is the most efficient graph data structure in Python? [closed]](https://stackoverflow.com/questions/1171/what-is-the-most-efficient-graph-data-structure-in-python)

I need to be able to manipulate a large (10^7 nodes) graph in python. The data corresponding to each node/edge is minimal, say, a small number of strings. What is the most efficient, in terms of memory and speed, way of doing this? A dict of dicts is more flexible and simpler to implement, but I intuitively expect a list of lists to be faster. The list option would also require that I keep the data separate from the structure, while dicts would allow for something of the sort:What would you suggest?Yes, I should have been a bit clearer on what I mean by efficiency. In this particular case I mean it in terms of random access retrieval.Loading the data in to memory isn't a huge problem. That's done once and for all. The time consuming part is visiting the nodes so I can extract the information and measure the metrics I'm interested in.I hadn't considered making each node a class (properties are the same for all nodes) but it seems like that would add an extra layer of overhead? I was hoping someone would have some direct experience with a similar case that they could share. After all, graphs are one of the most common abstractions in CS.

2008-08-04 12:00:57Z

I need to be able to manipulate a large (10^7 nodes) graph in python. The data corresponding to each node/edge is minimal, say, a small number of strings. What is the most efficient, in terms of memory and speed, way of doing this? A dict of dicts is more flexible and simpler to implement, but I intuitively expect a list of lists to be faster. The list option would also require that I keep the data separate from the structure, while dicts would allow for something of the sort:What would you suggest?Yes, I should have been a bit clearer on what I mean by efficiency. In this particular case I mean it in terms of random access retrieval.Loading the data in to memory isn't a huge problem. That's done once and for all. The time consuming part is visiting the nodes so I can extract the information and measure the metrics I'm interested in.I hadn't considered making each node a class (properties are the same for all nodes) but it seems like that would add an extra layer of overhead? I was hoping someone would have some direct experience with a similar case that they could share. After all, graphs are one of the most common abstractions in CS.I would strongly advocate you look at NetworkX. It's a battle-tested war horse and the first tool most 'research' types reach for when they need to do analysis of network based data. I have manipulated graphs with 100s of thousands of edges without problem on a notebook. Its feature rich and very easy to use. You will find yourself focusing more on the problem at hand rather than the details in the underlying implementation.Example of Erds-Rnyi random graph generation and analysisVisualizations are also straightforward:More visualization: http://jonschull.blogspot.com/2008/08/graph-visualization.htmlEven though this question is now quite old, I think it is worthwhile to mention my own python module for graph manipulation called graph-tool. It is very efficient, since the data structures and algorithms are implemented in C++, with template metaprograming, using the Boost Graph Library. Therefore its performance (both in memory usage and runtime) is comparable to a pure C++ library, and can be orders of magnitude better than typical python code, without sacrificing ease of use. I use it myself constantly to work with very large graphs.As already mentioned, NetworkX is very good, with another option being igraph. Both modules will have most (if not all) the analysis tools you're likely to need, and both libraries are routinely used with large networks.A dictionary may also contain overhead, depending on the actual implementation. A hashtable usually contain some prime number of available nodes to begin with, even though you might only use a couple of the nodes.Judging by your example, "Property", would you be better of with a class approach for the final level and real properties? Or is the names of the properties changing a lot from node to node?I'd say that what "efficient" means depends on a lot of things, like:I think that you'll find that a data structure that is speedy will generally consume more memory than one that is slow. This isn't always the case, but most data structures seems to follow this.A dictionary might be easy to use, and give you relatively uniformly fast access, it will most likely use more memory than, as you suggest, lists. Lists, however, generally tend to contain more overhead when you insert data into it, unless they preallocate X nodes, in which they will again use more memory.My suggestion, in general, would be to just use the method that seems the most natural to you, and then do a "stress test" of the system, adding a substantial amount of data to it and see if it becomes a problem.You might also consider adding a layer of abstraction to your system, so that you don't have to change the programming interface if you later on need to change the internal data structure.As I understand it, random access is in constant time for both Python's dicts and lists, the difference is that you can only do random access of integer indexes with lists.  I'm assuming that you need to lookup a node by its label, so you want a dict of dicts.However, on the performance front, loading it into memory may not be a problem, but if you use too much you'll end up swapping to disk, which will kill the performance of even Python's highly efficient dicts.  Try to keep memory usage down as much as possible.  Also, RAM is amazingly cheap right now; if you do this kind of thing a lot, there's no reason not to have at least 4GB.If you'd like advice on keeping memory usage down, give some more information about the kind of information you're tracking for each node.Making a class-based structure would probably have more overhead than the dict-based structure, since in python classes actually use dicts when they are implemented.No doubt NetworkX is the best data structure till now for graph. It comes with utilities like Helper Functions, Data Structures and Algorithms, Random Sequence Generators, Decorators, Cuthill-Mckee Ordering, Context ManagersNetworkX is great because it wowrs for graphs, digraphs, and multigraphs. It can write graph with multiple ways: Adjacency List, Multiline Adjacency List,

Edge List, GEXF, GML. It works with Pickle, GraphML, JSON, SparseGraph6 etc. It has implimentation of various radimade algorithms including:

Approximation, Bipartite, Boundary, Centrality, Clique, Clustering,    Coloring, Components, Connectivity, Cycles,  Directed Acyclic Graphs,

Distance Measures,  Dominating Sets, Eulerian, Isomorphism,  Link Analysis,  Link Prediction, Matching, Minimum Spanning Tree, Rich Club, Shortest Paths, Traversal, Tree.

What are the use cases of Node.js vs Twisted?

pmn

[What are the use cases of Node.js vs Twisted?](https://stackoverflow.com/questions/3461549/what-are-the-use-cases-of-node-js-vs-twisted)

Assuming a team of developers are equally comfortable with writing Javascript on the server side as they are with Python & Twisted, when is Node.js going to be more appropriate than Twisted (and vice versa)?

2010-08-11 18:21:05Z

Assuming a team of developers are equally comfortable with writing Javascript on the server side as they are with Python & Twisted, when is Node.js going to be more appropriate than Twisted (and vice versa)?Twisted is more mature -- it's been around for a long, long time, and has so many bells and whistles as to make your head spin (implementations of the fanciest protocols, integration of the reactor with a large variety of other event loops, and so forth).Node.js is said to be faster (I have not measured it myself) and might perhaps be simpler to use (if you need none of the extra bells and whistles) exactly because those extras aren't there (kind of like Tornado in the Python world -- again, I have never measured relative performance).So, I'd absolutely use Twisted if I needed any of its extra features or wanted to feel on a more solid ground by using a more mature package.  If these considerations don't apply, but top performance is a key goal of the project, then I'd write a simple benchmark (but still representative of at least one or two key performance-need situations for my actual project) in Twisted, Node.js, and Tornado, and do a lot of careful measurement before I decide which way to go overall.  "Extra features" (third party extensions and standard library) for Python vs server-side Javascript are also much more abundant, and that might be a key factor if any such extras are needed for the project.Finally, if none of these issues matter to a specific application scenario, have the development team vote on relative simplicity of the three candidates (Twisted, Node.js, Tornado) in terms of simplicity and familiarity -- any of them will probably be just fine, might as well pick whatever most of the team is most comfortable with!As of 2012, Node.js has proved to be a fast, scalable, mature, and widely used platform. Ryan Dahl, creator of Node.js quotes:More formally, the advantages of Node can be classified as:Therefore, Node seem more powerful and with a lighter future, so if there isn't any constraint to use it (like existing code, servers, team capability), it is recommended for any new collaborative network project aiming high speed and scalability.

What is the underlying data structure for Python lists?

Nixuz

[What is the underlying data structure for Python lists?](https://stackoverflow.com/questions/914233/what-is-the-underlying-data-structure-for-python-lists)

What is the typical underlying data structure used to implement Python's built-in list data type?

2009-05-27 06:22:56Z

What is the typical underlying data structure used to implement Python's built-in list data type?See also:

http://docs.python.org/library/collections.html#collections.dequeBtw, I find it interesting that the Python tutorial on data structures recommends using pop(0) to simulate a queue but does not mention O(n) or the deque option.http://docs.python.org/tutorial/datastructures.html#using-lists-as-queuesCPython:As can be seen on the following line, the list is declared as an array of pointers to PyObjects.In the Jython implementation, it's an ArrayList<PyObject>.

Why use lambda functions?

NoMoreZealots

[Why use lambda functions?](https://stackoverflow.com/questions/3259322/why-use-lambda-functions)

I can find lots of stuff showing me what a lambda function is, and how the syntax works and what not.  But other than the "coolness factor" (I can make a function in middle a call to another function, neat!) I haven't seen something that's overwelmingly compelling to say why I really need/want to use them.It seems to be more of a stylistic or structual choice in most examples I've seen.  And kinda breaks the "Only one correct way to do something" in python rule.  How does it make my programs, more correct, more reliable, faster, or easier to understand?  (Most coding standards I've seen tend to tell you to avoid overly complex statements on a single line.  If it makes it easier to read break it up.)

2010-07-15 19:41:54Z

I can find lots of stuff showing me what a lambda function is, and how the syntax works and what not.  But other than the "coolness factor" (I can make a function in middle a call to another function, neat!) I haven't seen something that's overwelmingly compelling to say why I really need/want to use them.It seems to be more of a stylistic or structual choice in most examples I've seen.  And kinda breaks the "Only one correct way to do something" in python rule.  How does it make my programs, more correct, more reliable, faster, or easier to understand?  (Most coding standards I've seen tend to tell you to avoid overly complex statements on a single line.  If it makes it easier to read break it up.)Here's a good example:versusFrom another angle: Lambda expressions are also known as "anonymous functions", and are very useful in certain programming paradigms, particularly functional programming, which lambda calculus provided the inspiration for.http://en.wikipedia.org/wiki/Lambda_calculusThe syntax is more concise in certain situations, mostly when dealing with map et al.seems better to me than:I think the lambda is a better choice in this situation because the def double seems almost disconnected from the map that is using it. Plus, I guess it has the added benefit that the function gets thrown away when you are done.There is one downside to lambda which limits its usefulness in Python, in my opinion: lambdas can have only one expression (i.e., you can't have multiple lines). It just can't work in a language that forces whitespace.Plus, whenever I use lambda I feel awesome.Lambda functions are most useful in things like callback functions, or places in which you need a throwaway function. JAB's example is perfect - It would be better accompanied by the keyword argument key, but it still provides useful information.Whenappears 300 lines away from what does key do? There's really no indication. You might have some sort of guess, especially if you're familiar with the function, but usually it requires going back to look. OTOH,tells you a lot more.There's probably some more information, but already that's a tremendous amount  that you get just by using an anonymous lambda function instead of a named function.Plus it doesn't pollute your namespace ;)For me it's a matter of the expressiveness of the code.  When writing code that people will have to support, that code should tell a story in as concise and easy to understand manner as possible.  Sometimes the lambda expression is more complicated, other times it more directly tells what that line or block of code is doing.  Use judgment when writing.Think of it like structuring a sentence.  What are the important parts (nouns and verbs vs. objects and methods, etc.) and how should they be ordered for that line or block of code to convey what it's doing intuitively.Yes, you're right  it is a structural choice. It probably does not make your programs more correct by just using lambda expressions. Nor does it make them more reliable, and this has nothing to do with speed.It is only about flexibility and the power of expression. Like list comprehension. You can do most of that defining named functions (possibly polluting namespace, but that's again purely stylistic issue).It can aid to readability by the fact, that you do not have to define a separate named function, that someone else will have to find, read and understand that all it does is to call a method blah() on its argument.It may be much more interesting when you use it to write functions that create and return other functions, where what exactly those functions do, depends on their arguments. This may be a very concise and readable way of parameterizing your code behaviour. You can just express more interesting ideas.But that is still a structural choice. You can do that otherwise. But the same goes for object oriented programming ;) Ignore for a moment the detail that it's specifically anonymous functions we're talking about.  functions, including anonymous ones, are assignable quantities (almost, but not really, values) in Python.  an expression likeexplicitly mentions four anonymous quantities: -1, 0, 10 and the result of the lambda operator, plus the implied result of the map call.  it's possible to create values of anonymous types in some languages.  so ignore the superficial difference between functions and numbers.  the question when to use an anonymous function as opposed to a named one is similar to a question of when to put a naked number literal in the code and when to declare a TIMES_I_WISHED_I_HAD_A_PONY or BUFFER_SIZE beforehand.  there are times when it's appropriate to use a (numeric, string or function) literal, and there are times when it's more appropriate to name such a thing and refer to it through its name.see eg. Allen Holub's provocative, thought-or-anger-provoking book on Design Patterns in Java; he uses anonymous classes quite a bit.Lambda, while useful in certain situations, has a large potential for abuse.  lambda's almost always make code more difficult to read.  And while it might feel satisfying to fit all your code onto a single line, it will suck for the next person who has to read your code.Direct from PEP8"One of Guido's key insights is that code is read much more often than it is written."One use of lambda function which I have learned, and where is not other good alternative or at least looks for me best is as default action in function parameter byThis returns the value without change, but you can supply one function optionally to perform a transformation or action (like printing the answer, not only returning)Also often it is useful to use in sorting as key:The effect is to sort by fieldth (zero based remember) element of each item in sequence. For reversing you do not need lambda as it is clearer to useOften it is almost as easy to do new real function and use that instead of lambda. If people has studied much Lisp or other functional programming, they also have natural tendency to use lambda function as in Lisp the function definitions are handled by lambda calculus.Lambdas are objects, not methods, and they cannot be invoked in the same way that methods are.

for e.gsucc mow holds a Proc object, which we can use like any other:gives us an output = 3I want to point out one situation other than list-processing where the lambda functions seems the best choice:And if we drop lambda function here, the callback may only execute the callback once.Another point is that python does not have switch statements. Combining lambdas with dicts can be an effective alternative. e.g.:Lambdas are anonymous functions (function with no name) that can be assigned to a variable or that can be passed as an argument to another function. The usefulness of lambda will be realized when you need a small piece of function that will be run one in a while or just once. Instead of writing the function in global scope or including it as part of your main program you can toss around few lines of code when needed to a variable or another function. Also when you pass the function as an argument to another function during the function call you can change the argument (the anonymous function) making the function itself dynamic. Suppose if the anonymous function uses variables outside its scope it is called closure. This is useful in callback functions.In some cases it is much more clear to express something simple as a lambda. Consider regular sorting vs. reverse sorting for example:For the latter case writing a separate full-fledged function just to return a -cmp(a, b) would create more misunderstanding then a lambda.    It is definitely true that abusing lambda functions often leads to bad and hard-to-read code. On the other hand, when used accurately, it does the opposite. There are already great answers in this thread, but one example I have come across is:This simplified case could be rewritten in many other ways without the use of lambda. Still, one can infer how lambda functions can increase readability and code reuse in perhaps more complex cases and functions with this example.Lambdas allow you to create functions on the fly. Most of the examples I've seen don't do much more than create a function with parameters passed at the time of creation rather than execution. Or they simplify the code by not requiring a formal declaration of the function ahead of use.A more interesting use would be to dynamically construct a python function to evaluate a mathematical expression that isn't known until run time (user input). Once created, that function can be called repeatedly with different arguments to evaluate the expression (say you wanted to plot it). That may even be a poor example given eval(). This type of use is where the "real" power is - in dynamically creating more complex code, rather than the simple examples you often see which are not much more than nice (source) code size reductions.

Python equivalent of Java StringBuffer?

user2902773

[Python equivalent of Java StringBuffer?](https://stackoverflow.com/questions/19926089/python-equivalent-of-java-stringbuffer)

Is there anything in Python like Java's StringBuffer? Since strings are immutable in Python too, editing them in loops would be inefficient.

2013-11-12 10:00:47Z

Is there anything in Python like Java's StringBuffer? Since strings are immutable in Python too, editing them in loops would be inefficient.Efficient String Concatenation in Python is a rather old article and its main statement that the naive concatenation is far slower than joining is not valid anymore, because this part has been optimized in CPython since then:I've adapted their code a bit and got the following results on my machine:Results:Conclusions:resultsDepends on what you want to do. If you want a mutable sequence, the builtin list type is your friend, and going from str to list and back is as simple as:If you want to build a large string using a for loop, the pythonic way is usually to build a list of strings then join them together with the proper separator (linebreak or whatever).Else you can also use some text template system, or a parser or whatever specialized tool is the most appropriate for the job. Perhaps use a bytearray:The appeal of using a bytearray is its memory-efficiency and convenient syntax. It can also be faster than using a temporary list:Note that much of the difference in speed is attributable to the creation of the container:The previously provided answers are almost always best. However, sometimes the string is built up across many method calls and/or loops, so it's not necessarily natural to build up a list of lines and then join them. And since there's no guarantee you are using CPython or that CPython's optimization will apply, then another approach is to just use print!Here's an example helper class, although the helper class is trivial and probably unnecessary, it serves to illustrate the approach (Python 3):this link might be useful for concatenation in pythonhttp://pythonadventures.wordpress.com/2010/09/27/stringbuilder/example from above link:Just a test I run on python 3.6.2 showing that "join" still win BIG!And the output was:I've added to Roee Gavirel's code 2 additional tests that show conclusively that joining lists into strings is not any faster than s += "something".  Results:Code:

Specifying targets for intersphinx links to numpy, scipy, and matplotlib

orome

[Specifying targets for intersphinx links to numpy, scipy, and matplotlib](https://stackoverflow.com/questions/21538983/specifying-targets-for-intersphinx-links-to-numpy-scipy-and-matplotlib)

Following the documentation for setting up Sphinx documentation links between packages, I have added to my conf.py, but can't seem to get links to any project other than Python itself to work. For examplejust takes me to the index page, without adding the expected #term-svg anchor, and I can't even locate the glossary for scipy or figure out how to determine what :ref:s or :term:s are supported by a package.Where can I find instructions on how to specify targets for :ref:s and :term:s in numpy, scipy, and matplotlib?For that matter, how do I link to Sphinx itself? Addinganddoesn't work.

2014-02-03 22:15:21Z

Following the documentation for setting up Sphinx documentation links between packages, I have added to my conf.py, but can't seem to get links to any project other than Python itself to work. For examplejust takes me to the index page, without adding the expected #term-svg anchor, and I can't even locate the glossary for scipy or figure out how to determine what :ref:s or :term:s are supported by a package.Where can I find instructions on how to specify targets for :ref:s and :term:s in numpy, scipy, and matplotlib?For that matter, how do I link to Sphinx itself? Addinganddoesn't work.I have a Gist with a handful of intersphinx mappings, which now includes all of numpy, scipy and matplotlib.  You should be able to use these entries directly in intersphinx_mapping, within your conf.py. If anyone has suggestions for further entries to be added to this list, please feel free to post requests into the comments of the Gist.For all of these packages, per fgoudra's answer I highly recommend using sphobjinv to search within the objects.inv file for each library.  (Full disclosure: I am the author of sphobjinv.) The suggest mode of the CLI interface is specifically designed to provide the information needed to compose intersphinx cross-references.numpy is complicated. Sometimes you need a fully-qualified name, e.g.:Other times (for C functions, for example) you can just reference the function's base name BUT you have to explicitly indicate the domain, e.g.:Yet other times you may have to reference the custom np domain, e.g.:There's really no way to know what the right syntax is without consulting the objects.inv.scipy is roughly as inscrutable as numpy. Things are further complicated by the introduction of numerous custom domains for the various scipy subpackages, e.g.:For matplotlib, it appears you always have to provide the (quite verbose) fully-specified object name in the reference, e.g.:All of the matplotlib code objects seem to reside in the default py domain, however, which simplifies things somewhat.For any of these, if you're having trouble getting a link to construct properly, the first thing I fall back to is using the generic :obj: role, e.g.:This will construct an intersphinx link regardless of the role in which a particular object was defined, though I think you still have to correctly specify any relevant non-default domain. If the reference doesn't work properly with an :obj: role, then there's an error in the object name or the domain somewhere. Check for typos in both places.It is possible to manually specify which inventory to look. For example, if intersphinx_mapping['sphinx'] = ('http://sphinx-doc.org/', None)

does not work, you can always download the inventory and manually append it to the mapping (e.g. download from http://sphinx-doc.org/objects.inv, save the binary file in your docs and append the path to it in the mapping; this will give something like:intersphinx_mapping['sphinx'] = ('http://sphinx-doc.org/', ('objects.inv', ), )To verify if a reference exists within the inventory, you can explore the binary with the sphobjinv python package and check where is the reference you want.This may not be a solution to your problem but can help to debug some things.In case this is still an issue..

you need to leave out the slash at the end of the URL:An additional way to know how to do cross reference is with the sphobjinv module.You can search local or even remote inventory files (with fuzzy matching). For instance with scipy:Note that you may need to use :py:func: and not :py:function: (I'd be happy to know why).

sparse 3d matrix/array in Python?

zhongqi

[sparse 3d matrix/array in Python?](https://stackoverflow.com/questions/7685128/sparse-3d-matrix-array-in-python)

In scipy, we can construct a sparse matrix using scipy.sparse.lil_matrix() etc. But the matrix is in 2d.I am wondering if there is an existing data structure for sparse 3d matrix / array (tensor) in Python?p.s. I have lots of sparse data in 3d and need a tensor to store / perform multiplication. Any suggestions to implement such a tensor if there's no existing data structure?

2011-10-07 09:08:40Z

In scipy, we can construct a sparse matrix using scipy.sparse.lil_matrix() etc. But the matrix is in 2d.I am wondering if there is an existing data structure for sparse 3d matrix / array (tensor) in Python?p.s. I have lots of sparse data in 3d and need a tensor to store / perform multiplication. Any suggestions to implement such a tensor if there's no existing data structure?Happy to suggest a (possibly obvious) implementation of this, which could be made in pure Python or C/Cython if you've got time and space for new dependencies, and need it to be faster.A sparse matrix in N dimensions can assume most elements are empty, so we use a dictionary keyed on tuples:and you would use it like so:You could make this implementation more robust by verifying that the input is in fact a tuple, and that it contains only integers, but that will just slow things down so I wouldn't worry unless you're releasing your code to the world later.EDIT - a Cython implementation of the matrix multiplication problem, assuming other tensor is an N Dimensional NumPy array (numpy.ndarray) might look like this:Although you will always need to hand roll this for the problem at hand, because (as mentioned in code comment) you'll need to define which indices you're summing over, and be careful about the array lengths or things won't work!EDIT 2 - if the other matrix is also sparse, then you don't need to do the three way looping:My suggestion for a C implementation would be to use a simple struct to hold the indices and the values:you'll then need some functions to allocate and maintain a dynamic array of such structs, and search them as fast as you need; but you should test the Python implementation in place for performance before worrying about that stuff.Have a look at sparray - sparse n-dimensional arrays in Python (by Jan Erik Solem). Also available on github.An alternative answer as of 2017 is the sparse package. According to the package itself it implements sparse multidimensional arrays on top of NumPy and scipy.sparse by generalizing the scipy.sparse.coo_matrix layout.Here's an example taken from the docs:Nicer than writing everything new from scratch may be to use scipy's sparse module as far as possible. This may lead to (much) better performance. I had a somewhat similar problem, but I only had to access the data efficiently, not perform any operations on them. Furthermore, my data were only sparse in two out of three dimensions.I have written a class that solves my problem and could (as far as I think) easily be extended to satisfiy the OP's needs. It may still hold some potential for improvement, though.I also need 3D sparse matrix for solving the 2D heat equations (2 spatial dimensions are dense, but the time dimension is diagonal plus and minus one offdiagonal.) I found this link to guide me. The trick is to create an array Number that maps the 2D sparse matrix to a 1D linear vector. Then build the 2D matrix by building a list of data and indices. Later the Number matrix is used to arrange the answer back to a 2D array.[edit] It occurred to me after my initial post, this could be handled better by using the .reshape(-1) method. After research, the reshape method is better than flatten because it returns a new view into the original array, but flatten copies the array. The code uses the original Number array. I will try to update later.[end edit]I test it by creating a 1D random vector and solving for a second vector. Then multiply it by the sparse 2D matrix and I get the same result. Note: I repeat this many times in a loop with exactly the same matrix M, so you might think it would be more efficient to solve for inverse(M). But the inverse of M is not sparse, so I think (but have not tested) using spsolve is a better solution. "Best" probably depends on how large the matrix is you are using.

How to replicate tee behavior in Python when using subprocess?

badp

[How to replicate tee behavior in Python when using subprocess?](https://stackoverflow.com/questions/2996887/how-to-replicate-tee-behavior-in-python-when-using-subprocess)

I'm looking for a Python solution that will allow me to save the output of a command in a file without hiding it from the console.FYI: I'm asking about tee (as the Unix command line utility) and not the function with the same name from Python intertools module.Here are some incomplete solutions I found so far:Diagram http://blog.i18n.ro/wp-content/uploads/2010/06/Drawing_tee_py.pngExpected output was to have the lines ordered. Remark, modifying the Popen to use only one PIPE is not allowed because in the real life I will want to do different things with stderr and stdout.Also even in the second case I was not able to obtain real-time like out, in fact all the results were received when the process finished. By default, Popen should use no buffers (bufsize=0).

2010-06-08 13:09:44Z

I'm looking for a Python solution that will allow me to save the output of a command in a file without hiding it from the console.FYI: I'm asking about tee (as the Unix command line utility) and not the function with the same name from Python intertools module.Here are some incomplete solutions I found so far:Diagram http://blog.i18n.ro/wp-content/uploads/2010/06/Drawing_tee_py.pngExpected output was to have the lines ordered. Remark, modifying the Popen to use only one PIPE is not allowed because in the real life I will want to do different things with stderr and stdout.Also even in the second case I was not able to obtain real-time like out, in fact all the results were received when the process finished. By default, Popen should use no buffers (bufsize=0).I see that this is a rather old post but just in case someone is still searching for a way to do this:This is a straightforward port of tee to Python.I'm running on Linux right now but this ought to work on most platforms.Now for the subprocess part, I don't know how you want to 'wire' the subprocess's stdin, stdout and stderr to your stdin, stdout, stderr and file sinks, but I know you can do this:Now you can access callee.stdin, callee.stdout and callee.stderr like normal files, enabling the above "solution" to work. If you want to get the callee.returncode, you'll need to make an extra call to callee.poll().Be careful with writing to callee.stdin: if the process has exited when you do that, an error may be rised (on Linux, I get IOError: [Errno 32] Broken pipe).This is how it can be done If you don't want to interact with the process you can use the subprocess module just fine.Example:tester.pytesting.pyIn your situation you can simply write stdout/stderr to a file first. You can send arguments to your process with communicate as well, though I wasn't able to figure out how to continually interact with the subprocess.If requiring python 3.6 isn't an issue there is now a way of doing this using asyncio. This method allows you to capture stdout and stderr separately but still have both stream to the tty without using threads. Here's a rough outline:The code above was based on this blog post: https://kevinmccarthy.org/2016/07/25/streaming-subprocess-stdin-and-stdout-with-asyncio-in-python/Try this :My solution isn't elegant, but it works.You can use powershell to gain access to "tee" under WinOS.

Using Flask-SQLAlchemy in Blueprint models without reference to the app [closed]

vicvicvic

[Using Flask-SQLAlchemy in Blueprint models without reference to the app [closed]](https://stackoverflow.com/questions/13058800/using-flask-sqlalchemy-in-blueprint-models-without-reference-to-the-app)

I'm trying to create a "modular application" in Flask using Blueprints.When creating models, however, I'm running into the problem of having to reference the app in order to get the db-object provided by Flask-SQLAlchemy. I'd like to be able to use some blueprints with more than one app (similar to how Django apps can be used), so this is not a good solution.*My questions are thus:I was asked to provide an example, so let's do something simple: Say I have a blueprint describing "flatpages" -- simple, "static" content stored in the database. It uses a table with just shortname (for URLs), a title and a body. This is simple_pages/__init__.py:Then, it would be nice to let this blueprint define its own model (this in simple_page/models.py):This question is related to:And various others, but all replies seem to rely on import the app's db instance, or doing the reverse. The "Large app how to" wiki page also uses the "import your app in your blueprint" pattern.* Since the official documentation shows how to create routes, views, templates and assets in a Blueprint without caring about what app it's "in", I've assumed that Blueprints should, in general, be reusable across apps. However, this modularity doesn't seem that useful without also having independent models.Since Blueprints can be hooked into an app more than once, it might simply be the wrong approach to have models in Blueprints?

2012-10-24 22:28:25Z

I'm trying to create a "modular application" in Flask using Blueprints.When creating models, however, I'm running into the problem of having to reference the app in order to get the db-object provided by Flask-SQLAlchemy. I'd like to be able to use some blueprints with more than one app (similar to how Django apps can be used), so this is not a good solution.*My questions are thus:I was asked to provide an example, so let's do something simple: Say I have a blueprint describing "flatpages" -- simple, "static" content stored in the database. It uses a table with just shortname (for URLs), a title and a body. This is simple_pages/__init__.py:Then, it would be nice to let this blueprint define its own model (this in simple_page/models.py):This question is related to:And various others, but all replies seem to rely on import the app's db instance, or doing the reverse. The "Large app how to" wiki page also uses the "import your app in your blueprint" pattern.* Since the official documentation shows how to create routes, views, templates and assets in a Blueprint without caring about what app it's "in", I've assumed that Blueprints should, in general, be reusable across apps. However, this modularity doesn't seem that useful without also having independent models.Since Blueprints can be hooked into an app more than once, it might simply be the wrong approach to have models in Blueprints?I believe the truest answer is that modular blueprints shouldn't concern themselves directly with data access, but instead rely on the application providing a compatible implementation.So given your example blueprint.From this, there is nothing preventing you from providing a default implementation.And in your configuration.The above could be made cleaner by not relying in direct inheritance from db.Model and instead just use a vanilla declarative_base from sqlalchemy, but this should represent the gist of it.I have similar needs of making Blueprints completely modular and having no reference to the App. I came up with a possibly clean solution but I'm not sure how correct it is and what its limitations are.The idea is to create a separate db object (db = SQLAlchemy()) inside the blueprint and call the init_app() and create_all() methods from where the root app is created.Here's some sample code to show how the project is structured:

The app is called jobs and the blueprint is called status and it is stored inside the blueprints folder.blueprints.status.models.pymodels.pyfactory.pyI tested it with gunicorn with gevent worker and it works. I asked a separate question about the robustness of the solution here:

Create one SQLAlchemy instance per blueprint and call create_all multiple timesYou asked "Are Blueprints not meant to be independent of the app and be redistributable ( la Django apps)? "The answer is yes. Blueprints are not similar to Django App.If you want to use different app/configurations, then you need to use "Application Dispatching" and not blueprints. Read this 

  [1]: http://flask.pocoo.org/docs/patterns/appdispatch/#app-dispatch [1]Also, the link here [1] http://flask.pocoo.org/docs/blueprints/#the-concept-of-blueprints [1]It clearly says and I quote "A blueprint in Flask is not a pluggable app because it is not actually an application  its a set of operations which can be registered on an application, even multiple times. Why not have multiple application objects? You can do that (see Application Dispatching), but your applications will have separate configs and will be managed at the WSGI layer."

matplotlib: can I create AxesSubplot objects, then add them to a Figure instance?

juanchopanza

[matplotlib: can I create AxesSubplot objects, then add them to a Figure instance?](https://stackoverflow.com/questions/6309472/matplotlib-can-i-create-axessubplot-objects-then-add-them-to-a-figure-instance)

Looking at the matplotlib documentation, it seems the standard way to add an AxesSubplot to a Figure is to use Figure.add_subplot:I would like to be able to create AxesSubPlot-like objects independently of the figure, so I can use them in different figures. Something likeIs this possible in matplotlib and if so, how can I do this?Update: I have not managed to decouple creation of Axes and Figures, but following examples in the answers below, can easily re-use previously created axes in new or olf Figure instances. This can be illustrated with a simple function:

2011-06-10 16:38:43Z

Looking at the matplotlib documentation, it seems the standard way to add an AxesSubplot to a Figure is to use Figure.add_subplot:I would like to be able to create AxesSubPlot-like objects independently of the figure, so I can use them in different figures. Something likeIs this possible in matplotlib and if so, how can I do this?Update: I have not managed to decouple creation of Axes and Figures, but following examples in the answers below, can easily re-use previously created axes in new or olf Figure instances. This can be illustrated with a simple function:Typically, you just pass the axes instance to a function. For example:To respond to your question, you could always do something like this:Also, you can simply add an axes instance to another figure:Resizing it to match other subplot "shapes" is also possible, but it's going to quickly become more trouble than it's worth.  The approach of just passing around a figure or axes instance (or list of instances) is much simpler for complex cases, in my experience...The following shows how to "move" an axes from one figure to another. This is the intended functionality of @JoeKington's last example, which in newer matplotlib versions is not working anymore, because axes cannot live in several figures at once.You would first need to remove the axes from the first figure, then append it to the next figure and give it some position to live in. For line plots, you can deal with the Line2D objects themselves:

Time complexity of python set operations?

Stephen Emslie

[Time complexity of python set operations?](https://stackoverflow.com/questions/7351459/time-complexity-of-python-set-operations)

What is the the time complexity of each of python's set operations in Big O notation?I am using Python's set type for an operation on a large number of items. I want to know how each operation's performance will be affected by the size of the set. For example, add, and the test for membership:Googling around hasn't turned up any resources, but it seems reasonable that the time complexity for Python's set implementation would have been carefully considered.If it exists, a link to something like this would be great. If nothing like this is out there, then perhaps we can work it out?Extra marks for finding the time complexity of all set operations.

2011-09-08 16:33:10Z

What is the the time complexity of each of python's set operations in Big O notation?I am using Python's set type for an operation on a large number of items. I want to know how each operation's performance will be affected by the size of the set. For example, add, and the test for membership:Googling around hasn't turned up any resources, but it seems reasonable that the time complexity for Python's set implementation would have been carefully considered.If it exists, a link to something like this would be great. If nothing like this is out there, then perhaps we can work it out?Extra marks for finding the time complexity of all set operations.According to Python wiki: Time complexity, set is implemented as a hash table. So you can expect to lookup/insert/delete in O(1) average. Unless your hash table's load factor is too high, then you face collisions and O(n).P.S. for some reason they claim O(n) for delete operation which looks like a mistype.P.P.S. This is true for CPython, pypy is a different story.The operation in should be independent from he size of the container, ie. O(1) -- given an optimal hash function. This should be nearly true for Python strings. Hashing strings is always critical, Python should be clever there and thus you can expect near-optimal results.

UnicodeEncodeError: 'ascii' codec can't encode character at special name [duplicate]

rhb1

[UnicodeEncodeError: 'ascii' codec can't encode character at special name [duplicate]](https://stackoverflow.com/questions/31137552/unicodeencodeerror-ascii-codec-cant-encode-character-at-special-name)

My python (ver 2.7) script is running well to get some company name from local html files but when it comes to some specific country name, it gives this error "UnicodeEncodeError: 'ascii' codec can't encode character"Specially getting error when this company name comesCompany Name: Khlfix Klteanlagen Ing.Gerhard Doczekal & Co. KGThe link cannot be processedError gives in this line of code: 

2015-06-30 11:53:34Z

My python (ver 2.7) script is running well to get some company name from local html files but when it comes to some specific country name, it gives this error "UnicodeEncodeError: 'ascii' codec can't encode character"Specially getting error when this company name comesCompany Name: Khlfix Klteanlagen Ing.Gerhard Doczekal & Co. KGThe link cannot be processedError gives in this line of code: Try setting the system default encoding as utf-8 at the start of the script, so that all strings are encoded using that.Example -The above should set the default encoding as utf-8 .You really want to do thisThis is the "encode late" strategy described in this unicode presentation (slides 32 through 35).

How can I get the IP address from NIC in Python?

Memor-X

[How can I get the IP address from NIC in Python?](https://stackoverflow.com/questions/24196932/how-can-i-get-the-ip-address-from-nic-in-python)

When an error occurs in a Python script on Unix , an email is sent.I have been asked to add {Testing Environment} to the subject line of the email if the IP address is 192.168.100.37 which is the testing server. This way we can have one version of a script and a way to tell if the email is coming from messed up data on the testing server.However, when I google I keep finding this code:However, that's giving me the IP address of 127.0.1.1. When I use ifconfig I get thisFirstly, I don't know where it got 127.0.1.1 from, but either way that's not what I want. When I google I keep coming to the same syntax, Bash scripts or netifaces and I'm trying to use standard libraries.So how can I get the IP address of eth0 in Python?

2014-06-13 02:46:10Z

When an error occurs in a Python script on Unix , an email is sent.I have been asked to add {Testing Environment} to the subject line of the email if the IP address is 192.168.100.37 which is the testing server. This way we can have one version of a script and a way to tell if the email is coming from messed up data on the testing server.However, when I google I keep finding this code:However, that's giving me the IP address of 127.0.1.1. When I use ifconfig I get thisFirstly, I don't know where it got 127.0.1.1 from, but either way that's not what I want. When I google I keep coming to the same syntax, Bash scripts or netifaces and I'm trying to use standard libraries.So how can I get the IP address of eth0 in Python?Two methods:You need to ask for the IP address that is bound to your eth0 interface. This is available from the netifaces packageYou can also get a list of all available interfaces viaHere's a way to get the IP address without using a python package:Note: detecting the IP address to determine what environment you are using is quite a hack. Almost all frameworks provide a very simple way to set/modify an environment variable to indicate the current environment. Try and take a look at your documentation for this. It should be as simple as doingAlternatively, if you want to get the IP address of whichever interface is used to connect to the network without having to know its name, you can use this:I know it's a little different than your question, but others may arrive here and find this one more useful. You do not have to have a route to 8.8.8.8 to use this. All it is doing is opening a socket, but not sending any data.A simple approach which returns a string with ip-addresses for the interfaces is:for more info see hostname.Since most of the answers use ifconfig to extract the IPv4 from the eth0 interface, which is deprecated in favor of ip addr, the following code could be used instead:Alternatively, you can shift part of the parsing task to the python interpreter by using split() instead of grep and awk, as @serg points out in the comment:But in this case you have to check the bounds of the array returned by each split() call.Another version using regex:If you only need to work on Unix, you can use a system call (ref. StackOverflow question Parse ifconfig to get only my IP address using Bash):Building on the answer from @jeremyjjbrown, another version that cleans up after itself as mentioned in the comments to his answer. This version also allows providing a different server address for use on private internal networks, etc..try below code, it works for me in Mac10.10.2:It worked for me Find the IP address of the first eth/wlan entry in ifconfig that's RUNNING:This is the result of ifconfig:Cutting a bit the output, we have:Now, we can go to python and do:Yet another way of obtaining the IP Address from a NIC, using Python.I had this as part of an app that I developed long time ago, and I didn't wanted to simply git rm script.py. So, here I provide the approach, using subprocess and list comprehensions for the sake of functional approach and less lines of code:Additionally, you can use a similar approach for obtaining a list of NICs:Here's the solution as a Gist.And you would have something like this:

sqlalchemy filter multiple columns

teggy

[sqlalchemy filter multiple columns](https://stackoverflow.com/questions/3332991/sqlalchemy-filter-multiple-columns)

How do I combine two columns and apply filter? For example, I want to search in both the "firstname" and "lastname" columns at the same time.  Here is how I have been doing it if searching only one column:

2010-07-26 07:32:39Z

How do I combine two columns and apply filter? For example, I want to search in both the "firstname" and "lastname" columns at the same time.  Here is how I have been doing it if searching only one column:You can simply call filter multiple times:You can use SQLAlchemy's or_ function to search in more than one column (the underscore is necessary to distinguish it from Python's own or).Here's an example:There are number of ways to do it:Using filter() (and operator)Using filter_by() (and operator)Chaining filter() or filter_by() (and operator)Using or_(), and_(), and not()

Python strip() multiple characters?

AP257

[Python strip() multiple characters?](https://stackoverflow.com/questions/3900054/python-strip-multiple-characters)

I want to remove any brackets from a string. Why doesn't this work properly?

2010-10-10 11:09:16Z

I want to remove any brackets from a string. Why doesn't this work properly?I did a time test here, using each method 100000 times in a loop.  The results surprised me.  (The results still surprise me after editing them in response to valid criticism in the comments.)Here's the script:Here are the results:Results on other runs follow a similar pattern.  If speed is not the primary concern, however, I still think string.translate is not the most readable; the other three are more obvious, though slower to varying degrees.Because that's not what strip() does. It removes leading and trailing characters that are present in the argument, but not those characters in the middle of the string.You could do:or:or maybe use a regex:string.translate with table=None works fine.Because strip() only strips trailing and leading characters, based on what you provided. I suggest:strip only strips characters from the very front and back of the string.To delete a list of characters, you could use the string's translate method:For example string s="(U+007c)"To remove only the parentheses from s, try the below one:

Retrieving a Foreign Key value with django-rest-framework serializers

hellsgate

[Retrieving a Foreign Key value with django-rest-framework serializers](https://stackoverflow.com/questions/17280007/retrieving-a-foreign-key-value-with-django-rest-framework-serializers)

I'm using the django rest framework to create an API. 

I have the following models:To create a serializer for the categories I'd do:... and this would provide me with:How would I go about getting the reverse from an Item serializer, ie:I've read through the docs on reverse relationships for the rest framework but that appears to be the same result as the non-reverse fields.  Am I missing something obvious?

2013-06-24 16:07:32Z

I'm using the django rest framework to create an API. 

I have the following models:To create a serializer for the categories I'd do:... and this would provide me with:How would I go about getting the reverse from an Item serializer, ie:I've read through the docs on reverse relationships for the rest framework but that appears to be the same result as the non-reverse fields.  Am I missing something obvious?Just use a related field without setting many=True.Note that also because you want the output named category_name, but the actual field is category, you need to use the source argument on the serializer field.The following should give you the output you need...In the DRF version 3.6.3  this worked for meMore info can be found here: Serializer Fields core argumentsAnother thing you can do is to:Your model would look like this.Your serializer would look like this. Note that the serializer will automatically get the value of the category_name model property by naming the field with the same name.this worked fine for me:Worked on 08/08/2018 and on DRF version 3.8.2:Using the Meta read_only_fields we can declare exactly which fields should be read_only. Then we need to declare the foreign field on the Meta fields (better be explicit as the mantra goes: zen of python).Simple solution

source='category.name' where category is foreign key and .name it's attribute.

openpyxl - adjust column width size

Satish

[openpyxl - adjust column width size](https://stackoverflow.com/questions/13197574/openpyxl-adjust-column-width-size)

I have following script which is converting a CSV file to an XLSX file, but my column size is very narrow. Each time I have to drag them with mouse to read data. Does anybody know how to set column width in openpyxl? Here is the code I am using.

2012-11-02 14:50:05Z

I have following script which is converting a CSV file to an XLSX file, but my column size is very narrow. Each time I have to drag them with mouse to read data. Does anybody know how to set column width in openpyxl? Here is the code I am using.You could estimate (or use a mono width font) to achieve this. Let's assume data is a nested array like [['a1','a2'],['b1','b2']]We can get the max characters in each column. Then set the width to that. Width is exactly the width of a monospace font (if not changing other styles at least). Even if you use a variable width font it is a decent estimation. This will not work with formulas.A bit of a hack but your reports will be more readable.My variation of Bufke's answer. Avoids a bit of branching with the array and ignores empty cells / columns.Now fixed for non-string cell values.As of openpyxl version 3.0.3 you need to use as the openpyxl library will raise a TypeError if you pass column_dimensions a number instead of a column letter, everything else can stay the same.Even more pythonic way to set the width of all columns that works at least in openpyxl version 2.4.0:The as_text function should be something that converts the value to a proper length string, like for Python 3:I have a problem with merged_cells and autosize not work correctly, if you have the same problem, you can solve with the next code:A slight improvement of the above accepted answer, that I think is more pythonic (asking for forgiveness is better than asking for permission)This is my version referring @Virako 's code snippetAnd how to use is as follows,All the above answers are generating an issue which is that col[0].column is returning number while worksheet.column_dimensions[column] accepts only character such as 'A', 'B', 'C' in place of column. I've modified @Virako's code and it is working fine now.We can convert numbers to their ASCII values and give it to column_dimension parameterI had to change @User3759685 above answer to this when the openpxyl updated. I was getting an error. Well @phihag reported this in the comments as wellAfter update from openpyxl2.5.2a to latest 2.6.4 (final version for python 2.x support), I got same issue in configuring the width of a column.Basically I always calculate the width for a column (dims is a dict maintaining each column width):Afterwards I am modifying the scale to something shortly bigger than original size, but now you have to give the "Letter" value of a column and not anymore a int value (col below is the value and is translated to the right letter):This will fix the visible error and assigning the right width to your column ;) 

Hope this help.Here is an answer for Python 3.8 and OpenPyXL 3.0.0.I tried to avoid using the get_column_letter function but failed. This solution uses the newly introduced assignment expressions aka "walrus operator":

Division in Python 2.7. and 3.3 [duplicate]

Erzsebet

[Division in Python 2.7. and 3.3 [duplicate]](https://stackoverflow.com/questions/21316968/division-in-python-2-7-and-3-3)

How can I divide two numbers in Python 2.7 and get the result with decimals?I don't get it why there is difference:in Python 3: in Python 2:Isn't this a modulo actually?

2014-01-23 18:54:24Z

How can I divide two numbers in Python 2.7 and get the result with decimals?I don't get it why there is difference:in Python 3: in Python 2:Isn't this a modulo actually?In python 2.7, the / operator is integer division if inputs are integers.If you want float division (which is something I always prefer), just use this special import:See it here:Integer division is achieved by using //, and modulo by using %EDITAs commented by user2357112, this import has to be done before any other normal import.In Python 3, / is float divisionIn Python 2, / is integer division (assuming int inputs)In both 2 and 3, // is integer division(To get float division in Python 2 requires either of the operands be a float, either as 20. or float(20))In Python 2.x, make sure to have at least one operand of your division in float. Multiple ways you may achieve this as the following examples:"/" is integer division in python 2 so it is going to round to a whole number. If you would like a decimal returned, just change the type of one of the inputs to float:float(20)/15 #1.33333333

How would you do the equivalent of preprocessor directives in Python?

intrepion

[How would you do the equivalent of preprocessor directives in Python?](https://stackoverflow.com/questions/482014/how-would-you-do-the-equivalent-of-preprocessor-directives-in-python)

Is there a way to do the following preprocessor directives in Python?

2009-01-27 01:10:43Z

Is there a way to do the following preprocessor directives in Python?There's __debug__, which is a special value that the compiler does preprocess. __debug__ will be replaced with a constant 0 or 1 by the compiler, and the optimizer will remove any if 0: lines before your source is interpreted.I wrote a python preprocessor called pypreprocessor that does exactly what you're describing.The source and documentation is available on GitHub.The package can also be downloaded/installed through the PyPI.Here's an example to accomplish what you're describing.pypreprocessor is capable of a lot more than just on-the-fly preprocessing. To see more use case examples check out the project on Google Code.Update: More info on pypreprocessorThe way I accomplish the preprocessing is simple. From the example above, the preprocessor imports a pypreprocessor object that's created in the pypreprocessor module. When you call parse() on the preprocessor it self-consumes the file that it is imported into and generates a temp copy of itself that comments out all of the preprocessor code (to avoid the preprocessor from calling itself recursively in an infinite loop) and comments out all of the unused portions.Commenting out the lines is, as opposed to removing them, is necessary to preserve line numbers on error tracebacks if the module throws an exception or crashes. And I've even gone as far as to rewrite the error traceback to report reflect the proper file name of the module that crashed.Then, the generated file containing the postprocessed code is executed on-the-fly.The upside to using this method over just adding a bunch of if statements inline in the code is, there will be no execution time wasted evaluating useless statements because the commented out portions of the code will be excluded from the compiled .pyc files.The downside (and my original reason for creating the module) is that you can't run both python 2x and python 3x in the same file because pythons interpreter runs a full syntax check before executing the code and will reject any version specific code before the preprocessor is allowed to run ::sigh::. My original goal was to be able to develop 2x and 3x code side-by-side in the same file that would create version specific bytecode depending on what it is running on.Either way, the preprocessor module is still very useful for implementing common c-style preprocessing capabilities. As well as, the preprocessor is capable of outputting the postprocessed code to a file for later use if you want.Also, if you want to generate a version that has all of the preprocessor directives as well as any of the #ifdefs that are excluded removed it's as simple as setting a flag in the preprocessor code before calling parse(). This makes removing unwanted code from a version specific source file a one step process (vs crawling through the code and removing if statements manually). I suspect you're gonna hate this answer.  The way you do that in Python isSince python is an interpreter, there's no preprocessing step to be applied, and no particular advantage to having a special syntax.  You can use the preprocessor in Python. Just run your scripts through the cpp (C-Preprocessor) in your bin directory. However I've done this with Lua and the benefits of easy interpretation have outweighed the more complex compilation IMHO.You can just use the normal language constructs:An alternative method is to use a bash script to comment out portions of code which are only relevant to debugging. Below is an example script which comments out lines that have a '#DEBUG' statement in it. It can also remove these comment markers again.Use a common m4 instead, like this:

Drop rows containing empty cells from a pandas DataFrame

Amrita Sawant

[Drop rows containing empty cells from a pandas DataFrame](https://stackoverflow.com/questions/29314033/drop-rows-containing-empty-cells-from-a-pandas-dataframe)

I have a pd.DataFrame that was created by parsing some excel spreadsheets. A column of which has empty cells. For example, below is the output for the frequency of that column, 32320 records have missing values for Tenant.I am trying to drop rows where Tenant is missing, however .isnull() option does not recognize the missing values. The column has data type "Object". What is happening in this case? How can I drop records where Tenant is missing?

2015-03-28 05:30:48Z

I have a pd.DataFrame that was created by parsing some excel spreadsheets. A column of which has empty cells. For example, below is the output for the frequency of that column, 32320 records have missing values for Tenant.I am trying to drop rows where Tenant is missing, however .isnull() option does not recognize the missing values. The column has data type "Object". What is happening in this case? How can I drop records where Tenant is missing?Pandas will recognise a value as null if it is a np.nan object, which will print as NaN in the DataFrame. Your missing values are probably empty strings, which Pandas doesn't recognise as null. To fix this, you can convert the empty stings (or whatever is in your empty cells) to np.nan objects using replace(), and then call dropna()on your DataFrame to delete rows with null tenants.To demonstrate, we create a DataFrame with some random values and some empty strings in a Tenants column:Now we replace any empty strings in the Tenants column with np.nan objects, like so:Now we can drop the null values:value_counts omits NaN by default so you're most likely dealing with "".So you can just filter them out likeEmpty strings are falsy, which means you can you filter on bool values like this:If your goal is to remove not only empty strings, but also strings only containing whitespace, use str.strip beforehand:.astype is a vectorised operation, this is faster than every option presented thus far. At least, from my tests. YMMV.Here is a timing comparison, I've thrown in some other methods I could think of. Benchmarking code, for reference:There's a situation where the cell has white space, you can't see it, use to replace white space as NaN, then You can use this variation:This will output(** - highlighting only desired rows):So to drop everything that does not have an 'education' value, use the code below:('~' indicating NOT)Result:

How can I plot a confusion matrix? [duplicate]

minks

[How can I plot a confusion matrix? [duplicate]](https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix)

I am using scikit-learn for classification of text documents(22000) to 100 classes. I use scikit-learn's confusion matrix method for computing the confusion matrix.This is how my confusion matrix looks like:However, I do not receive a clear or legible plot. Is there a better way to do this?

2016-02-23 08:06:07Z

I am using scikit-learn for classification of text documents(22000) to 100 classes. I use scikit-learn's confusion matrix method for computing the confusion matrix.This is how my confusion matrix looks like:However, I do not receive a clear or legible plot. Is there a better way to do this?you can use plt.matshow() instead of plt.imshow() or you can use seaborn module's heatmap (see documentation) to plot the confusion matrix@bninopaul 's answer is not completely for beginnershere is the code you can "copy and run" IF you want more data in you confusion matrix, including "totals column" and "totals line", and percents (%) in each cell, like matlab default (see image below)  including the Heatmap and other options...You should have fun with the module above, shared in the github  ; )https://github.com/wcipriano/pretty-print-confusion-matrixThis module can do your task easily and produces the output above with a lot of params to customize your CM:

Strip / trim all strings of a dataframe

bold

[Strip / trim all strings of a dataframe](https://stackoverflow.com/questions/40950310/strip-trim-all-strings-of-a-dataframe)

Cleaning the values of a multitype data frame in python/pandas, I want to trim the strings. I am currently doing it in two instructions :This is quite slow, what could I improve ?

2016-12-03 17:08:55Z

Cleaning the values of a multitype data frame in python/pandas, I want to trim the strings. I am currently doing it in two instructions :This is quite slow, what could I improve ?You can use DataFrame.select_dtypes to select string columns and then apply function str.strip.Notice: Values cannot be types like dicts or lists, because their dtypes is object.But if there are only a few columns use str.strip:Here's a compact version of using applymap with a straightforward lambda expression to call strip only when the value is of a string type: A more complete example:Here's a working example hosted by trinket:

https://trinket.io/python3/e6ab7fb4abIf you really want to use regex, thenBut it should be faster to do it like this:You can try:or more specifically for all string columnsYou can use the apply function of the Series object:Another option - use the apply function of the DataFrame object:

Joining multiple strings if they are not empty in Python

Goran

[Joining multiple strings if they are not empty in Python](https://stackoverflow.com/questions/8626694/joining-multiple-strings-if-they-are-not-empty-in-python)

I have four strings and any of them can be empty. I need to join them into one string with spaces between them. If I use:The result is a blank space on the beginning of the new string if string1 is empty. Also, I have three blank spaces if string2 and string3 are empty. How can I easily join them without blank spaces when I don't need them?

2011-12-24 20:23:14Z

I have four strings and any of them can be empty. I need to join them into one string with spaces between them. If I use:The result is a blank space on the beginning of the new string if string1 is empty. Also, I have three blank spaces if string2 and string3 are empty. How can I easily join them without blank spaces when I don't need them?By using None in the filter() call, it removes all falsy elements.If you KNOW that the strings have no leading/trailing whitespace:otherwise:and if any of the strings have non-leading/trailing whitespace, you may need to work harder still. Please clarify what it is that you actually have.

Transformlist of tuplesinto a flat list or a matrix

garth

[Transformlist of tuplesinto a flat list or a matrix](https://stackoverflow.com/questions/10632839/transform-list-of-tuples-into-a-flat-list-or-a-matrix)

With Sqlite, a "select..from" command returns the results "output", which prints (in python):It seems to be a list of tuples. I would like to either convert "output" in a simple 1D array (=list in Python I guess):or a 2x3 matrix:to be read via "output[i][j]"The flatten command does not do the job for the 1st option, and I have no idea for the second one... :)Could you please give me a hint?  Some thing fast would be great as real data are much bigger (here is just a simple example).

2012-05-17 09:16:58Z

With Sqlite, a "select..from" command returns the results "output", which prints (in python):It seems to be a list of tuples. I would like to either convert "output" in a simple 1D array (=list in Python I guess):or a 2x3 matrix:to be read via "output[i][j]"The flatten command does not do the job for the 1st option, and I have no idea for the second one... :)Could you please give me a hint?  Some thing fast would be great as real data are much bigger (here is just a simple example).By far the fastest (and shortest) solution posted:About 50% faster than the itertools solution, and about 70% faster than the map solution.List comprehension approach that works with Iterable types and is faster than other methods shown here.l is the list to flatten (called output in the OP's case)timeit result = 7.67 s  129 ns per looptimeit result = 11 s  433 ns per looptimeit result = 24.2 s  269 ns per loopIn Python 3 you can use the * syntax to flatten a list of iterables:use itertools chain:Or you can flatten the list like this:Update: Flattening using extend but without comprehension and without using list as iterator (fastest)After checking the next answer to this that provided a faster solution via a list comprehension with dual for I did a little tweak and now it performs better, first the execution of list(...) was dragging a big percentage of time, then changing a list comprehension for a simple loop shaved a bit more as well. The new solution is:Older: Flattening with map/extend:Flattening with list comprehension instead of mapsome timeits for new extend and the improvement gotten by just removing list(...) for [...]:you could easily move from list of tuple to single list as shown above.This is what numpy was made for, both from a data structures, as well as speed perspective.In case of arbitrary nested lists(just in case):

Convert ConfigParser.items('') to dictionary

Szymon Lipiski

[Convert ConfigParser.items('') to dictionary](https://stackoverflow.com/questions/1773793/convert-configparser-items-to-dictionary)

How can I convert the result of a ConfigParser.items('section') to a dictionary to format a string like here:

2009-11-20 23:11:28Z

How can I convert the result of a ConfigParser.items('section') to a dictionary to format a string like here:This is actually already done for you in config._sections.  Example:And then:Edit:  My solution to the same problem was downvoted so I'll further illustrate how my answer does the same thing without having to pass the section thru dict(), because config._sections is provided by the module for you already.Example test.ini:Magic happening:So this solution is not wrong, and it actually requires one less step.  Thanks for stopping by!Have you tried?How I did it in just one line.No more than other answers but when it is not the real businesses of your method and you need it just in one place use less lines and take the power of dict comprehension could be useful. I know this was asked a long time ago and a solution chosen, but the solution selected does not take into account defaults and variable substitution. Since it's the first hit when searching for creating dicts from parsers, thought I'd post my solution which does include default and variable substitutions by using ConfigParser.items().A convenience function to do this might look something like:For an individual section, e.g. "general", you can do:Here is another approach using Python 3.7 with configparser and ast.literal_eval:game.inigame.pyoutputCombining Michele d'Amico and Kyle's answer (no dict),

produces a less readable but somehow compelling:

Execute a file with arguments in Python shell

olidev

[Execute a file with arguments in Python shell](https://stackoverflow.com/questions/5788891/execute-a-file-with-arguments-in-python-shell)

I would like to run a command in Python Shell to execute a file with an argument.For example: execfile("abc.py") but how to add 2 arguments?

2011-04-26 10:18:11Z

I would like to run a command in Python Shell to execute a file with an argument.For example: execfile("abc.py") but how to add 2 arguments?execfile runs a Python file, but by loading it, not as a script. You can only pass in variable bindings, not arguments.If you want to run a program from within Python, use subprocess.call. E.g.try this:Note that when abc.py finishes, control will be returned to the calling program.  Note too that abc.py can call quit() if indeed finished.Actually, wouldn't we want to do this?You're confusing loading a module into the current interpreter process and calling a Python script externally. The former can be done by importing the file you're interested in. execfile is similar to importing but it simply evaluates the file rather than creates a module out of it. Similar to "sourcing" in a shell script. The latter can be done using the subprocess module. You spawn off another instance of the interpreter and pass whatever parameters you want to that. This is similar to shelling out in a shell script using backticks. For more interesting scenarios, you could also look at the runpy module. Since python 2.7, it has the run_path function. E.g:You can't pass command line arguments with execfile(). Look at subprocess instead.If you set PYTHONINSPECT in the python file you want to execute[repl.py]there is no need to use execfile, and you can directly run the file with arguments as usual in the shell:Besides subprocess.call, you can also use subprocess.Popen. Like the followingsubprocess.Popen(['./script', arg1, arg2])If you want to run the scripts in parallel and give them different arguments you can do like below. 

'WSGIRequest' object has no attribute 'user' Django admin

Gonalo Correia

['WSGIRequest' object has no attribute 'user' Django admin](https://stackoverflow.com/questions/37949198/wsgirequest-object-has-no-attribute-user-django-admin)

When I trying to access the admin page it gives me the following error:Im quite new in django ... but I followed this tutorial: https://docs.djangoproject.com/en/1.9/ref/contrib/admin/I dont have any custom AdminSites and custom AdminModels.I already googled about this problem but still I cannot solve it for my case in any way. Can you help ?here is my settings.py:and admin.py:

2016-06-21 15:55:52Z

When I trying to access the admin page it gives me the following error:Im quite new in django ... but I followed this tutorial: https://docs.djangoproject.com/en/1.9/ref/contrib/admin/I dont have any custom AdminSites and custom AdminModels.I already googled about this problem but still I cannot solve it for my case in any way. Can you help ?here is my settings.py:and admin.py:To resolve this go to settings.py where there is new-style MIDDLEWARE (introduced in Django 1.10)Change that to old-style MIDDLEWARE_CLASSEShttps://docs.djangoproject.com/en/stable/topics/http/middleware/#upgrading-pre-django-1-10-style-middlewareI found the answer. The variable name on:MIDDLEWARE is the new-style configuration introduced in Django 1.10. Change the name to MIDDLEWARE_CLASSES and now it works.So now the code is:In case anyone is having this problem with Django 2.0, the following configuration with new-style MIDDLEWARE seems to work (docs here):In case anyone is having the same problem in django 2.0.2 or later,just updatewith It worked for me cause i created my project with

django 1.0.x but later updated to django 2.0.2You should not change MIDDLEWARE to MIDDLEWARE_CLASSES. What happens here is more likely that you created the app with django 1.10 and now you are running it with 1.9 or a previous version. Make sure you use a specific version of django(and all other libraries) so your project doesn't break when deploying or running on different machines. If you have a stable codebase simply run:And then when deploying or setting up a new env just do:You should always consider using fixed versions of your libraries(and hopefully virtual envs), and when upgrading dependencies test each version change.My solution was that my Django ver. was 1.9 I reinstalled back to 1.10 without changing MIDDLEWARE to MIDDLEWARE_CLASSES.Simple.....If you have picked the code from somewhere else(mean project is not created on your pc)... then their may be a different configuration of MIDDLEWARE in setting file in your code.....so you need to just replace that MIDDLLEWARE from the one that your django produces( create a throwaway project->go to setting files ---> copy that MIDDLEWARE part and paste it in the project in which you are getting error).  I had a similar error in my production server and thanks to sentry's breadcrumbs I saw that the error that was raising had to do with my settings, especially the ALLOWED_HOSTS. Django version 1.10.8 with python 2.7.My previous settings: Sentry Breadcrumbs screen shot:

After that I looked around and found this:Link to Django official docsSo my final settings that solved my problem:Hope this was useful :)

Flask and uWSGI - unable to load app 0 (mountpoint='') (callable not found or import error)

Tampa

[Flask and uWSGI - unable to load app 0 (mountpoint='') (callable not found or import error)](https://stackoverflow.com/questions/12030809/flask-and-uwsgi-unable-to-load-app-0-mountpoint-callable-not-found-or-im)

I get the below error when I try and start Flask using uWSGI.

Here is how I start:Here is my directory structure:Contents of /path/to/folder/run.pyContents of /path/to/folder/app/__init__.py

2012-08-19 23:30:49Z

I get the below error when I try and start Flask using uWSGI.

Here is how I start:Here is my directory structure:Contents of /path/to/folder/run.pyContents of /path/to/folder/app/__init__.pyuWSGI doesn't load your app as __main__, so it never will find the app (since that only gets loaded when the app is run as name __main__). Thus, you need to import it outside of the if __name__ == "__main__": block.Really simple change:Now you can run the app directly with python run.py or run it through uWSGI the way you have it.NOTE: if you set --callable myapp, you'd need to change it from as application to myapp (by default uwsgi expects applicationI had problems with the accepted solution because my flask app was in a variable called app.  You can solve that with putting just this in your wsgi:So the problem was simply that uwsgi expects a variable called application.The uWSGI error unable to load app 0 (mountpoint='') (callable not found or import error) occured for me if I left out the last two lines of the following minimal working example for Flask applicationI am aware that this already implicitly said within the comments to another answer, but it still took me a while to figure that out, so I hope to save others' time.In the case of a pure Python Dash application, I can offer the following minimal viable code snippet:Again, the application = app.server is the essential part here.

Thread local storage in Python

Casebash

[Thread local storage in Python](https://stackoverflow.com/questions/1408171/thread-local-storage-in-python)

How do I use thread local storage in Python?

2009-09-10 23:03:33Z

How do I use thread local storage in Python?Thread local storage is useful for instance if you have a thread worker pool and each thread needs access to its own resource, like a network or database connection. Note that the threading module uses the regular concept of threads (which have access to the process global data), but these are not too useful due to the global interpreter lock. The different multiprocessing module creates a new sub-process for each, so any global will be thread local.Here is a simple example:This will print out:One important thing that is easily overlooked:  a threading.local() object only needs to be created once, not once per thread nor once per function call.  The global or class level are ideal locations.Here is why: threading.local() actually creates a new instance each time it is called (just like any factory or class call would), so calling threading.local() multiple times constantly overwrites the original object, which in all likelihood is not what one wants.  When any thread accesses an existing threadLocal variable (or whatever it is called), it gets its own private view of that variable.This won't work as intended:Will result in this output:All global variables are thread local, since the multiprocessing module creates a new process for each thread.Consider this example, where the processed counter is an example of thread local storage:It will output something like this:... of course, the thread IDs and the counts for each and order will vary from run to run.Thread-local storage can simply be thought of as a namespace (with values accessed via attribute notation). The difference is that each thread transparently gets its own set of attributes/values, so that one thread doesn't see the values from another thread.Just like an ordinary object, you can create multiple threading.local instances in your code. They can be local variables, class or instance members, or global variables. Each one is a separate namespace.Here's a simple example:Output:Note how each thread maintains its own counter, even though the ns attribute is a class member (and hence shared between the threads).The same example could have used an instance variable or a local variable, but that wouldn't show much, as there's no sharing then (a dict would work just as well). There are cases where you'd need thread-local storage as instance variables or local variables, but they tend to be relatively rare (and pretty subtle).As noted in the question, Alex Martelli gives a solution here. This function allows us to use a factory function to generate a default value for each thread.Can also writemydata.x will only exist in the current threadMy way of doing a thread local storage across modules / files. The following has been tested in Python 3.5 - In fileA, I start a thread which has a target function in another module/file.In fileB, I set a local variable I want in that thread. In fileC, I access the thread local variable of the current thread. Additionally, just print 'dictionary' variable so that you can see the default values available, like kwargs, args, etc.

Can I debug with python debugger when using py.test somehow?

Joel

[Can I debug with python debugger when using py.test somehow?](https://stackoverflow.com/questions/2678792/can-i-debug-with-python-debugger-when-using-py-test-somehow)

I am using py.test for unit testing my python program. I wish to debug my test code with the python debugger the normal way (by which I mean pdb.set_trace() in the code) but I can't make it work. Putting pdb.set_trace() in the code doesn't work (raises IOError: reading from stdin while output is captured). I have also tried running py.test with the option --pdb but that doesn't seem to do the trick if I want to explore what happens before my assertion. It breaks when an assertion fails, and moving on from that line means terminating the program.Does anyone know a way to get debugging, or is debugging and py.test just not meant to be together?

2010-04-20 21:28:14Z

I am using py.test for unit testing my python program. I wish to debug my test code with the python debugger the normal way (by which I mean pdb.set_trace() in the code) but I can't make it work. Putting pdb.set_trace() in the code doesn't work (raises IOError: reading from stdin while output is captured). I have also tried running py.test with the option --pdb but that doesn't seem to do the trick if I want to explore what happens before my assertion. It breaks when an assertion fails, and moving on from that line means terminating the program.Does anyone know a way to get debugging, or is debugging and py.test just not meant to be together?it's real simple: put an assert 0 where you want to start debugging in your code and run your tests with:done :) Alternatively, if you are using pytest-2.0.1 or above, there also is the pytest.set_trace() helper which you can put anywhere in your test code.  Here are the docs.  It will take care to internally disable capturing before sending you to the pdb debugger command-line.I found that I can run py.test with capture disabled, then use pdb.set_trace() as usual.The easiest way is using the py.test mechanism to create breakpointhttp://pytest.org/latest/usage.html#setting-a-breakpoint-aka-set-traceOr if you want pytest's debugger as a one-liner, change your import pdb; pdb.set_trace() into import pytest; pytest.set_trace() I'm not familiar with py.test, put for unittest, you do the following. Maybe py.test is similar:In your test module (mytestmodule.py):Then run the test withYou will get an interactive pdb shell.Looking at the docs, it looks like py.test has a --pdb command line option:http://codespeak.net/py/dist/test/features.html

Lazy logger message string evaluation

Zaar Hai

[Lazy logger message string evaluation](https://stackoverflow.com/questions/4148790/lazy-logger-message-string-evaluation)

I'm using standard python logging module in my python application:The issue is that although debug level is not enable, that stupid log message is evaluated on each loop iteration, which harms performance badly.Is there any solution for this?In C++ we have log4cxx package that provides macros like this:

LOG4CXX_DEBUG(logger, messasage) 

That effectively evaluates to But since there are no macros in Python (AFAIK), if there a efficient way to do logging?

2010-11-10 20:33:31Z

I'm using standard python logging module in my python application:The issue is that although debug level is not enable, that stupid log message is evaluated on each loop iteration, which harms performance badly.Is there any solution for this?In C++ we have log4cxx package that provides macros like this:

LOG4CXX_DEBUG(logger, messasage) 

That effectively evaluates to But since there are no macros in Python (AFAIK), if there a efficient way to do logging?The logging module already has partial support for what you want to do.  Do this:... instead of this:The logging module is smart enough to not produce the complete log message unless the message actually gets logged somewhere.To apply this feature to your specific request, you could create a lazyjoin class.Use it like this (note the use of a generator expression, adding to the laziness):Here is a demo that shows this works.In the demo, The logger.info() call hit the assertion error, while logger.debug() did not get that far.Of course the following is not as efficient as a Macro:but simple, evaluates in lazy fashion and is 4 times faster than the accepted answer:See benchmark-src for my setup.If you run the script, you'll notice the first logger.debug command does not take 20 seconds to execute. This shows the argument is not evaluated when the logging level is below the set level.As Shane points out, using... instead of this:saves some time by only performing the string formatting if the message is actually logged.This does not completely solve the problem, though, as you may have to pre-process the values to format into the string, such as:In that case, obj.get_a() and obj.get_b() will be computed even if no logging happens.A solution to that would be to use lambda functions, but this requires some extra machinery:... then you can log with the following:In that case, the lambda function will only be called if log.debug decides to perform the formatting, hence calling the __str__ method.Mind you: the overhead of that solution may very well exceed the benefit :-) But at least in theory, it makes it possible to do perfectly lazy logging.I present, Lazyfy:Usage:The original example:As you see, this also covers the other answer which uses lambda function, but uses more memory with the value atribute and expansion. However, it saves more memory with: Usage of __slots__?Finally, by far, the most efficient solution still being the following as suggested another answer:If you depend only on accessing global state attributes, you can instantiate a python class and lazify it by using the __str__ method:Related:

Base language of python

Dewsworld

[Base language of python](https://stackoverflow.com/questions/9451929/base-language-of-python)

What is the base language python written in? Actually I did a google search but not found any satisfying result.

2012-02-26 09:23:18Z

What is the base language python written in? Actually I did a google search but not found any satisfying result.You can't say that Python is written in some programming language, since Python as a language is just a set of rules (like syntax rules, or descriptions of standard functionality). So we might say, that it is written in English :).  However, mentioned rules can be implemented in some programming language. Hence, if you send a string like 'import this' to that program called interpreter, it'd return you "Zen of Python".Since most modern OS are written in C, compilers/interpreters for modern high-level languages are also written in C.  Python is not an exception - its most popular/"traditional" implementation is called CPython and is written in C.There are other implementations:The sources are public. Python is written in C (actually the default implementation is called CPython).Python is written in English.  But there are several implementations:it is written in C, its also called CPython.You get a good idea if you compile python from source. Usually it's gcc that compiles the *.c filesTo add to and reframe some of the other good answers:The specification for Python (question) is written in English, but could be written in a formal semantics, as Standard

ML and Scheme are.

See 

Programming language specification.There are implementations of Python in many languages, as noted by Gandaro, of which the fastest is surprisingly not the original CPython, which is written in C.

catch specific HTTP error in python

Arnab Sen Gupta

[catch specific HTTP error in python](https://stackoverflow.com/questions/3193060/catch-specific-http-error-in-python)

I want to catch a specific http error and not any one of the entire family..

what I was trying to do is --but what I end up is catching any kind of http error, but I want to catch only if the specified webpage doesn't exist!! probably that's HTTP error 404..but I don't know how to specify that catch only error 404 and let the system run the default handler for other events..ny suggestions??

2010-07-07 08:25:56Z

I want to catch a specific http error and not any one of the entire family..

what I was trying to do is --but what I end up is catching any kind of http error, but I want to catch only if the specified webpage doesn't exist!! probably that's HTTP error 404..but I don't know how to specify that catch only error 404 and let the system run the default handler for other events..ny suggestions??Python 3Python 2Just catch HTTPError, handle it, and if it's not Error 404, simply use raise to re-raise the exception. See the Python tutorial.e.g. complete example for Pyhton 2For Python 3.xTims answer seems to me as misleading. Especially when urllib2 does not return expected code. For example this Error will be fatal (believe or not - it is not uncommon one when downloading urls):Fast, but maybe not the best solution would be code using nested try/except block:

More information to the topic of nested try/except blocks Are nested try/except blocks in python a good programming practice?

How can I pretty-print ASCII tables with Python? [closed]

kdt

[How can I pretty-print ASCII tables with Python? [closed]](https://stackoverflow.com/questions/5909873/how-can-i-pretty-print-ascii-tables-with-python)

I'm looking for a way to pretty-print tables like this:I've found the asciitable library but it doesn't do the borders, etc. I don't need any complex formatting of data items, they're just strings. I do need it to auto-size columns.Do other libraries or methods exist, or do I need to spend a few minutes writing my own?

2011-05-06 10:09:21Z

I'm looking for a way to pretty-print tables like this:I've found the asciitable library but it doesn't do the borders, etc. I don't need any complex formatting of data items, they're just strings. I do need it to auto-size columns.Do other libraries or methods exist, or do I need to spend a few minutes writing my own?I've read this question long time ago, and finished writing my own pretty-printer for tables: tabulate.My use case is:Given your example, grid is probably the most similar output format:Other supported formats are plain (no lines), simple (Pandoc simple tables), pipe (like tables in PHP Markdown Extra), orgtbl (like tables in Emacs' org-mode), rst (like simple tables in reStructuredText). grid and orgtbl are easily editable in Emacs.Performance-wise, tabulate is slightly slower than asciitable, but much faster than PrettyTable and texttable.P.S. I'm also a big fan of aligning numbers by a decimal column. So this is the default alignment for numbers if there are any (overridable).Here's a quick and dirty little function I wrote for displaying the results from SQL queries I can only make over a SOAP API. It expects an input of a sequence of one or more namedtuples as table rows. If there's only one record, it prints it out differently.It is handy for me and could be a starting point for you:Sample output:ExampleFor some reason when I included 'docutils' in my google searches I stumbled across texttable, which seems to be what I'm looking for.I too wrote my own solution to this. I tried to keep it simple.https://github.com/Robpol86/terminaltablesVersion using w3m designed to handle the types MattH's version accepts:results in:If you want a table with column and row spans, then try my library dashtableWhich outputs:You can try BeautifulTable. It does what you want to do. Here's an example from it's documentationI know it the question is a bit old but here's my attempt at this: https://gist.github.com/lonetwin/4721748It is a bit more readable IMHO (although it doesn't differentiate between single / multiple rows like @MattH's solutions does, nor does it use NamedTuples).I use this small utility function.outputI just released termtables for this purpose. For example, thisgets youBy default, the table is rendered with Unicode box-drawing characters,termtables are very configurable; check out the tests for more examples.Here's my solution:This can be done with only builtin modules fairly compactly using list and string comprehensions.  Accepts a list of dictionaries all of the same format...Example:Output:

Difference between multiple if's and elif's?

Billjk

[Difference between multiple if's and elif's?](https://stackoverflow.com/questions/9271712/difference-between-multiple-ifs-and-elifs)

In python, is there a difference between say:andJust wondering if multiple ifs could cause any unwanted problems and if it would be better practice to use elifs.

2012-02-14 04:29:10Z

In python, is there a difference between say:andJust wondering if multiple ifs could cause any unwanted problems and if it would be better practice to use elifs.Multiple if's means your code would go and check all the if conditions, where as in case of elif, if one if condition satisfies it would not check other conditions..An other easy way to see the difference between the use of if and elif is this example here:Here you can see that when 18 is used as input the answer is (surprisingly) 2 sentences. That is wrong. It should only be the first sentence.That is because BOTH if statements are being evaluated. The computer sees them as two separate statements: The elif fixes this and makes the two if statements 'stick together' as one:Edit: corrected spellingYou can see that elif is slightly faster. This would be more apparent if there were more ifs and more elifs.Here's another way of thinking about this:Let's say you have two specific conditions that an if/else catchall structure will not suffice:Example:I have a 3 X 3 tic-tac-toe board and I want to print the coordinates of both diagonals and not the rest of the squares.  I decide to use and if/elif structure instead...The output is:But wait!  I wanted to include all three coordinates of diagonal2 since (1, 1) is part of diagonal 2 as well.The 'elif' caused a dependency with the 'if' so that if the original 'if' was satisfied the 'elif' will not initiate even if the 'elif' logic satisfied the condition as well.Let's change the second 'elif' to an 'if' instead.I now get the output that I wanted because the two 'if' statements are mutually exclusive.Ultimately knowing what kind or result you want to achieve will determine what type of conditional relationship/structure you code.In your above example there are differences, because your second code has indented the elif, it would be actually inside the if block, and is a syntactically and logically incorrect in this example.Python uses line indentions to define code blocks (most C like languages use {} to enclose a block of code, but python uses line indentions), so when you are coding, you should consider the indentions seriously.your sample 1:both if and elif are indented the same, so they are related to the same logic.

your second example:elif is indented more than if, before another block encloses it, so it is considered inside the if block. and since inside the if there is no other nested if, the elif is being considered as a syntax error by Python interpreter.elifis just a fancy way of expressing else: if,Multiple ifs execute multiple branches after testing, while the elifs are mutually exclusivly, execute acutally one branch after testing.Take user2333594's examplescould be rephrased as:The other example could be :When you use multiple if, your code will go back in every if statement to check the whether the expression suits your condition.

Sometimes, there are instances of sending many results for a single expression which you will not even expect.

But using elif terminates the process when the expression suits any of your condition.Here's how I break down control flow statements:will print hey and hiwill print hi only because the preceding statement evaluated to Falsewill print hello because all preceding statements failed to execute

Why are 0d arrays in Numpy not considered scalar?

Salim Fadhley

[Why are 0d arrays in Numpy not considered scalar?](https://stackoverflow.com/questions/773030/why-are-0d-arrays-in-numpy-not-considered-scalar)

Surely a 0d array is scalar, but Numpy does not seem to think so... am I missing something or am I just misunderstanding the concept? 

2009-04-21 15:02:51Z

Surely a 0d array is scalar, but Numpy does not seem to think so... am I missing something or am I just misunderstanding the concept? One should not think too hard about it. It's ultimately better for the mental health and longevity of the individual.The curious situation with Numpy scalar-types was bore out of the fact that there is no graceful and consistent way to degrade the 1x1 matrix to scalar types. Even though mathematically they are the same thing, they are handled by very different code.If you've been doing any amount of scientific code, ultimately you'd want things like max(a) to work on matrices of all sizes, even scalars. Mathematically, this is a perfectly sensible thing to expect. However for programmers this means that whatever presents scalars in Numpy should have the .shape and .ndim attirbute, so at least the ufuncs don't have to do explicit type checking on its input for the 21 possible scalar types in Numpy. On the other hand, they should also work with existing Python libraries that does do explicit type-checks on scalar type. This is a dilemma, since a Numpy ndarray have to individually change its type when they've been reduced to a scalar, and there is no way of knowing whether that has occurred without it having do checks on all access. Actually going that route would probably make bit ridiculously slow to work with by scalar type standards.The Numpy developer's solution is to inherit from both ndarray and Python scalars for its own scalary type, so that all scalars also have .shape, .ndim, .T, etc etc. The 1x1 matrix will still be there, but its use will be discouraged if you know you'll be dealing with a scalar. While this should work fine in theory, occasionally you could still see some places where they missed with the paint roller, and the ugly innards are exposed for all to see:There's really no reason why a[...] and a[()] should return different things, but it does. There are proposals in place to change this, but looks like they forgot to finish the job for 1x1 arrays.A potentially bigger, and possibly non-resolvable issue, is the fact that Numpy scalars are immutable. Therefore "spraying" a scalar into a ndarray, mathematically the adjoint operation of collapsing an array into a scalar, is a PITA to implement. You can't actually grow a Numpy scalar, it cannot by definition be cast into an ndarray, even though newaxis mysteriously works on it:In Matlab, growing the size of a scalar is a perfectly acceptable and brainless operation. In Numpy you have to stick jarring a = array(a) everywhere you think you'd have the possibility of starting with a scalar and ending up with an array. I understand why Numpy has to be this way to play nice with Python, but that doesn't change the fact that many new switchers are deeply confused about this. Some have explicit memory of struggling with this behaviour and eventually persevering, while others who are too far gone are generally left with some deep shapeless mental scar that frequently haunts their most innocent dreams. It's an ugly situation for all.You have to create the scalar array a little bit differently:It looks like scalars in numpy may be a bit different concept from what you may be used to from a purely mathematical standpoint.  I'm guessing you're thinking in terms of scalar matricies?

How to compare each item in a list with the rest, only once?

Bogdanovist

[How to compare each item in a list with the rest, only once?](https://stackoverflow.com/questions/16603282/how-to-compare-each-item-in-a-list-with-the-rest-only-once)

Say I have an array/list of things I want to compare. In languages I am more familiar with, I would do something likeThis ensures we only compare each pair once. For some context, I am doing collision detection on a bunch of objects contained in the list. For each collision detected, a small 'collision' object describing the collision is appended to a list, which another routine then loops through resolving each collision (depending on the nature of the two colliding objects). Obviously, I only want to report each collision once.Now, what is the pythonic way of doing this, since Python favors using iterators rather than looping over indices?I had the following (buggy) code:But this clearly picks up each collision twice, which lead to some strange behavior when trying to resolve them. So what is the pythonic solution here?

2013-05-17 07:02:12Z

Say I have an array/list of things I want to compare. In languages I am more familiar with, I would do something likeThis ensures we only compare each pair once. For some context, I am doing collision detection on a bunch of objects contained in the list. For each collision detected, a small 'collision' object describing the collision is appended to a list, which another routine then loops through resolving each collision (depending on the nature of the two colliding objects). Obviously, I only want to report each collision once.Now, what is the pythonic way of doing this, since Python favors using iterators rather than looping over indices?I had the following (buggy) code:But this clearly picks up each collision twice, which lead to some strange behavior when trying to resolve them. So what is the pythonic solution here?Of course this will generate each pair twice as each for loop will go through every item of the list.You could use some itertools magic here to generate all possible combinations:itertools.combinations will pair each element with each other element in the iterable, but only once.You could still write this using index-based item access, equivalent to what you are used to, using nested for loops:Of course this may not look as nice and pythonic but sometimes this is still the easiest and most comprehensible solution, so you should not shy away from solving problems like that.Use itertools.combinations(mylist, 2)I think using enumerate on the outer loop and using the index to slice the list on the inner loop is pretty Pythonic:Your solution is correct, but your outer loop is still longer than needed. You don't need to compare the last element with anything else because it's been already compared with all the others in the previous iterations. Your inner loop still prevents that, but since we're talking about collision detection you can save the unnecessary check.Using the same language you used to illustrate your algorithm, you'd come with something like this:This code will count frequency and remove duplicate elements:

Django. You don't have permission to edit anything

Sergei Basharov

[Django. You don't have permission to edit anything](https://stackoverflow.com/questions/3718077/django-you-dont-have-permission-to-edit-anything)

I created a little app a while ago. I created admin.py and used admin.site.register(MenuEntry) to add the class to admin console. It showed the items of that class just fine.

Then I began working on another app and created everything as before. But now it says: 

You don't have permission to edit anything.

I compared files from that and from this apps and they look quite similar, so I just can't find the difference and I can't realize what to do now to make it work.

2010-09-15 13:31:48Z

I created a little app a while ago. I created admin.py and used admin.site.register(MenuEntry) to add the class to admin console. It showed the items of that class just fine.

Then I began working on another app and created everything as before. But now it says: 

You don't have permission to edit anything.

I compared files from that and from this apps and they look quite similar, so I just can't find the difference and I can't realize what to do now to make it work.I checked files one more time and found the difference. I forgot to add admin.autodiscover() in urls.py of the project. Thanks.I had another case where this happened. I had an app called "transcription", with two models: Project and Recording. After getting it mostly developed I decided to rename the app "recordings". The admin app worked fine as the admin but any non-admin user got this error message. Eventually I found (in my sqlite db) the table django_content_type. It had these records:Somewhere along the way I had managed to add two (almost - don't know why "recording" in record 10) correct records while leaving the now

incorrect records intact. The admin user worked just fine (I wonder why), but any other group got the error. When I 

looked at auth_group_permissions I saw that only records 8 and 9 were being assigned and of course there

was no longer an app called "transcription". Hence the error.I deleted records 10 and 11 and changed the app_labels of 8 and 9 to "recordings" and there's joy in 

Mudville.Upgrade your Django to 1.7 or more, This problem will be automatically solved.Upgrading Django:I was receiving the same error and had to refactor the appname as it conflicted with one of the modules being used.

My app's name was admin and I was also using Django's admin.Check the link - Change app's name, on how to do it.I just simply Removed all currently installed versions of Django. Then freshly install the latest version of Django and it works

Logging, StreamHandler and standard streams

L1ker

[Logging, StreamHandler and standard streams](https://stackoverflow.com/questions/1383254/logging-streamhandler-and-standard-streams)

I can't figure out how to log info-level messages to stdout, but everything else to stderr. I already read this http://docs.python.org/library/logging.html. Any suggestion?

2009-09-05 12:45:17Z

I can't figure out how to log info-level messages to stdout, but everything else to stderr. I already read this http://docs.python.org/library/logging.html. Any suggestion?The following script, log1.py:when run, produces the following results.As you'd expect, since on a terminal sys.stdout and sys.stderr are the same. Now, let's redirect stdout to a file, tmp:So the INFO message has not been printed to the terminal - but the messages directed to sys.stderr have been printed. Let's look at what's in tmp:So that approach appears to do what you want.Generally, I think it makes sense to redirect messages lower than WARNING to stdout, instead of only INFO messages.Based on Vinay Sajip's excellent answer, I came up with this:Since my edit was rejected, here's my answer. @goncalopp's answer is good but doesn't stand alone or work out of the box. Here's my improved version:Simplest handler to send colored output to stderr:Use with:Or you can even apply the formatter directly to the current log handler, with no need for ColorStderr:Try This Monkey Patch ~~TestTest Output

Get lat/long given current point, distance and bearing

David M

[Get lat/long given current point, distance and bearing](https://stackoverflow.com/questions/7222382/get-lat-long-given-current-point-distance-and-bearing)

Given an existing point in lat/long, distance in (in KM) and bearing (in degrees converted to radians), I would like to calculate the new lat/long. This site crops up over and over again, but I just can't get the formula to work for me. The formulas as taken the above link are:The above formula is for MSExcel where-  Here's the code I've got in Python.I get 

2011-08-28 16:56:14Z

Given an existing point in lat/long, distance in (in KM) and bearing (in degrees converted to radians), I would like to calculate the new lat/long. This site crops up over and over again, but I just can't get the formula to work for me. The formulas as taken the above link are:The above formula is for MSExcel where-  Here's the code I've got in Python.I get Needed to convert answers from radians back to degrees. Working code below:The geopy library supports this:Found via https://stackoverflow.com/a/4531227/37610May be a bit late for answering, but after testing the other answers, it appears they don't work correctly. Here is a PHP code we use for our system. Working in all directions.PHP code:This question is known as the direct problem in the study of geodesy. This is indeed a very popular question and one that is a constant cause of confusion. The reason is that most people are looking for a simple and straight-forward answer. But there is none, because most people asking this question are not supplying enough information, simply because they are not aware that:Therefore there are many different assumptions used in the various geometric models that apply differently, depending on your needed accuracy. So to answer the question you need to consider to what accuracy you would like to have your result. Some examples:So you can have many choices in which algorithm to use. In addition each programming language has it's own implementation or "package" multiplied by number of models and the model developers specific needs. For all practical purposes here, it pays off to ignore any other language apart javascript, since it very closely resemble pseudo-code by its nature. Thus it can be easily converted to any other language, with minimal changes.Then the main models are: References:lon1 and lat1 in degreesbrng = bearing in radiansd = distance in kmR = radius of the Earth in kmI implemented your algorithm and mine in PHP and benchmarked it. This version ran in about 50% of the time. The results generated were identical, so it seems to be mathematically equivalent.I didn't test the python code above so there might be syntax errors.Quick way using geopyI ported answer by Brad to vanilla JS answer, with no Bing maps dependencyhttps://jsfiddle.net/kodisha/8a3hcjtd/In addition, I added geoJSON export, so you can simply paste resulting geoJSON to 

http://geojson.io/#map=17/41.89017/12.49171 and see results instantly.Result: 

Also late but for those who might find this, you will get more accurate results using the geographiclib library. Check out the geodesic problem descriptions and the JavaScript examples for an easy introduction to how to use to answer the subject question as well as many others. Implementations in a variety of languages including Python. Far better than coding your own if you care about accuracy; better than VincentyDistance in the earlier "use a library" recommendation. As the documentation says: "The emphasis is on returning accurate results with errors close to round-off (about 515 nanometers)."Just interchange the values in the atan2(y,x) function. Not atan2(x,y)!I ported the Python to Javascript. This returns a Bing Maps Location object, you can change to whatever you like.Here is a PHP version based on Ed Williams Aviation Formulary.  Modulus is handled a little different in PHP.  This works for me.I ported the answer from @David M to java if anyone wanted this... I do get a slight different result of 52.20462299620793, 0.360433887489931

Check if all values in list are greater than a certain number

O.rka

[Check if all values in list are greater than a certain number](https://stackoverflow.com/questions/20229822/check-if-all-values-in-list-are-greater-than-a-certain-number)

How to I check if all values in list are >= 30? my_list1 should work and my_list2 should not.The only thing I could think of doing was:In hindsight, after dealing with bigger datasets where speed actually matters and utilizing numpy...I would do this:You could also do something like: 

2013-11-26 23:01:18Z

How to I check if all values in list are >= 30? my_list1 should work and my_list2 should not.The only thing I could think of doing was:In hindsight, after dealing with bigger datasets where speed actually matters and utilizing numpy...I would do this:You could also do something like: Use the all() function with a generator expression:Note that this tests for greater than or equal to 30, otherwise my_list1 would not pass the test either.If you wanted to do this in a function, you'd use:e.g. as soon as you find a value that proves that there is a value below 30, you return False, and return True if you found no evidence to the contrary.Similarly, you can use the any() function to test if at least 1 value matches the condition....any reason why you can't use min()?I don't know if this is exactly what you want, but technically, this is what you asked for...There is a builtin function all:Being limit the value greater than which all numbers must be.You can use all():Note that this includes all numbers equal to 30 or higher, not strictly above 30.The overall winner between using the np.sum, np.min, and all seems to be np.min in terms of speed for large arrays:(i need to put the np.array definition inside the function, otherwise the np.min function remembers the value and does not do the computation again when testing for speed with timeit)The performance of "all" depends very much on when the first element that does not satisfy the criteria is found, the np.sum needs to do a bit of operations, the np.min is the lightest in terms of computations in the general case.When the criteria is almost immediately met and the all loop exits fast, the all function is winning just slightly over np.min:But when "all" needs to go through all the points, it is definitely much worse, and the np.min wins:But usingcan be very useful is one wants to know how many values are below x.You could do the following:This will return the values that are greater than 30 as True, and the values that are smaller as false.I write this functionThenEmpty list on min() will raise ValueError. So I added if not x in condition.

Can generators be recursive?

Aguy

[Can generators be recursive?](https://stackoverflow.com/questions/38254304/can-generators-be-recursive)

I naively tried to create a recursive generator. Didn't work. This is what I did: All I got was the first item 6. Is there a way to make such code work? Essentially transferring the yield command to the level above in a recursion scheme?

2016-07-07 20:01:20Z

I naively tried to create a recursive generator. Didn't work. This is what I did: All I got was the first item 6. Is there a way to make such code work? Essentially transferring the yield command to the level above in a recursion scheme?Try this:I should point out this doesn't work because of a bug in your function. It should probably include a check that lis isn't empty, as shown below:In case you are on Python 2.7 and don't have yield from, check this question out.In your code, the generator function:The second instance of the iterator, the one recursively created, is never being iterated over. That's why you only got the first item of the list.A generator function is useful to automatically create an iterator object (an object that implements the iterator protocol), but then you need to iterate over it: either manually calling the next() method on the object or by means of a loop statement that will automatically use the iterator protocol.The answer is yes. Now back to your code, if you really want to do this with a generator function, I guess you could try:Note: the items are returned in reversed order, so you might want to use some_list.reverse() before calling the generator the first time.The important thing to note in this example is: the generator function recursively calls itself in a for loop, which sees an iterator and automatically uses the iteration protocol on it, so it actually gets values from it.This works, but I think this is really not useful. We are using a generator function to iterate over a list and just get the items out, one at a time, but... a list is an iterable itself, so no need for generators! 

Of course I get it, this is just an example, maybe there are useful applications of this idea. Let's recycle the previous example (for lazyness). Lets say we need to print the items in a list, adding to every item the count of previous items (just a random example, not necessarily useful). The code would be:Now, as you can see, the generator function is actually doing something before returning list items AND the use of recursion starts to make sense. Still, just a stupid example, but you get the idea.Note: off course, in this stupid example the list is expected to contain only numbers. If you really want to go  try and break it, just put in a string in some_list and have fun. Again, this is only an example, not production code!Recursive generators are useful for traversing non-linear structures.  For example, let a binary tree be either None or a tuple of value, left tree, right tree.  A recursive generator is the easiest way to visit all nodes. Example:Edit: replace if tree with if tree is not None to catch other false values as errors.Edit 2: about putting the recursive calls in the try: clause (comment by @jpmc26).For bad nodes, the code above just logs the ValueError and continues.  If, for instance, (9,None,None) is replaced by (9,None), the output isMore typical would be to reraise after logging, making the output beThe traceback gives the path from the root to the bad node.  One could wrap the original visit(tree) call to reduce the traceback to the path: (root, right, right, left, left).If the recursive calls are included in the try: clause, the error is recaught, relogged, and reraised at each level of the tree.The multiple logging reports are likely more noise than help.  If one wants the path to the bad node, it might be easiest to wrap each recursive call in its own try: clause and raise a new ValueError at each level, with the contructed path so far.Conclusion: if one is not using an exception for flow control (as may be done with IndexError, for instance) the presence and placements of try: statements depends on the error reporting one wants.Up to Python 3.4, a generator function used to have to raise StopIteration exception when it is done.

For the recursive case other exceptions (e.g. IndexError) are raised earlier than StopIteration, therefore we add it manually.Note that for loop will catch StopIteration exception.

More about this hereYes you can have recursive generators. However, they suffer from the same recursion depth limit as other recursive functions.This loop gets to about 3000 (for me) before crashing.However, with some trickery, you can create a function that feeds a generator to itself. This allows you to write generators like they are recursive but are not: https://gist.github.com/3noch/7969f416d403ba3a54a788b113c204ceThe reason your recursive call only executes once is that you are essentially creating nested generators. That is, you are creating a new generator inside a generator each time you call the function recursive_generator recursively. Try the following and you will see.One simple solution, like others mention, is to use yield from.

How do I stop getting ImportError: Could not import settings 'mofin.settings' when using django with wsgi?

Dan

[How do I stop getting ImportError: Could not import settings 'mofin.settings' when using django with wsgi?](https://stackoverflow.com/questions/1411417/how-do-i-stop-getting-importerror-could-not-import-settings-mofin-settings-wh)

I can't get wsgi to import my settings file for my project 'mofin'.The list of errors from the apache error log are as followsI got the "hello world!" wsgi app listed here(http://code.google.com/p/modwsgi/wiki/QuickConfigurationGuide) to work fine. The settings.py file loads fine with python manage.py (runserver|shell|syncdb|test store)

as does the application.Here is my wsgi file:the sys.path as printed in the error log isif I open an interactive shell with manage.py, sys.path isMy django settings file looks like this:

    # Django settings for mofin project.

2009-09-11 14:58:37Z

I can't get wsgi to import my settings file for my project 'mofin'.The list of errors from the apache error log are as followsI got the "hello world!" wsgi app listed here(http://code.google.com/p/modwsgi/wiki/QuickConfigurationGuide) to work fine. The settings.py file loads fine with python manage.py (runserver|shell|syncdb|test store)

as does the application.Here is my wsgi file:the sys.path as printed in the error log isif I open an interactive shell with manage.py, sys.path isMy django settings file looks like this:

    # Django settings for mofin project.This can also happen if you have an application (subdirectory to the project with an init file in it) named the same thing as the project.  Your settings.py file may be in your project folder, but it seems that a part of the django system looks first for a module inside the project by the same name as the project and when it can't find a settings.py in there, it fails with a misleading message.Just something else to check for anyone else having this problem.  Applies to Django 1.3 and probably others.I had a similar permissions problem, and although my settings.py had the right permissions, the .pyc's did not!!! So watch out for this.Hey, just adding an additional answer to this problem. I had the exact same issue, but it wasn't file permissions. I was appending "path/to/project", but not also appending "path/to". Linked   is mod_wsgi's Django integration explanation that showed me the answer.I found the answer... file permissions. /home/django was set to 700. i.e. only django can view the contents. apache runs as Apache and so can't get past /home/django. I think you need to have a trailing forward slash on that its what I have to do in my wsgi script in apache before I load up django.In my caseAnother cause of this problem is that you can't name your application the same as another python module. For example I called mine site, little realising that site is already a python module.You can check this by starting python, and running import site, help(site), and it will show you it isn't using your module. This of course gives you errors when django tries to import site.settings which doesn't exist.Possible problem:you forgot the __init__.py file, which must be in your project and in all directories which you consider a python module for import.Other thing you could try is to add the path directly into the manage.py file, like :I hope it helpsI had the same problem but another solution :My project folder was named exactly as one of my application.I had :/home/myApp

/home/myApp/settings.py

/home/myApp/manage.py

/home/myApp/rights.py

/home/myApp/static/

/home/myApp/static/

/home/myApp/myApp/model.py

/home/myApp/myApp/admin.py

/home/myApp/myApp/views.pyThis kind of tree doesn't seems to be possible easily.

I changed the name of my project root folder and the problem was solved!(I wrote up this same answer for Django deployment problem in Apache/mod_wsgi. ImportError: Could not import settings 'site.settings' in case someone only finds this question.)This doesn't appear to be the problem in your case, but I ran smack into the same ImportError when I used the WSGIPythonPath directive (instead of the .wsgi file) to set up sys.path. That worked fine until I switched to running WSGI in daemon mode. Once you do that, you have to use the python-path argument to the WSGIDaemonProcess directive instead.In my case, I had a circular import that was causing this error. From settings.py I was importing one function in another module, and from that module I was importing a settings variable. To fix it, instead of directly importing from settings, I did this:Let me add and my experience for that issue. After head banging for few hours and try all from the above answers I found that few lines in settings.py file  cause the problem:After that I made copy of the settings.py, named scripts_settings.py whithout that lines, and used that file and everything is ok now.At first look I'd say the python path is wrong but compared to interactive shell it looks ok.

So maybe try this:I was going to say that you can just insert/append your project directory to your sys.path in your wsgi file but if your settings file is atThen you should be good there.That pretty much sums up what you are looking for.Interesting that the error propagates though:but you have what appears to be the exact default.You might want to check which python interpreter is pointed to by wsgi.  Are you intending to use a virtualenv but wsgi is looking at your system install?You can also set the user and group that wsgi is running under.  I use something like:WSGIDaemonProcess mysite.com user=skyl group=skyl processes=n threads=N python-path=/home/skyl/pinax/pinax-env2/lib/python2.6/site-packagesI had a similar problem, solved it with the following snippet in my python:Source: http://code.google.com/p/modwsgi/wiki/VirtualEnvironments#Application_Environments I just had this error and the solution was to enable my virtual environment via myvenv/source/activate. 

How do I install SciPy on 64 bit Windows?

Peter Mortensen

[How do I install SciPy on 64 bit Windows?](https://stackoverflow.com/questions/1517129/how-do-i-install-scipy-on-64-bit-windows)

How do I install SciPy on my system?For the NumPy part (that SciPy depends on) there is actually an installer for 64 bit Windows: numpy-1.3.0.win-amd64-py2.6.msi (is direct download URL, 2310144 bytes).Running the SciPy superpack installer results in this

message in a dialog box:I already have Python 2.6.2 installed (and a working Django installation

in it), but I don't know about any Registry story.The registry entries seem to already exist:What I have done so far:Step 1Downloaded the NumPy superpack installer

numpy-1.3.0rc2-win32-superpack-python2.6.exe

(direct download URL, 4782592 bytes). Running this installer

resulted in the same message, "Cannot install. Python

version 2.6 required, which was not found in the registry.".

Update: there is actually an installer for NumPy that works - see beginning of the question.Step 2Tried to install NumPy in another way. Downloaded the zip

package numpy-1.3.0rc2.zip (direct download URL, 2404011 bytes),

extracted the zip file in a normal way to a temporary

directory, D:\temp7\numpy-1.3.0rc2 (where setup.py and

README.txt is). I then opened a command line window and:This ran for a long time and also included use of cl.exe

(part of Visual Studio). Here is a nearly 5000 lines long

transcript (230 KB).This seemed to work. I can now do this in Python:with this result:Step 3Downloaded the SciPy superpack installer, scipy-0.7.1rc3-

win32-superpack-python2.6.exe (direct download URL, 45597175

bytes). Running this installer resulted in the message

listed in the beginningStep 4Tried to install SciPy in another way. Downloaded the zip

package scipy-0.7.1rc3.zip (direct download URL, 5506562

bytes), extracted the zip file in a normal way to a

temporary directory, D:\temp7\scipy-0.7.1 (where setup.py

and README.txt is). I then opened a command line window and:This did not achieve much - here is a transcript (about 95

lines).And it fails:Platform: Python 2.6.2 installed in directory D:\Python262,

Windows XP 64 bit SP2, 8 GB RAM, Visual Studio 2008

Professional Edition installed.The startup screen of the installed Python is:Value of PATH, result from SET in a command line window:

2009-10-04 19:01:43Z

How do I install SciPy on my system?For the NumPy part (that SciPy depends on) there is actually an installer for 64 bit Windows: numpy-1.3.0.win-amd64-py2.6.msi (is direct download URL, 2310144 bytes).Running the SciPy superpack installer results in this

message in a dialog box:I already have Python 2.6.2 installed (and a working Django installation

in it), but I don't know about any Registry story.The registry entries seem to already exist:What I have done so far:Step 1Downloaded the NumPy superpack installer

numpy-1.3.0rc2-win32-superpack-python2.6.exe

(direct download URL, 4782592 bytes). Running this installer

resulted in the same message, "Cannot install. Python

version 2.6 required, which was not found in the registry.".

Update: there is actually an installer for NumPy that works - see beginning of the question.Step 2Tried to install NumPy in another way. Downloaded the zip

package numpy-1.3.0rc2.zip (direct download URL, 2404011 bytes),

extracted the zip file in a normal way to a temporary

directory, D:\temp7\numpy-1.3.0rc2 (where setup.py and

README.txt is). I then opened a command line window and:This ran for a long time and also included use of cl.exe

(part of Visual Studio). Here is a nearly 5000 lines long

transcript (230 KB).This seemed to work. I can now do this in Python:with this result:Step 3Downloaded the SciPy superpack installer, scipy-0.7.1rc3-

win32-superpack-python2.6.exe (direct download URL, 45597175

bytes). Running this installer resulted in the message

listed in the beginningStep 4Tried to install SciPy in another way. Downloaded the zip

package scipy-0.7.1rc3.zip (direct download URL, 5506562

bytes), extracted the zip file in a normal way to a

temporary directory, D:\temp7\scipy-0.7.1 (where setup.py

and README.txt is). I then opened a command line window and:This did not achieve much - here is a transcript (about 95

lines).And it fails:Platform: Python 2.6.2 installed in directory D:\Python262,

Windows XP 64 bit SP2, 8 GB RAM, Visual Studio 2008

Professional Edition installed.The startup screen of the installed Python is:Value of PATH, result from SET in a command line window:I haven't tried it, but you may want to download this version of Portable Python. It comes with Scipy-0.7.0b1 running on Python 2.5.4. Unofficial 64-bit installers for NumPy and SciPy are available at http://www.lfd.uci.edu/~gohlke/pythonlibs/Make sure that you download & install the packages (aka. wheels) that match your CPython version and bitness (ie. cp35 = Python v3.5; win_amd64 = x86_64).You'll want to install NumPy first;  From a CMD prompt with administrator privileges for a system-wide (aka. Program Files) install:Or include the --user flag to install to the current user's application folder (Typically %APPDATA%\Python on Windows) from a non-admin CMD prompt:Then do the same for SciPy:Don't forget to replace <version>, <ver-spec>, and <cpu-build> appropriately if you copy & paste any of these examples.  And also that you must use the numpy & scipy packages from the ifd.uci.edu link above (or else you will get errors if you try to mix & match incompatible packages -- uninstall any conflicting packages first [ie. pip list]).Short answer: Windows 64 bit support is still work in progress at this time. The superpack will certainly not work on a 64-bits Python (but it should work fine on a 32 bits Python, even on Windows 64 bit).The main issue with Windows 64 bit is that building with mingw-w64 is not stable at this point: it may be our's (NumPy developers) fault, Python's fault or mingw-w64. Most likely a combination of all those :). So you have to use proprietary compilers: anything other than the Microsoft compiler crashes NumPy randomly; for the Fortran compiler, ifort is the one to use. As of today, both NumPy and SciPy source code can be compiled with VisualStudio2008 and ifort (all tests passing), but building it is still quite a pain, and not well supported by the NumPy build infrastructure.As the transcript for SciPy told you, SciPy isn't really supposed to work on Win64:So I would suggest to install the 32-bit version of Python, and stop attempting to build SciPy yourself. If you still want to try anyway, you first need to compile BLAS and LAPACK, as PiotrLegnica says. See the transcript for the places where it was looking for compiled versions of these libraries.WinPython is an open-source distribution that has 64-bit NumPy and SciPy.Another alternative: http://www.pythonxy.com/Free and includes lots of stuff meant to work together smoothly.This person says Though I'm not quite sure what that means.Update:This appears to be dead. I use Anaconda now, which has 32-bit or 64-bit installers.For completeness:  Enthought has a Python distribution which includes SciPy; however, it's not free.  Caveat:  I've never used it.Update: This answer had been long forgotten until an upvote brought me back to it.  At this time, I'll second endolith's suggestion of Anaconda, which is free.Try to install Python 2.6.3 over your 2.6.2 (this should also add correct Registry entry), or to register your existing installation using this script. Installer should work after that.Building SciPy requires a Fortran compiler and libraries - BLAS and LAPACK.It is terrible to install such Python data science packages independently on Windows. Try Anaconda (one installer, 400 more Python packages, py2 & py3 support). Anaconda really helps me a lot!I have a 32-bit Python 3.5 on a 64-bit Windows8.1 machine. I just tried almost every way I can find on StackOverflow and no one works!Then on here I found it. It says:mkl matters!! But nobody said anything about that before!Then I installed mkl:Then I installed SciPy:It worked~ yeah :)A tip: You can just google "whl_file_name.whl" to know where to download it~ :)Update:After all these steps you will find that you still can not use SciPy in Python3. If you print "import scipy" you will find there are error messages, but don't worry, there is only one more thing to do. Here

just comment out that line, simple and useful.I promise that it is the last thing to do :)PS: Before all these steps, you better install NumPy first. That's very simple using this command:Install a Python distribution, http://www.python.org/download/.Download and install the Anaconda Python distribution.Make the Anaconda Python distribution link to Python3.3 if you want NumPy, SciPy or Matplotlib to work in Python3.3, or just use it like that to have only Python2.7 and older functionality.The blog post Anaconda Python Distribution Python 3.3 linking provides more detail about Anaconda.I was getting this same error on a 32-bit machine.  I fixed it by registering my Python installation, using the script at:http://effbot.org/zone/python-register.htm It's possible that the script would also make the 64-bit superpack installers work.You can either download a scientific Python distribution. One of the ones mentioned here: https://scipy.org/install.htmlOr pip install from a whl file here if the above is not an option for you.http://www.lfd.uci.edu/~gohlke/pythonlibs/#scipyOkay a lot has been said, but just in case nothing of the previous answers work, you can try;https://www.scipy.org/install.htmlAccording to them;Still for me, Anaconda did solve this problem. Do remember to check the bit (32/64 bit) version before downloading and re-adjust your compiler to the Python implementation installed with the Python distribution you are installing.Okey, here I am going to share what I have done to install SciPy on my Windows PC without the command line.I found this solution after days.Firstly, which Python version you want to install?If you want for Python 2.7 version:STEP 1:If you want for Python 3.4 version:If you want for Python 3.5 version:numpy1.11.3+mklcp35cp35mwin_amd64.whlIf you want for Python 3.6 version:Link: clickOnce finishing installation, go to your directory.For example, my directory:STEP 2:From same web site based on the Python version again:After that use same thing again in the script folder:And test it in the Python folder.

How can I resolve 'django_content_type already exists'?

Dan O'Boyle

[How can I resolve 'django_content_type already exists'?](https://stackoverflow.com/questions/29760817/how-can-i-resolve-django-content-type-already-exists)

After upgrading to django 1.8 I'm recieving the error during migration:I'd be interested in the background behind this error, but more importantly,

How can I resolve it?

2015-04-21 00:30:58Z

After upgrading to django 1.8 I'm recieving the error during migration:I'd be interested in the background behind this error, but more importantly,

How can I resolve it?Initial migrations on a project can sometimes be troubleshot using --fake-initialIt's new in 1.8. In 1.7, --fake-initial was an implicit default, but explicit in 1.8.From the Docs:https://docs.djangoproject.com/en/1.8/ref/django-admin/#django-admin-option---fake-initialI granted all privileges to the user on that specific database and it solved the issue.I solved this issue on Django 2.2.7 or Django 3.0 hosted on Ubuntu 18.04 + Postgres 10.10 version.

C and Python - different behaviour of the modulo (%) operation

psihodelia

[C and Python - different behaviour of the modulo (%) operation](https://stackoverflow.com/questions/1907565/c-and-python-different-behaviour-of-the-modulo-operation)

I have found that the same mod operation produces different results depending on what language is being used.In Python:produces 9In C it produces -1 !

2009-12-15 13:46:07Z

I have found that the same mod operation produces different results depending on what language is being used.In Python:produces 9In C it produces -1 !Python has a "true" modulo operation, while C has a remainder operation.It has a direct relation with how the negative integer division is handled, i.e. rounded towards 0 or minus infinite. Python rounds towards minus infinite and C(99) towards 0, but in both languages (n/m)*m + n%m == n, so the % operator must compensate in the correct direction.Ada is more explicit and has both, as mod and rem.In C89/90 the behavior of division operator and remainder operator with negative operands is implementation-defined, meaning that depending on the implementation you can get either behavior. It is just required that the operators agree with each other: from a / b = q and a % b = r follows a = b * q + r. Use static asserts in your code to check the behavior, if it relies critically on the result. In C99 the behavior you observe has become standard.In fact, either behaviors have certain logic in it. The Python's behavior implements the true modulo operation. The behavior you observed is C is consistent with rounding towards 0 (it's also Fortran behavior). One of the reasons the rounding towards 0 is preferred in C is that it is rather natural to expect the result of -a / b be the same as -(a / b). In case of true modulo behavior, -1 % 10 would evaluate to 9, meaning that -1 / 10 has to be -1. This might be seen as rather unnatural, since -(1 / 10) is 0.Both answers are correct since -1 modulo 10 is the same as 9 modulo 10. You can be sure that |r| < |n|, but not what the value of r is.   There are 2 answers, negative and positive.In C89, although the answer will always be correct, the exact value of a modulo operation (they refer to it as remainder) is undefined, meaning it can be either a negative result or a positive result.   In C99 the result is defined. If you want the positive answer though, you can simply add 10 if you find your answer is negative. To get the modulo operator to work the same on all languages, just remember that:and in general: Performing Euclidean division a = b*q + r, is like rounding the fraction a/b to an integer quotient q, and then compute the remainder r.The different results you see depends on the convention used for rounding the quotient...If you round toward zero (truncate), you will get a symmetry around zero like in C:If you round toward negative infinity (floor), you will get a remainder like in Python:If you round to nearest int (tie to whatever you want, to even, or away from zero) you'll get a centered modulo:You could try to implement your own modulo with rounding toward positive infinity (ceil), and you would invent a rather unconventional modulo, but it would still be kind of modulo...Since python 3.7 you can also use .remainder() from math built-in module.From docs:

Send log messages from all celery tasks to a single file

Martijn Pieters

[Send log messages from all celery tasks to a single file](https://stackoverflow.com/questions/6192265/send-log-messages-from-all-celery-tasks-to-a-single-file)

I'm wondering how to setup a more specific logging system. All my tasks useas a module-wide logger.I want celery to log to "celeryd.log" and my tasks to "tasks.log" but I got no idea how to get this working. Using CELERYD_LOG_FILE from django-celery I can route all celeryd related log messages to celeryd.log but there is no trace of the log messages created in my tasks.

2011-05-31 19:05:00Z

I'm wondering how to setup a more specific logging system. All my tasks useas a module-wide logger.I want celery to log to "celeryd.log" and my tasks to "tasks.log" but I got no idea how to get this working. Using CELERYD_LOG_FILE from django-celery I can route all celeryd related log messages to celeryd.log but there is no trace of the log messages created in my tasks.Note: This answer is outdated as of Celery 3.0, where you now use get_task_logger() to get your per-task logger set up. Please see the Logging section of the What's new in Celery 3.0 document for details.Celery has dedicated support for logging, per task. See the Task documentation on the subject:Under the hood this is all still the standard python logging module. You can set the CELERYD_HIJACK_ROOT_LOGGER option to False to allow your own logging setup to work, otherwise Celery will configure the handling for you.However, for tasks, the .get_logger() call does allow you to set up a separate log file per individual task. Simply pass in a logfile argument and it'll route log messages to that separate file:Last but not least, you can just configure your top-level package in the python logging module and give it a file handler of it's own. I'd set this up using the celery.signals.after_setup_task_logger signal; here I assume all your modules live in a package called foo.tasks (as in foo.tasks.email and foo.tasks.scaling):Now any logger whose name starts with foo.tasks will have all it's messages sent to tasks.log instead of to the root logger (which doesn't see any of these messages because .propagate is False).Just a hint: Celery has its own logging handler:Also, Celery logs all output from the task. More details at Celery docs for Task Loggingjoin 

--concurrency=1 --loglevel=INFO

with the command to run celery workereg: python xxxx.py celery worker --concurrency=1 --loglevel=INFOBetter to set loglevel inside each python files too

Understanding NumPy's Convolve

Nyxynyx

[Understanding NumPy's Convolve](https://stackoverflow.com/questions/20036663/understanding-numpys-convolve)

When calculating a simple moving average, numpy.convolve appears to do the job.Question: How is the calculation done when you use np.convolve(values, weights, 'valid')? When the docs mentioned convolution product is only given for points where the signals overlap completely, what are the 2 signals referring to?If any explanations can include examples and illustrations, it will be extremely useful.

2013-11-17 21:53:15Z

When calculating a simple moving average, numpy.convolve appears to do the job.Question: How is the calculation done when you use np.convolve(values, weights, 'valid')? When the docs mentioned convolution product is only given for points where the signals overlap completely, what are the 2 signals referring to?If any explanations can include examples and illustrations, it will be extremely useful.Convolution is a mathematical operator primarily used in signal processing. Numpy simply uses this signal processing nomenclature to define it, hence the "signal" references. An array in numpy is a signal. The convolution of two signals is defined as the integral of the first signal, reversed, sweeping over ("convolved onto") the second signal and multiplied (with the scalar product) at each position of overlapping vectors. The first signal is often called the kernel, especially when it is a 2-D matrix in image processing or neural networks, and the reversal becomes a mirroring in 2-D (NOT transpose). It can more clearly be understood using the animations on wikipedia.Convolutions have multiple definitions depending on the context. Some start the convolution when the overlap begins while others start when the overlap is only partial. In the case of numpy's "valid" mode, the overlap is specified to be always complete. It is called "valid" since every value given in the result is done without data extrapolation.For instance,  if your array X have a length of 2 and your array Y have a length of 4, the convolution of X onto Y in "valid" mode will give you an array of length 3.First step, for X = [4 3] and Y = [1 1 5 5]:Note: If X was not reversed, the operation would be called a cross-correlation instead of a convolution.Second Step:Third step:The result of the convolution for mode "valid" would then be [7 23 35].If the overlap is be specified as one single data point (as the case in mode "full"), the result would have given you an array of length 5. The first step being:And so on. More extrapolation modes exist.

How to use MinGW's gcc compiler when installing Python package using Pip?

demalexx

[How to use MinGW's gcc compiler when installing Python package using Pip?](https://stackoverflow.com/questions/3297254/how-to-use-mingws-gcc-compiler-when-installing-python-package-using-pip)

I configured MinGW and distutils so now I can compile extensions using this command:MinGW's gcc compiler will be used and package will be installed. For that I installed MinGW and created distutils.cfg file with following content:It's cool but now I'd like to use all pip benefits. Is there a way to use the same MinGW's gcc compiler in pip? So that when I run this:pip will use MinGW's gcc compiler and compile C code if needed?Currently I get this error: Unable to find vcvarsall.bat. Seems pip doesn't know that I have gcc compiler. How can I configure pip to use gcc compiler?

2010-07-21 07:56:07Z

I configured MinGW and distutils so now I can compile extensions using this command:MinGW's gcc compiler will be used and package will be installed. For that I installed MinGW and created distutils.cfg file with following content:It's cool but now I'd like to use all pip benefits. Is there a way to use the same MinGW's gcc compiler in pip? So that when I run this:pip will use MinGW's gcc compiler and compile C code if needed?Currently I get this error: Unable to find vcvarsall.bat. Seems pip doesn't know that I have gcc compiler. How can I configure pip to use gcc compiler?Even though configuration file solves this problem, it's not always an option. I had the same issue for my command line installation process and I was not able to change config files on all the machines and python distributions.This is my solution:For mingw32 and packages, which use VC++ as default:For Visual C++ on WinPython, which uses mingw32 as default:

How to add a single item to a Pandas Series

eran

[How to add a single item to a Pandas Series](https://stackoverflow.com/questions/13331518/how-to-add-a-single-item-to-a-pandas-series)

How Do I add a single item to a serialized panda series. I know it's not the most efficient way memory wise, but i still need to do that.Something along:also, how can i add a single row to a pandas DataFrame? 

2012-11-11 13:26:27Z

How Do I add a single item to a serialized panda series. I know it's not the most efficient way memory wise, but i still need to do that.Something along:also, how can i add a single row to a pandas DataFrame? How to add single item. This is not very effective but follows what you are asking for:produces x:Obviously there are better ways to generate this series in only one shot.  For your second question check answer and references of SO question add one row in a pandas.DataFrame. TLDR: do not append items to a series one by one, better extend with an ordered collectionI think the question in its current form is a bit tricky. And the accepted answer does answer the question. But the more I use pandas, the more I understand that it's a bad idea to append items to a Series one by one. I'll try to explain why for pandas beginners.You might think that appending data to a given Series might allow you to reuse some resources, but in reality a Series is just a container that stores a relation between an index and a values array. Each is a numpy.array under the hood, and the index is immutable. When you add to Series an item with a label that is missing in the index, a new index with size n+1 is created, and a new values values array of the same size. That means that when you append items one by one, you create two more arrays of the n+1 size on each step.By the way, you can not append a new item by position (you will get an IndexError) and the label in an index does not have to be unique, that is when you assign a value with a label, you assign the value to all existing items with the the label, and a new row is not appended in this case. This might lead to subtle bugs.The moral of the story is that you should not append data one by one, you should better extend with an ordered collection. The problem is that you can not extend a Series inplace. That is why it is better to organize your code so that you don't need to update a specific instance of a Series by reference.If you create labels yourself and they are increasing, the easiest way is to add new items to a dictionary, then create a new Series from the dictionary (it sorts the keys) and append the Series to an old one. If the keys are not increasing, then you will need to create two separate lists for the new labels and the new values.Below are some code samples:When we update an existing item, the index and the values array stay the same (if you do not change the type of the value)But when you add a new item, a new index and a new values array is generated:That is if you are going to append several items, collect them in a dictionary, create a Series, append it to the old one and save the result:If you have an index and value. Then you can add to Series as:this will add a new value to Series (at the end of Series).You can use the append function to add another element to it. Only, make a series of the new element, before you append it:Adding to joquin's answer the following form might be a bit cleaner (at least nicer to read):which would produce the same outputalso, a bit less orthodox but if you wanted to simply add a single element to the end:As far as @joaqin's solution is deprecated, because set_value method will be removed in a future pandas release, I would mention the other option to add a single item to pandas series, using .at[] accessor.It produces the same output.Here is another thought n for appending multiple items in one line without changing the name of series. However, this may be not as efficient as the other answer.

Does virtualenv serve a purpose (in production) when using docker?

siebz0r

[Does virtualenv serve a purpose (in production) when using docker?](https://stackoverflow.com/questions/27017715/does-virtualenv-serve-a-purpose-in-production-when-using-docker)

For development we use virtualenv to have an isolated development when it comes to dependencies. From this question it seems deploying Python applications in a virtualenv is recommended.Now we're starting to use docker for deployment. This provides a more isolated environment so I'm questioning the use of virtualenv inside a docker container. In the case of a single application I do not think virtualenv has a purpose as docker already provides isolation. In the case where multiple applications are deployed on a single docker container, I do think virtualenv has a purpose as the applications can have conflicting dependencies. Should virtualenv be used when a single application is deployed in a docker container?Should docker contain multiple applications or only one application per container?If so, should virtualenv be used when deploying a container with multiple applications?

2014-11-19 13:11:53Z

For development we use virtualenv to have an isolated development when it comes to dependencies. From this question it seems deploying Python applications in a virtualenv is recommended.Now we're starting to use docker for deployment. This provides a more isolated environment so I'm questioning the use of virtualenv inside a docker container. In the case of a single application I do not think virtualenv has a purpose as docker already provides isolation. In the case where multiple applications are deployed on a single docker container, I do think virtualenv has a purpose as the applications can have conflicting dependencies. Should virtualenv be used when a single application is deployed in a docker container?Should docker contain multiple applications or only one application per container?If so, should virtualenv be used when deploying a container with multiple applications?Virtualenv was created long before docker. Today, I lean towards docker instead of virtualenv for these reasons:The main drawback for Docker was the poor Windows support. That changed with the version for Windows 10.As for "how many apps per container", the usual policy is 1. Yes. You should still use virtualenv. Also, you should be building wheels instead of eggs now. Finally, you should make sure that you keep your Docker image lean and efficient by building your wheels in a container with the full build tools and installing no build tools into your application container.You should read this excellent article. https://glyph.twistedmatrix.com/2015/03/docker-deploy-double-dutch.htmlThe key take away isIntroducing virtualenv is very easy, so I'd say start without it on your docker container. If the need arises, then maybe you can install it. Running "pip freeze > requirements.txt" will give you all your python packages. 

However, I doubt you'll ever need virtualenv inside a docker container as creating another container would be a more preferable alternative. I would not recommend having more than one application in a single container. When you get to this point, your container is doing too much.I use both because with that you can more easily use multi stage builds and simply move your dependencies you built in one stage into later images/layers. Example can be found here.If someone wants to replace virtualenv completely using docker he can.Just create different Dockerfile for different environment and use port and volumes as you need for environment.As an example for development you can use this project. Run docker compose and start coding. 

Write your own Dockerfiles for different environments like test, staging and production by putting your log and data in volume.This link is also useful https://vsupalov.com/docker-python-development/.

How can I tell if NumPy creates a view or a copy?

Hooked

[How can I tell if NumPy creates a view or a copy?](https://stackoverflow.com/questions/11524664/how-can-i-tell-if-numpy-creates-a-view-or-a-copy)

For a minimal working example, let's digitize a 2D array. numpy.digitize requires a 1D array:Now the documentation says:How do I know if the ravel copy it is "needed" in this case? In general - is there a way I can determine if a particular operation creates a copy or a view?

2012-07-17 14:26:05Z

For a minimal working example, let's digitize a 2D array. numpy.digitize requires a 1D array:Now the documentation says:How do I know if the ravel copy it is "needed" in this case? In general - is there a way I can determine if a particular operation creates a copy or a view?This question is very similar to a question that I asked a while back:You can check the base attribute.However, that's not perfect.  You can also check to see if they share memory using np.may_share_memory.There's also the flags attribute that you can check:But this last one seems a little fishy to me, although I can't quite put my finger on why...In the documentation for reshape there is some information about how to ensure an exception if a view cannot be made:

This is not exactly an answer to your question, but in certain cases it may be just as useful.

What's the fastest way of checking if a point is inside a polygon in python

Ruben Perez-Carrasco

[What's the fastest way of checking if a point is inside a polygon in python](https://stackoverflow.com/questions/36399381/whats-the-fastest-way-of-checking-if-a-point-is-inside-a-polygon-in-python)

I found two main methods to look if a point belongs inside a polygon. One is using the ray tracing method used here, which is the most recommended answer, the other is using matplotlib path.contains_points (which seems a bit obscure to me). I will have to check lots of points continuously. Does anybody know if any of these two is more recommendable than the other or if there are even better third options? UPDATE:I checked the two methods and matplotlib looks much faster. which gives,Same relative difference was obtained one using a triangle instead of the 100 sides polygon. I will also check shapely since it looks a package just devoted to these kind of problems 

2016-04-04 09:50:22Z

I found two main methods to look if a point belongs inside a polygon. One is using the ray tracing method used here, which is the most recommended answer, the other is using matplotlib path.contains_points (which seems a bit obscure to me). I will have to check lots of points continuously. Does anybody know if any of these two is more recommendable than the other or if there are even better third options? UPDATE:I checked the two methods and matplotlib looks much faster. which gives,Same relative difference was obtained one using a triangle instead of the 100 sides polygon. I will also check shapely since it looks a package just devoted to these kind of problems You can consider shapely:From the methods you've mentioned I've only used the second, path.contains_points, and it works fine. In any case depending on the precision you need for your test I would suggest creating a numpy bool grid with all nodes inside the polygon to be True (False if not). If you are going to make a test for a lot of points this might be faster (although notice this relies you are making a test within a "pixel" tolerance):, the results is this:If speed is what you need and extra dependencies are not a problem, you maybe find numba quite useful (now it is pretty easy to install, on any platform). The classic ray_tracing approach you proposed can be easily ported to numba by using numba @jit decorator and casting the polygon to a numpy array. The code should look like:  The first execution will take a little longer than any subsequent call:Which, after compilation will decrease to:If you need speed at the first call of the function you can then pre-compile the code in a module using pycc. Store the function in a src.py like:  Build it with python src.py and run:In the numba code I used:

    'b1(f8, f8, f8[:,:])'In order to compile with nopython=True, each var needs to be declared before the for loop. In the prebuild src code the line:Is used to declare the function name and its I/O var types, a boolean output b1 and two floats f8 and a two-dimensional array of floats f8[:,:] as input. Your test is good, but it measures only some specific situation:

we have one polygon with many vertices, and long array of points to check them within polygon.Moreover, I suppose that you're measuring not 

matplotlib-inside-polygon-method vs ray-method,

but 

